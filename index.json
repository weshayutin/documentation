[{"content":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster.\nVideo Walkthrough If you prefer a more visual medium, you can watch Paul Czarkowski walk through this quickstart on YouTube .\nPrerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended\nLog into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL\naz aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials\naz aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.\nDeploy an application to OpenShift\nSee the following video for a guide on easy application deployment on OpenShift.\nDelete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP Adendum Adding Quota to ARO account Create an Azure Support Request Set Issue Type to “Service and subscription limits (quotas)”\nSet Quota Type to “Compute-VM (cores-vCPUs) subscription limit increases”\nClick Next Solutions »\nClick Enter details\nSet Deployment Model to “Resource Manager\nSet Locations to “(US) East US”\nSet Types to “Standard”\nUnder Standard check “DSv3” and “DSv4”\nSet New vCPU Limit for each (example “60”)\nClick Save and continue\nClick Review + create »\nWait until quota is increased.\n","description":"","tags":["ARO","Azure","Quickstarts"],"title":"ARO Quickstart","uri":"/docs/quickstart-aro/"},{"content":"A Quickstart guide to deploying a Red Hat OpenShift cluster on AWS.\nVideo Walkthrough Quick Introduction to ROSA by Charlotte Fung on AWS YouTube channel If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube .\nPrerequisites AWS an AWS account with the AWS ROSA Prerequisites met.\nMacOS\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the macOS command line\ncurl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / Linux\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the Linux command line\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Windows\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the Windows command line\nC:\\\u003e msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Docker\nSee AWS Docs for alternative install options.\nTo run the AWS CLI version 2 Docker image, use the docker run command.\ndocker run --rm -it amazon/aws-cli command Prepare AWS Account for OpenShift Configure the AWS CLI by running the following command\naws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format\n% aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user\nValidate your credentials\naws sts get-caller-identity You should receive output similar to the following\n{ \"UserId\": \u003cyour ID\u003e, \"Account\": \u003cyour account\u003e, \"Arn\": \u003cyour arn\u003e } If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Get a Red Hat Offline Access Token Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/token/rosa Copy the Offline Access Token and save it for the next step\nSet up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine\nPlace the extracted oc executable in your OS path or local directory\nSet up the ROSA CLI Download the OS specific ROSA CLI from Red Hat Unzip the downloaded file on your local machine\nPlace the extracted rosa and kubectl executables in your OS path or local directory\nLog in to ROSA\nrosa login You will be prompted to enter in the Red Hat Offline Access Token you retrieved earlier and should receive the following message\nLogged in as \u003cemail address\u003e on 'https://api.openshift.com' Verify ROSA privileges Verify that ROSA has the minimal permissions\nrosa verify permissions Expected output: AWS SCP policies ok\nVerify that ROSA has the minimal quota\nrosa verify quota Expected output: AWS quota ok\nInitialize ROSA Initialize the ROSA CLI to complete the remaining validation checks and configurations\nrosa init Deploy Red Hat OpenShift on AWS (ROSA) Interactive Installation ROSA can be installed using command line parameters or in interactive mode. For an interactive installation run the following command\nrosa create cluster --interactive --mode auto As part of the interactive install you will be required to enter the following parameters or accept the default values (if applicable)\nCluster name: Multiple availability zones (y/N): AWS region (select): OpenShift version (select): Install into an existing VPC (y/N): Compute nodes instance type (optional): Enable autoscaling (y/N): Compute nodes [2]: Machine CIDR [10.0.0.0/16]: Service CIDR [172.30.0.0/16]: Pod CIDR [10.128.0.0/14]: Host prefix [23]: Private cluster (y/N): Note: the installation process should take between 30 - 45 minutes\nGet the web console link to the ROSA cluster To get the web console link run the following command.\nSubstitute your actual cluster name for \u003ccluster-name\u003e\nrosa describe cluster --cluster=\u003ccluster-name\u003e Create cluster-admin user By default, only the OpenShift SRE team will have access to the ROSA cluster. To add a local admin user, run the following command to create the cluster-admin account in your cluster.\nSubstitute your actual cluster name for \u003ccluster-name\u003e\nrosa create admin --cluster=\u003ccluster-name\u003e Refresh your web browser and you should see the cluster-admin option to log in\nDelete Red Hat OpenShift on AWS (ROSA) Deleting a ROSA cluster consists of two parts\nDelete the cluster instance, including the removal of AWS resources. Substitute your actual cluster name for \u003ccluster-name\u003e\nrosa delete cluster --cluster=\u003ccluster-name\u003e Delete Cluster’s operator-roles and oidc-provider as shown in the above delete cluster command’s output. For e.g.\nrosa delete operator-roles -c \u003ccluster-name\u003e rosa delete oidc-provider -c \u003ccluster-name\u003e Delete the CloudFormation stack, including the removal of the osdCcsAdmin user rosa init --delete-stack ","description":"","tags":["AWS","ROSA","Quickstarts"],"title":"ROSA Quickstart","uri":"/docs/quickstart-rosa/"},{"content":"OpenShift on AWS allows users to run OpenShift in the Amazon Web Services (AWS) cloud, using AWS resources such as compute, storage, and networking. With OpenShift on AWS, users can leverage the benefits of the AWS cloud while also taking advantage of the features and capabilities of OpenShift. OpenShift on AWS is designed to be easy to use and to provide a smooth experience for developers, enabling them to focus on building and deploying applications rather than worrying about infrastructure. ","description":"MOBB Docs and Guides for rosa","tags":null,"title":"Red Hat Openshift Service on AWS","uri":"/docs/rosa/"},{"content":"Azure Red Hat OpenShift is a fully managed, cloud-based service that allows users to quickly and easily deploy and manage containerized applications on the Azure platform. This product is a collaboration between Microsoft Azure and Red Hat, two industry leaders in cloud computing and open source software development. With Azure Red Hat OpenShift, users can leverage the benefits of Azure’s global infrastructure and scalability, as well as Red Hat’s expertise in containerization and open source technologies. This product is ideal for businesses that want to take advantage of the agility and flexibility of containers, but also need the reliability and security of a trusted cloud provider. ","description":"MOBB Docs and Guides for aro","tags":null,"title":"Azure Red Hat OpenShift","uri":"/docs/aro/"},{"content":"","description":"","tags":null,"title":"Ansible","uri":"/tags/ansible/"},{"content":"Deploying ROSA PrivateLink Cluster with Ansible Background This guide shows an example of how to deploy a classic Red Hat OpenShift Services on AWS (ROSA) cluster with PrivateLink with STS enabled using Ansible playbook from our MOBB GitHub repository and makefiles to compile them. Note that this is an unofficial Red Hat guide and your implementation may vary.\nThis guide is broken down into two main sections namely the architectural landscape of this deployment scenario including the AWS services and open source products and services that we will be using, and the implementation steps which includes the prerequisites needed to run the deployment itself.\nArchitecture In this section, we will first discuss about the architecture of the cluster we are going to create in high level. Afterward, we will talk about the components that we are utilizing in this deployment, from the Git repository where this deployment will be cloned from, the makefile that compiles the codes needed to run the deployment, to the Ansible commands that the makefile compiles.\nROSA Cluster with PrivateLink PrivateLink allows you to securely access AWS services over private network connections, without exposing your traffic to the public internet. In this scenario, we will be using Transit Gateway (TGW) allowing inter-VPC and VPC-to-on-premises communications by providing a scalable and efficient way to handle traffic between these networks.\nTo help with DNS resolution, we will be using DNS forwarder to forward the queries to Route 53 Inbound Resolver allowing the cluster to accept incoming DNS queries from external sources and thus establishing the desired connection without exposing the underlying infrastructure.\nIn addition, Egress VPC will be provisioned serving as a dedicated network component for managing outbound traffic from the cluster. A NAT Gateway will be created within the public subnet of the Egress VPC and along with it a Squid -based proxy to restrict egress traffic from the cluster to only the permitted endpoints or destinations .\nWe will also be using VPC Endpoints to privately access AWS resources, e.g. gateway endpoint for S3 bucket, interface endpoint for STS, interface endpoint for EC2 instances, etc.\nFinally, once the cluster is created, we will access it by establishing secure SSH connection using a jump host that is set up within the Egress VPC, and to do so we will be using sshuttle .\nGit Git is version control system that tracks changes to files and enables collaboration, while GitHub is a web-based hosting service for Git repositories. And in this scenario, the deployment will be based on the Ansible playbook from MOBB GitHub repository at https://github.com/rh-mobb/ansible-rosa .\nWe are specifying the default variables in ./roles/_vars/defaults/main.yml, and these default variables will then be overridden by specific variables located in ./environment/*/group_vars/all.yaml depends on the scenario.\nFor now, let’s take a look at what these default variables are. Below are the snippets from ./roles/_vars/defaults/main.yml:\n# defaults for roles/cluster_create rosa_private: false rosa_private_link: false rosa_sts: true rosa_disable_workload_monitoring: false rosa_enable_autoscaling: false rosa_hcp: false # wait for rosa to finish installing rosa_wait: true rosa_multi_az: false rosa_admin_password: \"Rosa1234password67890\" rosa_vpc_endpoints_enabled: false rosa_subnet_ids: [] rosa_machine_cidr: ~ # defaults for roles/juphost-create # when not set will search based on ami_name jumphost_ami: \"\" jumphost_ami_name: \"RHEL-8.8.0_HVM-*-x86_64-*Hourly*\" jumphost_instance_type: t1.micro # enable this if you want a second jumphost in the # rosa private subnet, useful for testing TGW connectivity jumphost_private_instance: false proxy_enabled: false # when not set will search based on ami_name # proxy_ami: ami-0ba62214afa52bec7 proxy_ami: \"\" proxy_ami_name: \"RHEL-8.8.0_HVM-*-x86_64-*Hourly*\" proxy_instance_type: m4.large # defaults for roles/vpc_create rosa_vpc_cidr: \"10.0.0.0/16\" rosa_region: \"us-east-2\" # defaults file for roles/tgw_create rosa_tgw_enabled: false # the full CIDR that TGW should route for rosa_egress_vpc_enabled: false rosa_egress_vpc_multi_az: false rosa_tgw_cidr: \"10.0.0.0/8\" rosa_egress_vpc_cidr: \"10.10.0.0/24\" As mentioned previously, we are going to override the above default variables with ones that are relevant to our scenario, and in this case it would be ROSA with PrivateLink and Transit Gateway, and to do so, we will be running the variables specified from ./environment/transit-gateway-egress/group_vars/all.yaml instead:\nrosa_private_link: true # note private-link forces private to be true # regardless of the following value. rosa_private: false rosa_sts: true rosa_multi_az: true rosa_vpc_endpoints_enabled: true rosa_version: 4.12.12 rosa_region: us-east-1 rosa_vpc_cidr: \"10.0.0.0/16\" rosa_vpc_public_subnets: [] rosa_tgw_enabled: true # The full cidr that encompasses all VPCs the TGW will be # attached to. rosa_tgw_cidr: \"10.0.0.0/8\" rosa_egress_vpc_enabled: true rosa_egress_vpc_multi_az: true # defaults file for roles/vpc rosa_egress_vpc_cidr: \"10.10.0.0/16\" jumphost_instance_type: t2.micro proxy_enabled: true Next, we will talk about what makefile is and how it helps compiling the code for our deployment in this scenario.\nMakefile Make is a build automation tool to manage the compilation and execution of programs. It reads a file called a makefile that contains a set of rules and dependencies, allowing developers to define how source code files should be compiled, linked, and executed.\nIn this scenario, the makefile can be found in the root directory of the GitHub repository, and here below is the snippet where the cluster name is set up along with the virtual environment that makefile will compile when we are running make virtualenv:\nCLUSTER_NAME ?= ans-$(shell whoami) EXTRA_VARS ?= --extra-vars \"cluster_name=$(CLUSTER_NAME)\" VIRTUALENV ?= \"./virtualenv/\" ANSIBLE = $(VIRTUALENV)/bin/ansible-playbook $(EXTRA_VARS) virtualenv: LC_ALL=en_US.UTF-8 python3 -m venv $(VIRTUALENV) . $(VIRTUALENV)/bin/activate pip install pip --upgrade LC_ALL=en_US.UTF-8 $(VIRTUALENV)/bin/pip3 install -r requirements.txt #--use-feature=2020-resolver $(VIRTUALENV)/bin/ansible-galaxy collection install -r requirements.yml As you can see above, the cluster_name variable is hardcoded in the makefile to be ans-${username}.\nAnd below are what the makefile will compile when we are running make create.tgw and make delete.tgw for this scenario.\ncreate.tgw: $(ANSIBLE) -v create-cluster.yaml -i ./environment/transit-gateway-egress/hosts delete.tgw: $(ANSIBLE) -v delete-cluster.yaml -i ./environment/transit-gateway-egress/hosts Here we see that we have Ansible commands that trigger the deployment. So next, let’s discuss about what Ansible is and how it helps building the cluster in this scenario.\nAnsible Ansible is an open-source automation tool that simplifies system management and configuration. It uses a declarative approach, allowing users to define desired states using YAML-based Playbooks . With an agentless architecture and a vast library of modules , Ansible enables automation of tasks such as configuration management, package installation, and user management.\nRecall that we have the following code snippet in the Makefile section that will be run for make create.tgw command:\ncreate.tgw: $(ANSIBLE) -v create-cluster.yaml -i ./environment/transit-gateway-egress/hosts In this case, we will be running Ansible command by executing a playbook called create-cluster.yaml and specifying ./environment/transit-gateway-egress/hosts as the inventory file.\nLet’s take a quick look at the create-cluster.yaml playbook which can be found in the repository’s root folder:\n--- - hosts: \"all\" connection: \"local\" vars_files: - vars/main.yaml roles: - name: roles/network_math - name: roles/tgw_create when: rosa_tgw_enabled | bool - name: roles/egress_vpc_create when: rosa_egress_vpc_enabled | bool - name: roles/vpc_create when: rosa_subnet_ids | default([]) | length == 0 - name: roles/jumphost_create when: - (rosa_private or rosa_private_link) or (enable_jumphost | default(False) | bool) - name: roles/proxy_create when: - (rosa_private_link | bool) and (proxy_enabled | default(False) | bool) - name: roles/cluster_create - name: roles/dns_resolver_create when: rosa_tgw_enabled | bool - name: roles/create_admin - name: roles/finish - hosts: \"jumphost\" connection: \"ssh\" remote_user: ec2-user vars: cluster_api: \"{{ hostvars.localhost._cluster_info.cluster.api.url }}\" roles: - name: roles/post_install when: - (rosa_private or rosa_private_link) or (enable_jumphost | default(False) | bool) As you can see above, the playbook consists of two plays targeting different hosts, the first one targeting all hosts and the second one targeting jump host. And within each plays, there are different tasks specified. The first play’s tasks are to be executed locally using roles that are specified depends on the scenario. For example, in this case, it will be executing the tasks in the ./roles/tgw_create because rosa_tgw_enabled value will be returned true. In similar vein, the second play’s tasks required SSH connection to the host, and in this case, it will execute the tasks in the ./roles/post_install since the value for both rosa_private_link and enable_jumphost will be returned true.\nImplementation Now that we understand the architecture of the cluster that we are going to create in this scenario in high level, along with the components and the commands that needed to run the deployment, we can now start preparing to build the cluster itself.\nPrerequisites AWS CLI ROSA CLI \u003e= 1.2.22 ansible \u003e= 2.15.0 python \u003e= 3.6 boto3 \u003e= 1.22.0 botocore \u003e= 1.25.0 make sshuttle Deployment Once you have all of the prerequisites installed, clone our repository and go to the ansible-rosa directory.\ngit clone https://github.com/rh-mobb/ansible-rosa cd ansible-rosa Then, run the following command to create python virtual environment.\nmake virtualenv Next, run the following command to allow Ansible playbook to create the cluster.\nmake create.tgw Note that the cluster setup may take up to one hour. Once the cluster is successfully deployed, you will see following snippet (note this is an example):\n...omitted... TASK [roles/finish : set_fact] ************************************************* ok: [localhost] =\u003e changed=false ansible_facts: jumphost_dns: ec2-54-89-148-118.compute-1.amazonaws.com when: _jumphost_info.instances | length \u003e 0 TASK [roles/finish : debug] **************************************************** ok: [localhost] =\u003e msg: |- Cluster API: https://api.ans-dianasari.caxr.p1.openshiftapps.com:6443 Cluster Console: https://console-openshift-console.apps.ans-dianasari.caxr.p1.openshiftapps.com Authenticate to cluster: oc login https://api.ans-dianasari.caxr.p1.openshiftapps.com:6443 \\ --username cluster-admin --password Rosa1234password67890 TASK [roles/finish : debug] **************************************************** ok: [localhost] =\u003e msg: |- SSH to Jumphost: ssh ec2-user@ec2-54-89-148-118.compute-1.amazonaws.com SSH VPN via Jumphost: sshuttle --dns -NHr ec2-user@ec2-54-89-148-118.compute-1.amazonaws.com 10.0.0.0/8 ...omitted... Next, let’s connect to the jump host and login to the cluster using the credentials provided by Ansible upon the creation task completion as seen above. In this example, we will be using sshuttle (note that this is just an example, so please refer to the one provided by your Ansible deployment):\nsshuttle --dns -NHr ec2-user@ec2-54-89-148-118.compute-1.amazonaws.com 10.0.0.0/8 And you may need your password for local sudo access and proceed with yes when asked for if you want to continue connecting. And once you see that it is connected in your terminal, hop on to your browser using the console URL provided. In the example above, the URL was https://console-openshift-console.apps.ans-dianasari.caxr.p1.openshiftapps.com (note that this is just an example, so please refer to the one provided by your Ansible deployment).\nIt will then ask you to login using htpasswd, so click that button, and use the username and the password provided by the Ansible deployment. In this case, as you can see from the snippet above, the username is cluster-admin and the password is Rosa1234password67890. And finally click the login button.\nNote that cluster-admin username with its password are precreated in this scenario, so you might want to modify this for your own deployment.\nAnd once you are done with the cluster, use the following command to delete the cluster.\nmake delete.tgw ","description":"","tags":["ROSA","Ansible"],"title":"Deploying ROSA PrivateLink Cluster with Ansible","uri":"/docs/rosa/ansible-rosa/"},{"content":"","description":"","tags":null,"title":"ROSA","uri":"/tags/rosa/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"},{"content":"","description":"","tags":null,"title":"ARO","uri":"/tags/aro/"},{"content":"","description":"","tags":null,"title":"Azure","uri":"/tags/azure/"},{"content":"There are two way to configure this set up\nSelf provision the storage account and file share Auto provision the storage account and file share can dynamically provision the storage via PVC dont have to create PVs, the PVC will create that for you Pre Requisites private aro cluster Set Environment Variables export AZR_REGION=eastus \\ AZR_RESOURCE_GROUP=\u003cmy-rg\u003e \\ AZR_VNET=\u003cmy-vnet\u003e \\ AZR_CLUSTER_NAME=\u003cmy-cluster-name\u003e \\ AZR_STORAGE_ACCOUNT_NAME=\u003cmy-storage-account\u003e \\ AZR_SERVICES_SUBNET=\u003cprivate-endpoint-subnet-name\u003e \\ AZR_FILE_SHARE=\u003cyour-file-share-name\u003e OC_STORAGE_ACCOUNT_SECRET_NAME=\u003cyour-name\u003e Self-Provision Storage Account and Create/Configure the Private Endpoint Note if you would like to dynamically provision the storage account using the CSI provisioner skip the first step Create the storage account and attach the private endpoint to it az storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --location $AZR_REGION \\ --sku Standard_LRS \\ --kind StorageV2 Create the file share in your storage account az storage share create \\ --account-name $AZR_STORAGE_ACCOUNT_NAME \\ --name $AZR_FILE_SHARE \\ --quota 5 *NOTE make sure the share quota matches the quota set later when creating the PVC\nCreate a services subnet in the cluster rg and vnet for the Private Endpoint az network vnet subnet create \\ --name $AZR_SERVICES_SUBNET \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_VNET \\ --address-prefix 10.0.12.0/23 address prefix will be configured based on your vnet IP addressing *NOTE we recommend creating separate subnets for services, especially when you are using a private ARO environment\nCreate private endpoint az network private-endpoint create \\ --name $AZR_CLUSTER_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_VNET \\ --subnet $AZR_SERVICES_SUBNET \\ --private-connection-resource-id $(az resource show -g $AZR_RESOURCE_GROUP -n $AZR_STORAGE_ACCOUNT_NAME --resource-type \"Microsoft.Storage/storageAccounts\" --query \"id\" -o tsv) \\ --location $AZR_REGION \\ --group-id file \\ --connection-name $AZR_STORAGE_ACCOUNT_NAME DNS Resolution for Private Connection Configure the private DNS zone for the private link connection In order to use the private endpoint connection you will need to create a private DNS zone, if not configured correctly, the connection will attempt to use the public IP (file.core.windows.net) whereas the private connection’s domain is prefixed with ‘privatelink’\naz network private-dns zone create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name \"privatelink.file.core.windows.net\" az network private-dns link vnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --zone-name \"privatelink.file.core.windows.net\" \\ --name $AZR_CLUSTER_NAME \\ --virtual-network $AZR_VNET \\ --registration-enabled false If you are using a custom DNS server on your network, clients must be able to resolve the FQDN for the storage account endpoint to the private endpoint IP address. You should configure your DNS server to delegate your private link subdomain to the private DNS zone for the VNet, or configure the A records for StorageAccountA.privatelink.file.core.windows.net with the private endpoint IP address.\nWhen using a custom or on-premises DNS server, you should configure your DNS server to resolve the storage account name in the privatelink subdomain to the private endpoint IP address. You can do this by delegating the privatelink subdomain to the private DNS zone of the VNet or by configuring the DNS zone on your DNS server and adding the DNS A records.\nFor MAG customers:\nGOV Private Endpoint DNS Custom DNS Config Retrieve the private IP from the private link connection: PRIVATE_IP=`az resource show \\ --ids $(az network private-endpoint show --name $AZR_CLUSTER_NAME --resource-group $AZR_RESOURCE_GROUP --query 'networkInterfaces[0].id' -o tsv) \\ --api-version 2019-04-01 \\ -o json | jq -r '.properties.ipConfigurations[0].properties.privateIPAddress'` Create the DNS records for the private link connection: az network private-dns record-set a create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --zone-name privatelink.file.core.windows.net \\ --resource-group $AZR_RESOURCE_GROUP az network private-dns record-set a add-record \\ --record-set-name $AZR_STORAGE_ACCOUNT_NAME \\ --zone-name privatelink.file.core.windows.net \\ --resource-group $AZR_RESOURCE_GROUP \\ -a $PRIVATE_IP test private endpoint connectivity on a Vm in the vnet run nslookup $AZR_STORAGE_ACCOUNT_NAME.file.core.windows.net Should return: Server:\tx.x.x.x Address:\tx.x.x.x#x Non-authoritative answer: \u003cstorage_account_name\u003e.file.core.windows.net\tcanonical name = \u003cstorage_account_name\u003e.privatelink.file.core.windows.net. Name:\t\u003cstorage_account_name\u003e.privatelink.file.core.windows.net Address: x.x.x.x Configure Cluster Storage Resources Login to your cluster\nCreate a secret object containing azure file creds\nAZR_STORAGE_KEY=$(az storage account keys list --account-name $AZR_STORAGE_ACCOUNT_NAME --query \"[0].value\") oc create secret generic $OC_STORAGE_ACCOUNT_SECRET_NAME --from-literal=azurestorageaccountname=$AZR_STORAGE_ACCOUNT_NAME --from-literal=azurestorageaccountkey=$AZR_STORAGE_KEY Create a custom storage class The CSI can either create volumes in pre created storage accounts or dynamically create the storage account with a volume inside the dynamic storage account\nUsing an existing storage account\nallowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \u003cstatic_sc_name\u003e parameters: resourceGroup: \u003ccluster_resource_group\u003e server: \u003cstorage_account\u003e.privatelink.file.core.windows.net skuName: Standard_LRS storageAccount: \u003cstorage_account\u003e secretName: \u003csecret_name\u003e secretNamespace: default shareName: \u003cfile_share_name\u003e provisioner: file.csi.azure.com reclaimPolicy: Delete volumeBindingMode: Immediate Configure so the provisioner dynamically creates the Storage Account in Azure\nallowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: \u003cdynamic_sc_name\u003e parameters: resourceGroup: \u003ccluster-resource-group\u003e skuName: Standard_LRS secretName: \u003csecret_name\u003e secretNamespace: default networkEndpointType: privateEndpoint provisioner: file.csi.azure.com reclaimPolicy: Delete volumeBindingMode: Immediate create PVC object that maps to the PV created PVCs are scoped at the namespace level so make sure you are creating this volume claim in the appropriate project apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"\u003cclaim_name\u003e\" spec: accessModes: - \"ReadWriteOnce\" resources: requests: storage: \"5Gi\" storageClassName: \u003cstorage_class_name\u003e Mount Azure file share in pod create pod that mounts existing pv optionally patch or create the Mount Path in the containers block of a deployment manifest as well as the PVC object in the volumes block apiVersion: v1 kind: Pod metadata: name: pod-name spec: containers: ... volumeMounts: - mountPath: \"/data\" name: \u003cname_your_volume\u003e volumes: - name: \u003cname_your_volume\u003e persistentVolumeClaim: claimName: \u003cclaim_name\u003e Testing Exec into a pod with the mounted volume oc exec -it \u003cpod_name\u003e -- /bin/bash Create a file in the file share’s mount path cd \u003cfile_share_mount_path\u003e touch test In your Azure portal or using the CLI, verify the created file exists in your Storage Account’s File Share Azure portal\nSearch for Storage Account or find the Storage Account icon on the services blade Find your storage account select File Share Open your File Share and verify the ’test’ file is in the file share CLI\naz storage file list --account-name $AZR_STORAGE_ACCOUNT_NAME --account-key $AZR_STORAGE_KEY --share-name $AZR_FILE_SHARE ","description":"","tags":["ARO","Azure"],"title":"Configure a Private ARO cluster with Azure File via a Private Endpoint","uri":"/docs/aro/private_endpoint/"},{"content":"","description":"","tags":null,"title":"AWS","uri":"/tags/aws/"},{"content":"Many organizations want to use GitOps methodologies as a main part of their operational practices. Often times, this includes infrastructure as well. The advantage to this practice is that anything controlled in this manner can exist as infrastructure-as-code, by way of Kubernetes YAML definitions, in a centralized repository backed by Git. Additionally, all processes and procedures become a part of the Git workflow with a standardized Continuous Deployment pipeline controlling the outcome. ROSA is not immune to these practices, however, there was not a native way to control ROSA clusters with Kubernetes YAML definitions. This struck up a community project called OCM Operator as a way to accomplish this.\nThis walkthrough takes a standard approach by first showing a user how to use the native OCM Operator CRDs and later guides a user that wants to use GitOps practices to control ROSA components such as Clusters, Machine Pools and Identity Providers.\nPrerequisites ROSA Cluster rosa aws oc jq Set Environment This step sets needed environment variables that are necessary to continue the walkthrough:\nAWS_ACCOUNT_ID: the AWS account ID where the ROSA cluster will be provisioned. ROSA_CLUSTER_NAME: the ROSA cluster name that the OCM operator will be installed upon. ROSA_USER_ROLE: the user role that is associated with the cluster. This is a mandatory input to the provisioning process. OCM_OPERATOR_NAMESPACE: the namespace where the ocm-operator will be installed. OCM_OPERATOR_VERSION: the version of ocm-operator that will be installed. export AWS_ACCOUNT_ID=111111111111 export ROSA_CLUSTER_NAME=dscott export ROSA_USER_ROLE=\"arn:aws:iam::${AWS_ACCOUNT_ID}:role/ManagedOpenShift-User-dscott_mobb-Role\" export OCM_OPERATOR_NAMESPACE=ocm-operator export OCM_OPERATOR_VERSION=v0.1.0 Install the OCM Operator Retrieve and Store the OCM Token Create a namespace where you wish to install the operator: oc new-project $OCM_OPERATOR_NAMESPACE Create a secret containing the OCM_TOKEN. This token can be obtained form https://console.redhat.com/openshift/token and is used by the operator to authenticate against the OCM API. This token must exist in the same namespace that the operator is running and be named ocm-token. It also expects the key to be called OCM_TOKEN as the operator is expecting this value as an environment variable. Be sure to substitute ${MY_OCM_TOKEN} with your token in the below command! oc create secret generic ocm-token \\ --namespace=$OCM_OPERATOR_NAMESPACE \\ --from-literal=OCM_TOKEN=${MY_OCM_TOKEN} Create the IAM Policies and Roles The operator will need to elevate privileges in order to perform things like creating the operator-roles for the clusters. Because of this, the operator must have a specific role created to allow it these permissions. In each instance, it is a best practice to create a new set of policies and roles for each instance of the OCM Operator. Policies and roles are prefixed with the ROSA_CLUSTER_NAME environment variable that is specified below.\nNOTE: please understand what you are doing if you deviate from the known good policies. If errors or more stringent security lockdowns are found, please submit a PR so that we can get this fixed.\nDownload, review and make the script executable, and finally run the script to create the required policies and roles. This creates a a policy for the operator, and a role which allows the operator to assume a role against the OIDC identity of the ROSA cluster. If the policies and roles already exist (prefixed by your cluster name), then the creation of them is skipped: # download curl -s https://raw.githubusercontent.com/rh-mobb/ocm-operator/main/test/scripts/generate-iam.sh \u003e ./ocm-operator-policies.sh # review cat ./ocm-operator-policies.sh # make executable and run chmod +x ./ocm-operator-policies.sh \u0026\u0026 ./ocm-operator-policies.sh As an alternative to the above, if you prefer Terraform, you can create the roles using Terraform using this example:\ncat \u003c\u003cEOF \u003e main.tf variable \"oidc_provider_url\" { type = string } variable \"cluster_name\" { type = string } module \"ocm_operator_iam\" { source = \"git::https://github.com/rh-mobb/ocm-operator//test/terraform?ref=main\" oidc_provider_url = var.oidc_provider_url ocm_operator_iam_prefix = var.cluster_name } output \"ocm_operator_iam\" { value = module.ocm_operator_iam } EOF terraform init terraform plan -out ocm.plan -var=\"oidc_provider_url=$(rosa describe cluster -c $ROSA_CLUSTER_NAME -o json | jq -r '.aws.sts.oidc_endpoint_url')\" -var=cluster_name=$ROSA_CLUSTER_NAME terraform apply \"ocm.plan\" Create the secret containing the assume role credentials: cat \u003c\u003cEOF \u003e /tmp/credentials [default] role_arn = arn:aws:iam::$AWS_ACCOUNT_ID:role/$ROSA_CLUSTER_NAME-OCMOperator web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc create secret generic aws-credentials \\ --namespace=$OCM_OPERATOR_NAMESPACE \\ --from-file=credentials=/tmp/credentials Install the Operator This step installs the OCM operator. The OCM operator will use the OCM token from the previous step to authenticate against OCM, as well as your AWS credentials file to be able to assume the previous role you created in order to authenticate against the AWS API:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: ocm-operator namespace: $OCM_OPERATOR_NAMESPACE spec: targetNamespaces: - $OCM_OPERATOR_NAMESPACE --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ocm-operator namespace: $OCM_OPERATOR_NAMESPACE spec: channel: alpha installPlanApproval: Automatic name: ocm-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: ocm-operator.$OCM_OPERATOR_VERSION EOF You have successfully installed the OCM operator, which now exists in OperatorHub!\nProvision a ROSA Cluster via Custom Resource Definition Documentation: oc explain rosacluster.spec Examples: https://github.com/rh-mobb/ocm-operator/tree/main/config/samples/cluster This shows you how to install a ROSA Cluster directly via CRD. This is important to understand the workflow and inputs to provisioning a cluster via GitOps. You may safely proceed to the GitOps provisioning step if you already understand this or would like to move directly to GitOps for provisioning objects.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: ocm.mobb.redhat.com/v1alpha1 kind: ROSACluster metadata: name: rosa-crd namespace: $OCM_OPERATOR_NAMESPACE spec: accountID: \"${AWS_ACCOUNT_ID}\" tags: owner: dscott iam: userRole: $ROSA_USER_ROLE defaultMachinePool: minimumNodesPerZone: 2 instanceType: m5.xlarge EOF You can view the status of the cluster installation in OCM .\nProvision a Machine Pool via Custom Resource Definition Documentation: oc explain machinepool.spec Examples: https://github.com/rh-mobb/ocm-operator/tree/main/config/samples/machinepool This shows you how to install a ROSA Cluster Machine Pool directly via CRD. This is important to understand the workflow and inputs to provisioning a cluster via GitOps. You may safely proceed to the GitOps provisioning step if you already understand this or would like to move directly to GitOps for provisioning objects.\nIMPORTANT This relies upon a fully operational ROSA cluster. If you are following along and just provisioned a ROSA cluster via CRD then you must wait for the cluster to be ready before proceeding.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: ocm.mobb.redhat.com/v1alpha1 kind: MachinePool metadata: name: rosa-crd-mp namespace: $OCM_OPERATOR_NAMESPACE spec: wait: false clusterName: \"rosa-crd\" minimumNodesPerZone: 1 maximumNodesPerZone: 1 instanceType: m5.xlarge EOF You can view the status of the Machine Pool by navigating to the Machine Pools tab of your cluster in OCM .\nProvision an Identity Provider via Custom Resource Definition Documentation: LDAP: oc explain ldapidentityprovider.spec GitLab: oc explain gitlabidentityprovider.spec Examples: https://github.com/rh-mobb/ocm-operator/tree/main/config/samples/identityprovider This shows you how to install a ROSA Cluster Identity Provider directly via CRD. This is important to understand the workflow and inputs to provisioning a cluster via GitOps. You may safely proceed to the GitOps provisioning step if you already understand this or would like to move directly to GitOps for provisioning objects.\nIt is important to note that, at this time, only the following Identity Providers are supported:\nLDAP GitLab Other identity providers have been requested and will be developed as needed. Please open an issue if your needed identity provider is missing. The example below shows an LDAP Identity provider but the mechanics are the same using a GitLab Identity Provider.\nIMPORTANT This relies upon a fully operational ROSA cluster. If you are following along and just provisioned a ROSA cluster via CRD then you must wait for the cluster to be ready before proceeding.\nFirst, create the secret which contains your LDAP credentials:\noc create secret generic ldap-crd \\ --namespace=$OCM_OPERATOR_NAMESPACE \\ --from-literal=bindPassword=${MY_BIND_PASSWORD} Next, create the LDAPIdentityProvider resource:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: ocm.mobb.redhat.com/v1alpha1 kind: LDAPIdentityProvider metadata: name: ldap-crd namespace: $OCM_OPERATOR_NAMESPACE spec: clusterName: rosa-crd displayName: ldap-test mappingMethod: claim url: ldap://test.example.com:389 bindDN: CN=test,OU=Users,DC=example,DC=com bindPassword: name: ldap-crd attributes: {} EOF You can view the your configured LDAP Identity Provider by navigating to the Access Control tab of your cluster in OCM and selecting Edit under your Identity Provider.\nProvision Objects via GitOps Install and Configure the OpenShift GitOps Operator Install the OpenShift GitOps operator:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-gitops-operator namespace: openshift-operators spec: channel: latest installPlanApproval: Automatic name: openshift-gitops-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: openshift-gitops-operator.v1.9.0 EOF Allow the GitOps operator to manage the OCM_OPERATOR_NAMESPACE namespace resources:\noc label ns $OCM_OPERATOR_NAMESPACE argocd.argoproj.io/managed-by=openshift-gitops Deploy the Application First, create the secret which contains your LDAP credentials, which will be used to provision the Identity Provider via GitOps. We do this first via a GitOps workflow to prevent storing secure information in a GitOps repository:\nNOTE There are other ways to handle this such as the External Secrets Operator or other projects like Sealed Secrets , however for simplicity for this walkthrough, we are going to pre-create the secret for this workflow.\noc create secret generic ldap-gitops \\ --namespace=$OCM_OPERATOR_NAMESPACE \\ --from-literal=bindPassword=${MY_BIND_PASSWORD} Next, view the upstream manifests in the demo repo .\nThese are the manifests that will be controlled via the GitOps definition.\nFinally, submit the Application resource which will tell the GitOps operator how to deploy the application. In this case, we are deploying a set of manifests that are used to control various OCM objects via our OCM operator.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: rosa-gitops namespace: openshift-gitops spec: project: default source: repoURL: https://github.com/rh-mobb/demos.git targetRevision: HEAD path: gitops/ocm_operator destination: server: https://kubernetes.default.svc namespace: $OCM_OPERATOR_NAMESPACE syncPolicy: automated: selfHeal: true prune: true EOF NOTE The cluster will appear in OCM. Be sure to understand that it will take a full cluster provision for the other objects such as the Identity Provider and the Machine Pools to be configured.\nList View:\nCluster View:\n","description":"","tags":["AWS","ROSA","GitOps","OCM"],"title":"Creating ROSA Components with GitOps","uri":"/docs/rosa/rosa-gitops/"},{"content":"","description":"","tags":null,"title":"GitOps","uri":"/tags/gitops/"},{"content":"","description":"","tags":null,"title":"OCM","uri":"/tags/ocm/"},{"content":"This guide demonstrates how to install and configure Red Hat SSO (Keycloak) into an Azure Red Hat OpenShift (ARO) cluster. It will also also configure the ARO cluster to use the SSO server as a mechanism to login by way of the OIDC protocol. In addition, Red Hat SSO can federate user identities with other identity providers. We will use Azure AD as an additional identity provider to show how this could be done.\nThis guide will walk through the following steps:\nInstall Red Hat SSO into an ARO cluster Configure Azure AD Configure Azure AD as an identity provider in Red Hat SSO Integrate ARO with Red Hat SSO for authentication Before you Begin Please review the Official Red Hat SSO Operator Documentation on this topic. Please ensure you have stood up an ARO cluster. See the Quick Start for to get started if needed. NOTE: there is also a Keycloak operator which is community supported. This is not covered as part of this documentation, however if desired, you may use the Keycloak operator instead. Please understand that the Keycloak operator does not have the same functionality and offers different API schemas and a different set of custom resources. For more information on the Keycloak operator, please visit https://www.keycloak.org/operator/installation .\nSet the Environment Set the environment variables needed in the following steps:\nSSO_NAMESPACE=keycloak AZ_REGION=eastus AZ_RESOURCE_GROUP=dscott-sso-rg AZ_VNET=dscott-sso-aro-vnet-eastus AZ_PRIVATE_ENDPOINT_SUBNET=dscott-sso-aro-machine-subnet-eastus AZ_CLUSTER_NAME=dscott-sso AZ_CONSOLE=$(az aro show -n $AZ_CLUSTER_NAME -g $AZ_RESOURCE_GROUP --query consoleProfile.url -o tsv) PG_USER=dscott PG_PASS='P@ssword1234' Deploy the Operator Create a project where your operator will be installed to:\noc new-project $SSO_NAMESPACE To install, first create an Operator Group for the operator:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: keycloak-operator namespace: $SSO_NAMESPACE spec: targetNamespaces: - $SSO_NAMESPACE EOF Next, install the subscription:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: rhsso-operator namespace: $SSO_NAMESPACE spec: channel: stable installPlanApproval: Automatic name: rhsso-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: rhsso-operator.7.6.3-opr-002 EOF Deploy the Database (Optional) It may be desirable to keep the database management to include backups and persistence outside of the OpenShift cluster. If this is a desired configuration, then you can create an Azure Postgres DB instance to store your Red Hat SSO data.\nFor reference, please see https://learn.microsoft.com/en-us/azure/postgresql/single-server/how-to-configure-privatelink-cli .\nFirst, create the database:\nNOTE Below is only a sample, be sure to replace arguments with your desired options.\naz postgres server create \\ -l $AZ_REGION \\ -g $AZ_RESOURCE_GROUP \\ -n $AZ_CLUSTER_NAME \\ -u $PG_USER \\ -p \"$PG_PASS\" \\ --sku-name GP_Gen5_2 \\ --ssl-enforcement Disabled \\ --public-network-access Disabled \\ --backup-retention 10 \\ --geo-redundant-backup Disabled \\ --storage-size 10240 \\ --version 11 Create a private endpoint to restrict network access to private connectivity only:\naz network private-endpoint create \\ --name $AZ_CLUSTER_NAME-pgsql \\ --resource-group $AZ_RESOURCE_GROUP \\ --vnet-name $AZ_VNET \\ --subnet $AZ_PRIVATE_ENDPOINT_SUBNET \\ --private-connection-resource-id $(az resource show -g $AZ_RESOURCE_GROUP -n $AZ_CLUSTER_NAME --resource-type \"Microsoft.DBforPostgreSQL/servers\" --query \"id\" -o tsv) \\ --group-id postgresqlServer \\ --connection-name $AZ_CLUSTER_NAME Configure the private DNS zone for the private link connection:\naz network private-dns zone create \\ --resource-group $AZ_RESOURCE_GROUP \\ --name \"privatelink.postgres.database.azure.com\" az network private-dns link vnet create \\ --resource-group $AZ_RESOURCE_GROUP \\ --zone-name \"privatelink.postgres.database.azure.com\" \\ --name $AZ_CLUSTER_NAME-pgsql \\ --virtual-network $AZ_VNET \\ --registration-enabled false Retrieve the private IP from the private link connection:\nPRIVATE_IP=`az resource show \\ --ids $(az network private-endpoint show --name $AZ_CLUSTER_NAME-pgsql --resource-group $AZ_RESOURCE_GROUP --query 'networkInterfaces[0].id' -o tsv) \\ --api-version 2019-04-01 \\ -o json | jq -r '.properties.ipConfigurations[0].properties.privateIPAddress'` Create the DNS records for the private link connection:\naz network private-dns record-set a create \\ --name $AZ_CLUSTER_NAME-pgsql \\ --zone-name privatelink.postgres.database.azure.com \\ --resource-group $AZ_RESOURCE_GROUP az network private-dns record-set a add-record \\ --record-set-name $AZ_CLUSTER_NAME-pgsql \\ --zone-name privatelink.postgres.database.azure.com \\ --resource-group $AZ_RESOURCE_GROUP \\ -a $PRIVATE_IP Deploy the Server With External Database If you deployed an external database , then you can create the server and use the existing database. Otherwise proceed to creating the server with an internal database .\nCreate the secret with the credential information: cat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: keycloak-db-secret namespace: $SSO_NAMESPACE stringData: POSTGRES_DATABASE: \"postgres\" POSTGRES_EXTERNAL_ADDRESS: \"$AZ_CLUSTER_NAME-pgsql.privatelink.postgres.database.azure.com\" POSTGRES_EXTERNAL_PORT: \"5432\" POSTGRES_HOST: \"keycloak-postgresql\" POSTGRES_PASSWORD: \"$PG_PASS\" POSTGRES_SUPERUSER: \"true\" POSTGRES_USERNAME: \"$PG_USER@$AZ_CLUSTER_NAME\" type: Opaque EOF Deploy the server: cat \u003c\u003cEOF | oc apply -f - apiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: name: keycloak namespace: $SSO_NAMESPACE labels: app: sso spec: instances: 2 externalAccess: enabled: True externalDatabase: enabled: true EOF With Internal Database Be sure to skip this step if you deployed an external database .\nIf you are deploying the server using an internal database, you can simply deploy the resource as follows which will create the server as well as an instance of PostgreSQL within your cluster as the backend database for the server:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: name: keycloak namespace: $SSO_NAMESPACE labels: app: sso spec: instances: 2 externalAccess: enabled: True EOF Retrieve Information About Red Hat SSO Installation IMPORTANT: this sets environment variables based on the installation that are needed in future steps.\nADMIN_USERNAME=$(oc -n $SSO_NAMESPACE exec -it keycloak-0 -- printenv SSO_ADMIN_USERNAME) ADMIN_PASSWORD=$(oc -n $SSO_NAMESPACE exec -it keycloak-0 -- printenv SSO_ADMIN_PASSWORD) ADMIN_CONSOLE=$(oc -n $SSO_NAMESPACE get route keycloak -o json | jq -r '.status.ingress[0].host') CALLBACK_URL_BASE=https://oauth-openshift.apps.$(az aro show -g $AZ_RESOURCE_GROUP -n $AZ_CLUSTER_NAME --query clusterProfile.domain -o tsv).$AZ_REGION.aroapp.io/oauth2callback Configure Red Hat SSO Configure Red Hat SSO as an OIDC Identity Provider for ARO (UI Method) This method walks you through configuring Red Hat SSO as an OIDC provider by using the user interface. If you would like to control your configuration via Kubernetes custom resources, please review the CRD Method instead.\nLogin to the host from the ADMIN_CONSOLE environment variable set in the previous step using the ADMIN_USERNAME and ADMIN_PASSWORD: Add a new realm by navigating to the top left corner, hovering over the Master realm (default) and clicking Add realm: Enter your desired realm name and make note of it, as it will be used in the URLs for this realm. Be sure that enabled is set to ON and then click Create: Create a new client for OIDC authentication by navigating to Clients \u003e Create and input your Client ID. Also make sure that the Client Protocol is set to openid-connect. Make note of the client ID as it will be used when configuring ARO to authenticate against the Red Hat SSO server. Once done, click Save: Configure the new client by setting the following values in the Settings tab. Be sure to Save before heading to the next step as these settings will create extra tabs in the UI for configuration: Login Theme: rh-sso Access Type: confidential Implicit Flow Enabled: ON Service Accounts Enabled: ON Valid Redirect URIs: \u003cValue of $CALLBACK_URL_BASE from above\u003e/ui. Note that the /ui is a custom unique suffix that is tied to the identity provider resource Web Origins: /* Ensure the Credentials tab has the following settings. Be sure to capture the Secret value for use in the next step: Client Authenticator: Client Id and Secret Secret: \u003cThis secret is used to configure the IDP in OpenShift\u003e. Note that the image below is redacted. Create the client secret. This is needed for the OAuth OpenShift resource to correctly create the Red Hat SSO IDP as an OIDC provider: oc -n openshift-config create secret generic red-hat-sso-ui --from-literal=clientSecret=\u003cCLIENT_SECRET_FROM_LAST_STEP\u003e Create a user for authentication by navigating to Users \u003e Add User, fill in the information and click Save. Ensure this user is Enabled: Set a password for the user in the Credentials tab for the user you created. Be sure Temporary is set to OFF and then click Set Password: Configure Red Hat SSO as an OIDC Identity Provider for ARO (CRD Method) This method walks you through configuring Red Hat SSO as an OIDC provider by using native Kubernetes CRDs. If you would like to control your configuration via the user interface instead, please review the UI Method .\nCreate the realm: cat \u003c\u003cEOF | oc apply -f - apiVersion: keycloak.org/v1alpha1 kind: KeycloakRealm metadata: name: crd namespace: $SSO_NAMESPACE labels: app: sso spec: instanceSelector: matchLabels: app: sso realm: realm: crd enabled: true # NOTE: you can set unmanaged to 'true' if you intend to manage this realm via the UI # unmanaged: true EOF Create the client: cat \u003c\u003cEOF | oc apply -f - apiVersion: keycloak.org/v1alpha1 kind: KeycloakClient metadata: name: crd namespace: $SSO_NAMESPACE labels: app: sso spec: realmSelector: matchLabels: app: sso client: clientId: aro name: aro description: \"Azure Red Hat OpenShift\" protocol: openid-connect enabled: true publicClient: false directAccessGrantsEnabled: true implicitFlowEnabled: true standardFlowEnabled: true serviceAccountsEnabled: true redirectUris: - $CALLBACK_URL_BASE/crd webOrigins: - \"/*\" defaultClientScopes: - acr - email - profile - roles - web-origins optionalClientScopes: - address - microprofile-jwt - offline_access - phone serviceAccountRealmRoles: - default-roles-crd EOF Create the client secret. This is needed for the OAuth OpenShift resource to correctly create the Red Hat SSO IDP as an OIDC provider: oc -n openshift-config create secret generic red-hat-sso-crd --from-literal=clientSecret=$(oc get secret keycloak-client-secret-crd -o json | jq -r '.data.CLIENT_SECRET' | base64 -d) Create a user for authentication: cat \u003c\u003cEOF | oc apply -f - apiVersion: keycloak.org/v1alpha1 kind: KeycloakUser metadata: name: crd namespace: $SSO_NAMESPACE labels: app: sso spec: realmSelector: matchLabels: app: sso user: enabled: true username: crd-admin firstName: CRD lastName: Admin email: crd-admin@example.com credentials: - temporary: false type: password value: 'P@ssword1234' EOF Configure OpenShift Configure OAuth Resource If you used the UI Method then you can update your OAuth resource as follows. Take note that if you have other identity providers configured here, you will want to ensure that you only add a new identity provider to the YAML rather than replace it.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: # NOTE: name needs to be lowercase as per https://access.redhat.com/solutions/6213561 # NOTE: this maps to the /ui suffix when creating the client - name: ui mappingMethod: claim type: OpenID openID: clientID: aro clientSecret: name: red-hat-sso-ui extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - preferred_username name: - name email: - email issuer: https://$ADMIN_CONSOLE/auth/realms/ui EOF If you used the CRD Method then you can update your OAuth resource as follows. Take note that if you have other identity providers configured here, you will want to ensure that you only add a new identity provider to the YAML rather than replace it.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: # NOTE: name needs to be lowercase as per https://access.redhat.com/solutions/6213561 # NOTE: this maps to the /ui suffix when creating the client - name: crd mappingMethod: claim type: OpenID openID: clientID: aro clientSecret: name: red-hat-sso-crd extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - preferred_username name: - name email: - email issuer: https://$ADMIN_CONSOLE/auth/realms/crd EOF Test Login to OpenShift IMPORTANT It takes a few minutes for the above configuration to be applied to the cluster. This is because the authentication operator has to reconfigure and reconcile the pods that are responsible for authentication to the cluster.\nIn a web browser, navigate to the Azure Red Hat OpenShift login page. You should have this value stored in the environment as the AZ_CONSOLE variable from this step .\nNOTE The below image shows both the crd and ui profiles as this documentation was put together while doing both side-by-side. You should see one or the other.\nSelect the appropriate provider, and input your username and password:\nCongratulations, you are now logged into the OpenShift Console with your user! Let’s take a look at the identities in the cluster:\noc get identities NAME IDP NAME IDP USER NAME USER NAME USER UID crd:de24ecb0-0475-42ce-b69b-bb3a80d6b633 crd de24ecb0-0475-42ce-b69b-bb3a80d6b633 crd-admin 88871fcc-74d1-486f-99ca-8dae17f72834 ui:7937fa9c-6231-42e0-8cd4-666990baeef7 ui 7937fa9c-6231-42e0-8cd4-666990baeef7 ui-admin 1a6a8150-5fb7-4031-a00c-5ab0a2c8c2b2 oc get users NAME UID FULL NAME IDENTITIES crd-admin 88871fcc-74d1-486f-99ca-8dae17f72834 CRD Admin crd:de24ecb0-0475-42ce-b69b-bb3a80d6b633 ui-admin 1a6a8150-5fb7-4031-a00c-5ab0a2c8c2b2 UI Admin ui:7937fa9c-6231-42e0-8cd4-666990baeef7 Just a reminder that you only have Developer permissions when logged in at this point. If you need this user to have elevated permissions within OpenShift, you can use standard Kubernetes RBAC procedures to assign permissions to your user.\nConfigure Azure AD Login to the Red Hat SSO UI, navigate to your realm, and select Identity Providers \u003e Add Provider \u003e Microsoft: Capture the Redirect URI from this screen. You will need it to register Red Hat SSO as an application in Azure AD. Once you have captured the URI, leave this screen up while we configure Azure AD: Register an application in Azure AD for Red Hat SSO .\nBe sure to use the redirect URI captured in the last step when setting this up. Also be sure to take note of the client ID and client secret as those will be used to finish configuration in Red Hat SSO.\nEnable optional claims and enable necessary Microsoft Graph permissions .\nInput the client ID and client secret from the application registration into the identity provider:\nTest Login to OpenShift with Azure AD Credentials In a web browser, navigate to the Azure Red Hat OpenShift login page. You should have this value stored in the environment as the AZ_CONSOLE variable from this step .\nNOTE The below image shows both the crd and ui profiles as this documentation was put together while doing both side-by-side. You should see one or the other.\nSelect the appropriate provider. You should now see an option to login with your Microsoft credentials:\nCongratulations, you are now logged into the OpenShift Console with your Azure AD user, using Red Hat SSO as a federated identity source! Let’s take a look at the identities in the cluster:\noc get identities NAME IDP NAME IDP USER NAME USER NAME USER UID crd:de24ecb0-0475-42ce-b69b-bb3a80d6b633 crd de24ecb0-0475-42ce-b69b-bb3a80d6b633 crd-admin 88871fcc-74d1-486f-99ca-8dae17f72834 ui:7937fa9c-6231-42e0-8cd4-666990baeef7 ui 7937fa9c-6231-42e0-8cd4-666990baeef7 ui-admin 1a6a8150-5fb7-4031-a00c-5ab0a2c8c2b2 sso:2d9a2753-f069-4286-9f1d-f1cfb80b0857 ui 2d9a2753-f069-4286-9f1d-f1cfb80b0857 dustin@mydomain.onmicrosoft.com c856d652-b865-4c03-82a2-24fc429d08be oc get users NAME UID FULL NAME IDENTITIES crd-admin 88871fcc-74d1-486f-99ca-8dae17f72834 CRD Admin crd:de24ecb0-0475-42ce-b69b-bb3a80d6b633 ui-admin 1a6a8150-5fb7-4031-a00c-5ab0a2c8c2b2 UI Admin ui:7937fa9c-6231-42e0-8cd4-666990baeef7 dustin@mydomain.onmicrosoft.com c856d652-b865-4c03-82a2-24fc429d08be Dustin Scott ui:2d9a2753-f069-4286-9f1d-f1cfb80b0857 Just a reminder that you only have Developer permissions when logged in at this point. If you need this user to have elevated permissions within OpenShift, you can use standard Kubernetes RBAC procedures to assign permissions to your user.\n","description":"","tags":["Azure","SSO","Keycloak"],"title":"Configure Red Hat SSO with Azure AD as a Federated Identity Provider","uri":"/docs/idp/azuread-red-hat-sso/"},{"content":"","description":"","tags":null,"title":"Keycloak","uri":"/tags/keycloak/"},{"content":"","description":"","tags":null,"title":"SSO","uri":"/tags/sso/"},{"content":"This guide describes how configure and deploy an Azure Container Registry, limiting the access to the registry and connecting privately from a Private ARO cluster, eliminating exposure from the public internet.\nYou can limit access to the ACR by assigning virtual network private IP addresses to the registry endpoints and using Azure Private Link .\nNetwork traffic between the Private ARO cluster and the registry’s private endpoints traverses the virtual network and a private link on the Microsoft backbone network, eliminating exposure from the public internet.\nNOTE: If you are interested in deploy and integrate an ACR with a public endpoint and connect them into an ARO cluster follow the How-to Use ACR with ARO guide .\nPrepare your ARO cluster Deploy a Private ARO cluster Set some environment variables\nexport NAMESPACE=aro-acr export AZR_CLUSTER=aro-mobb export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=aro-mobb-rg export ACR_NAME=acr$((RANDOM)) export PRIVATEENDPOINTSUBNET_PREFIX=\"10.0.8.0/23\" export PRIVATEENDPOINTSUBNET_NAME=\"PrivateEndpoint-subnet\" export ARO_VNET_NAME=\"aro-mobb-vnet\" Create ACR and restrict the access using Private Endpoint You can limit access to the ACR instance by assigning virtual network private IP addresses to the registry endpoints and using Azure Private Link.\nNetwork traffic between the clients on the virtual network and the registry’s private endpoints traverses the virtual network and a private link on the Microsoft backbone network, eliminating exposure from the public internet. Private Link also enables private registry access from on-premises through Azure ExpressRoute private peering or a VPN gateway.\nRegister the resource provider for Azure Container Registry in your subscription: az provider register --namespace Microsoft.ContainerRegistry Create PrivateEndpoint-subnet for allocate the ACR PrivateEndpoint resources (among others): az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name $ARO_VNET_NAME \\ --name $PRIVATEENDPOINTSUBNET_NAME \\ --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \\ --disable-private-endpoint-network-policies NOTE: Disable network policies such as network security groups in the subnet for the private endpoint it’s needed for the integration with Private Endpoint in this scenario.\nCreate the Azure Container Registry disabling the public network access for the container registry: az acr create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $ACR_NAME \\ --sku Premium \\ --public-network-enabled false \\ --admin-enabled true Create a private Azure DNS zone for the private Azure container registry domain: az network private-dns zone create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name 'privatelink.azurecr.io' NOTE: To use a private zone to override the default DNS resolution for your Azure container registry, the zone must be named privatelink.azurecr.io.\nAssociate your private zone with the virtual network: az network private-dns link vnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name 'AcrDNSLink' \\ --zone-name 'privatelink.azurecr.io' \\ --virtual-network $ARO_VNET_NAME \\ --registration-enabled false Get the resource ID of your registry: REGISTRY_ID=$(az acr show -n $ACR_NAME --query 'id' -o tsv) Create the registry’s private endpoint in the virtual network: az network private-endpoint create \\ --name 'acrPvtEndpoint' \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name $ARO_VNET_NAME \\ --subnet $PRIVATEENDPOINTSUBNET_NAME \\ --private-connection-resource-id $REGISTRY_ID \\ --group-id 'registry' \\ --connection-name 'acrConnection' Create a DNS zone group for a private endpoint in Azure Container Registry (ACR): az network private-endpoint dns-zone-group create \\ --name 'ACR-ZoneGroup' \\ --resource-group $AZR_RESOURCE_GROUP \\ --endpoint-name 'acrPvtEndpoint' \\ --private-dns-zone 'privatelink.azurecr.io' \\ --zone-name 'ACR' Query the Private Endpoint for the Network Interface ID: NETWORK_INTERFACE_ID=$(az network private-endpoint show \\ --name 'acrPvtEndpoint' \\ --resource-group $AZR_RESOURCE_GROUP \\ --query 'networkInterfaces[0].id' \\ --output tsv) Get the FQDN of the ACR: REGISTRY_FQDN=$(az network nic show \\ --ids $NETWORK_INTERFACE_ID \\ --query \"ipConfigurations[?privateLinkConnectionProperties.requiredMemberName=='registry'].privateLinkConnectionProperties.fqdns\" \\ --output tsv) Get the Private IP address of the ACR: REGISTRY_PRIVATE_IP=$(az network nic show \\ --ids $NETWORK_INTERFACE_ID \\ --query \"ipConfigurations[?privateLinkConnectionProperties.requiredMemberName=='registry'].privateIPAddress\" \\ -o tsv) You can nslookup the FQDN to check that the record it’s propagated properly, and answers with the privatelink one: nslookup $REGISTRY_FQDN Get the Username and Password for login to the ACR instance: ACR_USER=$(az acr credential show -n $ACR_NAME --query \"username\" -o tsv) ACR_PASS=$(az acr credential show -n $ACR_NAME --query \"passwords[0].value\" -o tsv) Try to login with podman or docker to the registry outside of the vNET: podman login --username $ACR_USER $REGISTRY_FQDN NOTE: you will receive an error, that it’s what we’re expecting, because the access to the ACR it’s restricted outside of the vNET (peering or VPN/ER needs to be used).\nGet (and save) the ARO_URL and the KUBEADMIN password: ARO_KUBEPASS=$(az aro list-credentials --name $AZR_CLUSTER --resource-group $AZR_RESOURCE_GROUP -o tsv --query kubeadminPassword) ARO_URL=$(az aro show -g $AZR_RESOURCE_GROUP -n $AZR_CLUSTER --query apiserverProfile.url -o tsv) Automation with Terraform (Optional) If you want to deploy everything on this blog post automated, clone the rh-mobb terraform-aro repo and deploy it:\ngit clone https://github.com/rh-mobb/terraform-aro.git cd terraform-aro terraform init terraform plan -out aro.plan \\ -var \"cluster_name=aro-$(shell whoami)\" \\ -var \"restrict_egress_traffic=true\"\t\\ -var \"api_server_profile=Private\" \\ -var \"ingress_profile=Private\" \\ -var \"acr_private=true\" terraform apply aro.plan Testing the Azure Container Registry from the Private ARO cluster Once we have deployed the ACR, we need to test the ACR instance deployed, and limited the access only from within the vNET (or using peering, VPN or ExpressRoute connectivity).\nSSH to the JUMPHOST to be able to test and push a example image: export JUMPHOST=\"xxx\" ssh -l aro $JUMPHOST Inside of the JUMPHOST (within the vNET) install oc and docker/podman: sudo dnf update -y --disablerepo=* --enablerepo='*microsoft*' rhui-azure-rhel8-eus sudo dnf install telnet wget bash-completion podman -y wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz tar -xvf openshift-client-linux.tar.gz sudo mv oc kubectl /usr/bin/ oc completion bash \u003e oc_bash_completion sudo cp oc_bash_completion /etc/bash_completion.d/ Login to the registry (this time should work): export REGISTRY_FQDN=\"xxx\" export ACR_USER=\"xxx\" export ARO_URL=\"xxx\" podman login --username $ACR_USER $REGISTRY_FQDN Push an example image to the ACR: podman pull quay.io/centos7/httpd-24-centos7 podman tag quay.io/centos7/httpd-24-centos7 $REGISTRY_FQDN/centos7/httpd-24-centos7 podman push $REGISTRY_FQDN/centos7/httpd-24-centos7 Login to the Private ARO cluster and create a test namespace: oc login --username kubeadmin --server=$ARO_URL oc new-project test-acr Create the Kubernetes secret for storing the credentials to access the ACR inside of the ARO cluster: oc create -n test-acr secret docker-registry \\ --docker-server=$REGISTRY_FQDN \\ --docker-username=$ACR_USER \\ --docker-password=******** \\ --docker-email=unused \\ acr-secret Link the secret to the service account: oc secrets link default acr-secret --for=pull Deploy an example app using the ACR container image pushed in the previous step: oc create -n test-acr deployment httpd --image=$REGISTRY_FQDN/centos7/httpd-24-centos7 After a couple of minutes, check the status of the pod: oc get pod -n test-acr It should work, deploying the container image in the Private ARO cluster.\n","description":"","tags":["ARO","Azure"],"title":"Using Azure Container Registry in Private ARO clusters","uri":"/docs/aro/aro-acr/"},{"content":"The AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in Secrets Manager and then retrieve them through your workloads running on ROSA or OSD.\nThis is made even easier and more secure through the use of AWS STS and Kubernetes PodIdentity.\nPrerequisites A ROSA cluster deployed with STS Helm 3 aws CLI oc CLI jq Preparing Environment Validate that your cluster has STS\noc get authentication.config.openshift.io cluster -o json \\ | jq .spec.serviceAccountIssuer You should see something like the following, if not you should not proceed, instead look to the Red Hat documentation on creating an STS cluster .\n\"https://xxxxx.cloudfront.net/xxxxx\" Set SecurityContextConstraints to allow the CSI driver to run\noc new-project csi-secrets-store oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:secrets-store-csi-driver oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws Create some environment variables to refer to later\nexport REGION=us-east-2 export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster \\ -o jsonpath='{.spec.serviceAccountIssuer}' | sed 's|^https://||') export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" Deploy the AWS Secrets and Configuration Provider Use Helm to register the secrets store csi driver\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm upgrade --install -n csi-secrets-store \\ csi-secrets-store-driver secrets-store-csi-driver/secrets-store-csi-driver Deploy the AWS provider\noc -n csi-secrets-store apply -f \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/content/docs/misc/secrets-store-csi/aws-provider-installer.yaml Check that both Daemonsets are running\noc -n csi-secrets-store get ds \\ csi-secrets-store-provider-aws \\ csi-secrets-store-driver-secrets-store-csi-driver Creating a Secret and IAM Access Policies Create a secret in Secrets Manager\nSECRET_ARN=$(aws --region \"$REGION\" secretsmanager create-secret \\ --name MySecret --secret-string \\ '{\"username\":\"shadowman\", \"password\":\"hunter2\"}' \\ --query ARN --output text) echo $SECRET_ARN Create IAM Access Policy document\ncat \u003c\u003c EOF \u003e policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"secretsmanager:GetSecretValue\", \"secretsmanager:DescribeSecret\" ], \"Resource\": [\"$SECRET_ARN\"] }] } EOF Create an IAM Access Policy\nPOLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name openshift-access-to-mysecret-policy \\ --policy-document file://policy.json) echo $POLICY_ARN Create IAM Role trust policy document\nNote the trust policy is locked down to the default service account of a namespace you will create later.\ncat \u003c\u003cEOF \u003e trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Condition\": { \"StringEquals\" : { \"${OIDC_ENDPOINT}:sub\": [\"system:serviceaccount:my-application:default\"] } }, \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create IAM Role\nROLE_ARN=$(aws iam create-role --role-name openshift-access-to-mysecret \\ --assume-role-policy-document file://trust-policy.json \\ --query Role.Arn --output text) echo $ROLE_ARN Attach Role to the Policy\naws iam attach-role-policy --role-name openshift-access-to-mysecret \\ --policy-arn $POLICY_ARN Create an Application to use this secret Create an OpenShift project\noc new-project my-application Annotate the default service account to use the STS Role\noc annotate -n my-application serviceaccount default \\ eks.amazonaws.com/role-arn=$ROLE_ARN Create a secret provider class to access our secret\ncat \u003c\u003c EOF | oc apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: my-application-aws-secrets spec: provider: aws parameters: objects: | - objectName: \"MySecret\" objectType: \"secretsmanager\" EOF Create a Deployment using our secret\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: my-application labels: app: my-application spec: volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"my-application-aws-secrets\" containers: - name: my-application-deployment image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true EOF Verify the Pod has the secret mounted\noc exec -it my-application -- cat /mnt/secrets-store/MySecret Cleanup Delete application\noc delete project my-application Delete the secrets store csi driver\nhelm delete -n csi-secrets-store csi-secrets-store-driver Delete Security Context Constraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:csi-secrets-store:secrets-store-csi-driver oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws Delete the AWS provider\noc -n csi-secrets-store delete -f \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/content/docs/misc/secrets-store-csi/aws-provider-installer.yaml Delete AWS Roles and Policies\naws iam detach-role-policy --role-name openshift-access-to-mysecret \\ --policy-arn $POLICY_ARN aws iam delete-role --role-name openshift-access-to-mysecret aws iam delete-policy --policy-arn $POLICY_ARN Delete the Secrets Manager secret\naws secretsmanager --region $REGION delete-secret --secret-id $SECRET_ARN ","description":"","tags":["AWS","ROSA"],"title":"Using AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS","uri":"/docs/rosa/aws-secrets-manager-csi/"},{"content":"","description":"","tags":null,"title":"IDP","uri":"/tags/idp/"},{"content":"Author: Ricardo Macedo Martins May 24, 2023\nIn this guide, we will discuss key considerations when using Azure Active Directory (AAD) as the Identity Provider (IDP) for your ARO or ROSA cluster. Below are some helpful references:\nConfigure ARO to Use Azure AD Configuring IDP for ROSA, OSD, and ARO Default Access for All Users in Azure Active Directory Once you set up AAD as the IDP for your cluster, it’s important to note that by default, all users in your Azure Active Directory instance will have access to the cluster. They can log in using their AAD credentials through the OpenShift Web Console endpoint:\nHowever, for security purposes, it’s recommended to restrict access and only allow specific users who are assigned to access the cluster.\nRestricting Access To implement access restrictions, follow these steps:\nLog in to the Azure Portal and navigate to your AAD instance.\nUnder Enterprise applications, select the application created for the ARO IDP configuration.\nIn the selected Enterprise application, go to Properties and switch the “Assignment required?” option to YES. If you attempt to log in at this point, you will receive a denial error: Enter your username:\nEnter your password:\nThe error message indicates that only users specifically granted access to the application are allowed:\nTo allow access, go to Users and groups in the main blade, click + Add user/group, and add the desired users/groups who should have access to the ARO cluster. Search for the desired user/group and click Select.\nVerify that the user has been assigned:\nYou should now be able to log in with the specified user/group to your cluster: Enter your username:\nEnter your password:\nYou will then be logged in:\nApproval Workflow If you receive a message like the one below, it means that your AAD has the admin consent workflow enabled:\nIn this case, you will need to request and wait for approval from your AAD domain admin. To request access, fill out the request form:\nAnd wait for approval:\nSelf-Approval Process If you have administrative privileges, you can self-approve the request by following these steps:\nPlease note that these steps are based on the official guidance from Microsoft, which is available here. Go to your Azure Active Directory Tenant \u003e Enterprise Applications \u003e Admin Consent Requests \u003e All (Preview): Select the application (openshift, in this case) and click Review permissions and consent: A new window will open, prompting you to log in with credentials of an admin with permissions: Click Accept to consent to the permission: You will then see that the request was approved:\nNow you will be able to log in through the AAD option:\nEnter your username:\nEnter your password:\nIt worked!\nAs a best practice, we recommend removing the kubeadmin user after setting up an identity provider. You can find instructions on how to do this here .\nUsing the Group Sync Operator Integrating groups from external identity providers with OpenShift, such as synchronizing groups from AAD, can be a valuable feature to enhance your system’s functionality. To accomplish this, you can leverage the usage of the Group Sync Operator .\nWe have published a comprehensive how-to guide that walks you through the process, accessible here . By following these instructions, you’ll be able to seamlessly synchronize AAD groups into your OpenShift environment, optimizing your workflow and streamlining access management.\n","description":"","tags":["Azure","IDP","ARO","ROSA"],"title":"What to consider when using Azure AD as IDP?","uri":"/docs/idp/considerations-aad-ipd/"},{"content":"This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user’s group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Azure Red Hat OpenShift (ARO) to authenticate and manage authorization using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation .\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because \u003ccode\u003ezsh\u003c/code\u003e disables comments in interactive shells from being used .\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username”, as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation .\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\nNext, select the “Add groups claim” button.\nSelect the “Security groups” option and click the “Add” button to configure group claims for your Azure AD application.\nNote: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend _scoping the groups provided by the group claim to only those groups which are applicable to OpenShift.\nGrant the admin consent for the in the API Permission section\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider.\nTo do so, ensure you are logged in to the OpenShift command line interface (oc) by running the following command, making sure to replace the variables specified:\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:\nCLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster’s OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:\nIDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat \u003c\u003c EOF \u003e cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email groups: - groups name: - name preferredUsername: - email clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: - profile - openid issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml).\nFinally, apply the new configuration to the cluster’s OAuth provider by running the following command:\noc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored.\nOnce the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). The provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\nIf you have a private cluster behind a firewall, you may get an error message like the image below when you try login into the web console using the AAD option. In this case you should open a firewall rule allowing access from the cluster to graph.microsoft.com.\nIf you are using Azure Firewall, you can run those commands to allow this access:\naz network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Microsoft_Graph' --action allow --priority 100 \\ -n 'Microsoft_Graph' --source-address '*' --protocols 'any' \\ --source-addresses '*' --destination-fqdns 'graph.microsoft.com' \\ --destination-ports '*' Now you should be able to login choosing the AAD option:\nThen inform the user you would like to use:\n4. Grant additional permissions to individual groups Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID.\nGROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access.\nFor more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .\n","description":"","tags":null,"title":"Configure ARO to use Azure AD Group Claims","uri":"/docs/idp/group-claims/aro/"},{"content":"","description":"","tags":null,"title":"ACM","uri":"/tags/acm/"},{"content":"Submariner is an open source tool that can be used with Red Hat Advanced Cluster Management for Kubernetes to provide direct networking between pods and compatible multicluster service discovery across two or more Kubernetes clusters in your environment, either on-premises or in the cloud.\nThis article describes how to deploy ACM Submariner for connecting overlay networks of ARO and ROSA clusters.\nNOTE: Submariner for connecting ARO and ROSA clusters only works from ACM 2.7 onwards!\nPrerequisites OpenShift Cluster version 4 (ROSA/ARO or non-ROSA/ARO) az cli rosa cli aws cli (optional) Manage Multiple Logins In order to manage several clusters, we will add a new Kubeconfig file to manage the logins and change quickly from one context to another: rm -rf /var/tmp/acm-lab-kubeconfig touch /var/tmp/acm-lab-kubeconfig export KUBECONFIG=/var/tmp/acm-lab-kubeconfig Deploy ACM Cluster HUB We will use the first OpenShift cluster to deploy ACM Hub.\nLogin into the HUB OpenShift cluster and set the proper context oc login --username xxx --password xxx --server=https://api.cluster-xxx.xxx.xxx.xxx.com:6443 kubectl config rename-context $(oc config current-context) hub kubectl config use hub Create the namespace for ACM cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: open-cluster-management labels: openshift.io/cluster-monitoring: \"true\" EOF Create the OperatorGroup for ACM cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: open-cluster-management namespace: open-cluster-management spec: targetNamespaces: - open-cluster-management EOF Install Operator ACM 2.7 cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: advanced-cluster-management namespace: open-cluster-management spec: channel: release-2.7 installPlanApproval: Automatic name: advanced-cluster-management source: redhat-operators sourceNamespace: openshift-marketplace EOF NOTE: you can select from ACM 2.7 onwards for install ACM Submariner for ROSA/ARO.\nCheck that the Operator has installed successfully kubectl get csv -n open-cluster-management NAME DISPLAY VERSION REPLACES PHASE advanced-cluster-management.v2.7.2 Advanced Cluster Management for Kubernetes 2.7.2 advanced-cluster-management.v2.7.1 Succeeded NOTE: ACM Submariner for ROSA clusters only works with ACM 2.7 or newer!\nInstall MultiClusterHub instance in the ACM namespace cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operator.open-cluster-management.io/v1 kind: MultiClusterHub metadata: namespace: open-cluster-management name: multiclusterhub spec: {} EOF Check that the MultiClusterHub is installed and running properly kubectl get multiclusterhub -n open-cluster-management -o json | jq '.items[0].status.phase' \"Running\" NOTE: if it’s not in Running state, wait a couple of minutes and check again.\nDeploy ROSA Cluster Define the prerequisites for install the ROSA cluster export VERSION=4.11.36 \\ ROSA_CLUSTER_NAME=rosa-sbmr1 \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=eu-west-1 \\ AWS_PAGER=\"\" \\ CIDR=\"10.10.0.0/16\" NOTE: it’s critical that the Machine CIDR of the ROSA and ARO clusters not overlap, for that reason we’re setting different CIDRs than the out of the box ROSA / ARO cluster install.\nCreate the IAM Account Roles rosa create account-roles --mode auto --yes Generate a STS ROSA cluster rosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} \\ --machine-cidr $CIDR \\ --sts Create the Operator and OIDC Roles rosa create operator-roles --cluster ${ROSA_CLUSTER_NAME} --mode auto --yes rosa create oidc-provider --cluster ${ROSA_CLUSTER_NAME} --mode auto --yes Check the status of the Rosa cluster (40 mins wait until is in ready status) rosa describe cluster --cluster ${ROSA_CLUSTER_NAME} | grep State State: ready Set the admin user for the ROSA cluster rosa create admin --cluster=$ROSA_CLUSTER_NAME Login into the rosa cluster and set the proper context oc login https://api.rosa-sbmr1.xxx.xxx.xxx.com:6443 --username cluster-admin --password xxx kubectl config rename-context $(oc config current-context) $ROSA_CLUSTER_NAME kubectl config use $ROSA_CLUSTER_NAME kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' Generate ROSA New nodes for submariner Create new node/s that will be used to run Submariner gateway using the following command (check https://github.com/submariner-io/submariner/issues/1896 for more details) rosa create machinepool --cluster $ROSA_CLUSTER_NAME --name=sm-gw-mp --replicas=1 --labels='submariner.io/gateway=true' NOTE: setting replicas=2 means that we allocate two nodes for SM GW , to support GW Active/Passive HA (check Gateway Failover section ), if GW HA is not needed you can set replicas=1.\nCheck the machinepools requested, including the submariner machinepool requested rosa list machinepools -c $ROSA_CLUSTER_NAME ID AUTOSCALING REPLICAS INSTANCE TYPE LABELS TAINTS AVAILABILITY ZONES SPOT INSTANCES Default No 2 m5.xlarge eu-west-1a N/A sm-gw-mp No 1 m5.xlarge submariner.io/gateway=true eu-west-1a No After a couple of minutes, check the new nodes generated kubectl get nodes --show-labels | grep submariner Deploy ARO Cluster IMPORTANT: To enable Submariner in ROSA - ARO clusters, the POD_CIDR and SERVICE_CIDR can’t overlap between them. To avoid IP address conflicts, the ARO cluster needs to modify the default IP CIDRs. Check the Submariner docs for more information.\nDefine the prerequisites for install the ROSA cluster AZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=aro-sbmr2-rg AZR_CLUSTER=aro-sbmr2 AZR_PULL_SECRET=~/Downloads/pull-secret.txt POD_CIDR=\"10.132.0.0/14\" SERVICE_CIDR=\"172.31.0.0/16\" Create an Azure resource group az group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Create virtual network az network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet az network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet az network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the ARO cluster az aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --pod-cidr \"$POD_CIDR\" \\ --service-cidr \"$SERVICE_CIDR\" \\ --pull-secret @$AZR_PULL_SECRET Get ARO OpenShift API Url ARO_URL=$(az aro show -g $AZR_RESOURCE_GROUP -n $AZR_CLUSTER --query apiserverProfile.url -o tsv) Login into the ARO cluster and set context ARO_KUBEPASS=$(az aro list-credentials --name $AZR_CLUSTER --resource-group $AZR_RESOURCE_GROUP -o tsv --query kubeadminPassword) Login into the ARO cluster and set context oc login --username kubeadmin --password $ARO_KUBEPASS --server=$ARO_URL kubectl config rename-context $(oc config current-context) $AZR_CLUSTER kubectl config use $AZR_CLUSTER kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' NOTE: ARO doesn’t need to generate extra nodes to have the ACM submariner components deployed.\nCreate ManagedClusterSets Create a ManagedClusterSet for ROSA and ARO clusters kubectl config use hub kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1beta1 kind: ManagedClusterSet metadata: name: rosa-aro-clusters EOF Import ROSA cluster in ACM (CLI) We will import the cluster using the auto-import secret and using the Klusterlet Addon Config.\nIf you want to import your cluster using the RHACM UI, refer to the official Importing a managed cluster by using console documentation.\nRetrieve ROSA TOKEN the ROSA API from the ROSA cluster kubectl config use $ROSA_CLUSTER_NAME SUB1_API=$(oc whoami --show-server) echo \"$ROSA_CLUSTER_NAME API: $SUB1_API\\n\" SUB1_TOKEN=$(oc whoami -t) echo \"$ROSA_CLUSTER_NAME Token: $SUB1_TOKEN\\n\" Config the Hub as the current context kubectl config use hub kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' Create (in ACM Hub cluster) ManagedCluster object defining the cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: $ROSA_CLUSTER_NAME labels: name: $ROSA_CLUSTER_NAME cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-aro-clusters env: $ROSA_CLUSTER_NAME annotations: {} spec: hubAcceptsClient: true EOF Create (in ACM Hub cluster) auto-import-secret.yaml secret defining the the token and server from ROSA cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: auto-import-secret namespace: $ROSA_CLUSTER_NAME stringData: autoImportRetry: \"2\" token: \"${SUB1_TOKEN}\" server: \"${SUB1_API}\" type: Opaque EOF Create and apply the klusterlet add-on configuration file for the ROSA cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: agent.open-cluster-management.io/v1 kind: KlusterletAddonConfig metadata: name: $ROSA_CLUSTER_NAME namespace: $ROSA_CLUSTER_NAME spec: clusterName: $ROSA_CLUSTER_NAME clusterNamespace: $ROSA_CLUSTER_NAME clusterLabels: name: $ROSA_CLUSTER_NAME cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-aro-clusters env: $ROSA_CLUSTER_NAME applicationManager: enabled: true policyController: enabled: true searchCollector: enabled: true certPolicyController: enabled: true iamPolicyController: enabled: true EOF Check the imported cluster in ACM kubectl get ManagedCluster NAME HUB ACCEPTED MANAGED CLUSTER URLS JOINED AVAILABLE AGE local-cluster true https://api.cluster-xxxx.xxxx.xxxx.xxx.com:6443 True True 5h9m rosa-sbmr1 true https://api.rosa-subm1.xxxx.p1.openshiftapps.com:6443 True True 1m Import ARO cluster into ACM (CLI) Retrieve the ARO token and the ARO API url from the ARO cluster kubectl config use $AZR_CLUSTER SUB2_API=$(oc whoami --show-server) echo \"$AZR_CLUSTER API: $SUB2_API\\n\" SUB2_TOKEN=$(oc whoami -t) echo \"$AZR_CLUSTER Token: $SUB2_TOKEN\\n\" Config the Hub as the current context kubectl config use hub kubectl get mch -A Create (in the Hub) ManagedCluster object defining the ARO cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: $AZR_CLUSTER labels: name: $AZR_CLUSTER cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-aro-clusters env: $AZR_CLUSTER annotations: {} spec: hubAcceptsClient: true EOF Create (in the Hub) auto-import-secret.yaml secret defining the the token and server from ARO cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: auto-import-secret namespace: $AZR_CLUSTER stringData: autoImportRetry: \"2\" token: \"${SUB2_TOKEN}\" server: \"${SUB2_API}\" type: Opaque EOF cat \u003c\u003c EOF | kubectl apply -f - apiVersion: agent.open-cluster-management.io/v1 kind: KlusterletAddonConfig metadata: name: $AZR_CLUSTER namespace: $AZR_CLUSTER spec: clusterName: $AZR_CLUSTER clusterNamespace: $AZR_CLUSTER clusterLabels: Name: $AZR_CLUSTER cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-aro-clusters env: $AZR_CLUSTER applicationManager: enabled: true policyController: enabled: true searchCollector: enabled: true certPolicyController: enabled: true iamPolicyController: enabled: true EOF Review the clusters imported in ACM Check the managed clusters in ACM kubectl config use hub kubectl get managedclusters NAME HUB ACCEPTED MANAGED CLUSTER URLS JOINED AVAILABLE AGE aro-submr2 true https://api.xxxx.xxxx.xxxx:6443 True True 2m34s local-cluster true https://api.cluster-xxxx.xxxx.xxxx.xxxx.com:6443 True True 2d rosa-sbmr1 true https://api.rosa-xxxx.xxxx.p1.openshiftapps.com:6443 True True 46h Now it’s time to deploy submariner in our Managed Clusters (ROSA and ARO). Either deploy using the RHACM UI or with CLI (choose one).\nDeploy Submariner Addon in Managed ROSA and ARO clusters from the RHACM UI Inside of the ClusterSets tab, go to the rosa-aro-clusters generated.\nGo to Submariner add-ons and Click in “Install Submariner Add-Ons”\nConfigure the Submariner addons adding both ROSA and ARO clusters generated:\nThe Submariner Add-on installation will start, and will take up to 10 minutes to finish.\nDeploy Submariner Addon in Managed ROSA and ARO clusters with CLI NOTE: All of this commands are executed in the ACM Hub cluster, not in the ACM Managed Clusters (ROSA / ARO created).\nAfter the ManagedClusterSet is created, the submariner-addon creates a namespace called managed-cluster-set-name-broker and deploys the Submariner broker to it. kubectl get ns | grep broker default-broker Active 2d rosa-aro-clusters-broker Active 8m1s Create the Broker configuration on the hub cluster in the rosa-clusters-broker namespace: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: submariner.io/v1alpha1 kind: Broker metadata: name: submariner-broker namespace: rosa-aro-clusters-broker spec: globalnetEnabled: false EOF NOTE: Set the the value of globalnetEnabled to true if you want to enable Submariner Globalnet in the ManagedClusterSet.\nCheck the Submariner Broker in the rosa-clusters-broker namespace: $ kubectl get broker -n rosa-aro-clusters-broker NAME AGE submariner-broker 5s Deploy the SubmarinerConfig for the ROSA cluster imported: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: submarineraddon.open-cluster-management.io/v1alpha1 kind: SubmarinerConfig metadata: name: submariner namespace: $ROSA_CLUSTER_NAME spec: IPSecNATTPort: 4500 NATTEnable: true cableDriver: libreswan loadBalancerEnable: true EOF Deploy the SubmarinerConfig for the ARO cluster imported: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: submarineraddon.open-cluster-management.io/v1alpha1 kind: SubmarinerConfig metadata: name: submariner namespace: $AZR_CLUSTER spec: IPSecNATTPort: 4500 NATTEnable: true cableDriver: libreswan loadBalancerEnable: true EOF Deploy Submariner on the ROSA cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: addon.open-cluster-management.io/v1alpha1 kind: ManagedClusterAddOn metadata: name: submariner namespace: $ROSA_CLUSTER_NAME spec: installNamespace: submariner-operator EOF Deploy Submariner on the ARO cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: addon.open-cluster-management.io/v1alpha1 kind: ManagedClusterAddOn metadata: name: submariner namespace: $AZR_CLUSTER spec: installNamespace: submariner-operator EOF The Submariner Add-on installation will start, and will take up to 10 minutes to finish.\nCheck the Status of the Submariner Networking Add-On Few minutes (up to 10 minutes) after we can check that the app Connection Status and the Agent Status are Healthy: ","description":"","tags":["Submariner","ROSA","ARO","ACM"],"title":"Deploy ACM Submariner for connect overlay networks ARO - ROSA clusters","uri":"/docs/redhat/acm/submariner/aro/"},{"content":"Submariner is an open source tool that can be used with Red Hat Advanced Cluster Management for Kubernetes to provide direct networking between pods and compatible multicluster service discovery across two or more Kubernetes clusters in your environment, either on-premises or in the cloud.\nThis article describes how to deploy ACM Submariner for connecting ROSA clusters overlay networks.\nNOTE: ACM Submariner for ROSA clusters only works with ACM 2.7 or newer!\nPrerequisites OpenShift Cluster version 4 (ROSA or non-ROSA) rosa cli aws cli (optional) ACM 2.7 or newer Manage Multiple Logins In order to manage several clusters, we will add a new Kubeconfig file to manage the logins and change quickly from one context to another: rm -rf /var/tmp/acm-lab-kubeconfig touch /var/tmp/acm-lab-kubeconfig export KUBECONFIG=/var/tmp/acm-lab-kubeconfig Deploy ACM Cluster HUB We will use the first OpenShift cluster to deploy ACM Hub.\nLogin into the HUB OpenShift cluster and set the proper context: oc login --username xxx --password xxx --server=https://api.cluster-xxx.xxx.xxx.xxx.com:6443 kubectl config rename-context $(oc config current-context) hub kubectl config use hub Create the namespace for ACM cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: open-cluster-management labels: openshift.io/cluster-monitoring: \"true\" EOF Create the OperatorGroup for ACM cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: open-cluster-management namespace: open-cluster-management spec: targetNamespaces: - open-cluster-management EOF Install Operator ACM 2.7 cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: advanced-cluster-management namespace: open-cluster-management spec: channel: release-2.7 installPlanApproval: Automatic name: advanced-cluster-management source: redhat-operators sourceNamespace: openshift-marketplace EOF Check that the Operator has installed successfully oc get csv NAME DISPLAY VERSION REPLACES PHASE advanced-cluster-management.v2.7.0 Advanced Cluster Management for Kubernetes 2.7.0 Succeeded NOTE: ACM Submariner will only work from 2.7 onwards! Ensure that you have a \u003e= 2.7 ACM version.\nInstall MultiClusterHub instance in the ACM namespace cat \u003c\u003c EOF | kubectl apply -f - apiVersion: operator.open-cluster-management.io/v1 kind: MultiClusterHub metadata: namespace: open-cluster-management name: multiclusterhub spec: {} EOF Check that the MultiClusterHub is properly installed kubectl get mch -n open-cluster-management multiclusterhub -o jsonpath='{.status.phase}' NOTE: if it’s not in Running state, wait a couple of minutes and check again.\nDeploy First ROSA Cluster Define the prerequisites for install the ROSA cluster export VERSION=4.11.36 \\ ROSA_CLUSTER_NAME_1=rosa-sbmr1 \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=eu-west-1 \\ AWS_PAGER=\"\" \\ CIDR=\"10.0.0.0/16\" NOTE: it’s critical that the Machine CIDR of the ROSA clusters not overlap, for that reason we’re setting different CIDRs than the out of the box ROSA cluster install.\nCreate the IAM Account Roles rosa create account-roles --mode auto --yes Generate a STS ROSA cluster rosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME_1} \\ --region ${REGION} --version ${VERSION} \\ --machine-cidr $CIDR \\ --sts Create the Operator and OIDC Roles rosa create operator-roles --cluster ${ROSA_CLUSTER_NAME_1} --mode auto --yes rosa create oidc-provider --cluster ${ROSA_CLUSTER_NAME_1} --mode auto --yes Check the status of the Rosa cluster (40 mins wait until is in ready status) rosa describe cluster --cluster ${ROSA_CLUSTER_NAME_1} | grep State State: ready Set the admin user for the ROSA cluster rosa create admin --cluster=$ROSA_CLUSTER_NAME_1 Login into the rosa cluster and set the proper context oc login https://api.rosa-sbmr1.xxx.xxx.xxx.com:6443 --username cluster-admin --password xxx kubectl config rename-context $(oc config current-context) $ROSA_CLUSTER_NAME_1 kubectl config use $ROSA_CLUSTER_NAME_1 kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' Generate ROSA New nodes for submariner Create new node/s that will be used to run Submariner gateway using the following command (check the related GitHub issue for more details) rosa create machinepool --cluster $ROSA_CLUSTER_NAME_1 --name=sm-gw-mp --replicas=1 --labels='submariner.io/gateway=true' NOTE: setting replicas=2 means that we allocate two nodes for SM GW , to support GW Active/Passive HA (check Gateway Failover section ), if GW HA is not needed you can set replicas=1.\nCheck the machinepools requested, including the submariner machinepool requested rosa list machinepools -c $ROSA_CLUSTER_NAME_1 ID AUTOSCALING REPLICAS INSTANCE TYPE LABELS TAINTS AVAILABILITY ZONES SPOT INSTANCES Default No 2 m5.xlarge eu-west-1a N/A sm-gw-mp No 2 m5.xlarge submariner.io/gateway=true eu-west-1a No After a couple of minutes, check the new nodes generated kubectl get nodes --show-labels | grep submariner Deploy Second ROSA Cluster IMPORTANT: To enable Submariner in both ROSA clusters, the POD_CIDR and SERVICE_CIDR can’t overlap between them. To avoid IP address conflicts, the second ROSA cluster needs to modify the default IP CIDRs. Check the Submariner docs for more information.\nDefine the prerequisites for install the second ROSA cluster export VERSION=4.11.36 \\ ROSA_CLUSTER_NAME_2=rosa-sbmr2 \\ AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) \\ REGION=us-east-2 \\ AWS_PAGER=\"\" \\ CIDR=\"10.20.0.0/16\" \\ POD_CIDR=\"10.132.0.0/14\" \\ SERVICE_CIDR=\"172.31.0.0/16\" Create the IAM Account Roles rosa create account-roles --mode auto --yes Generate the second STS ROSA cluster (with the POD_CIDR and SERVICE_CIDR modified) rosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME_2} \\ --region ${REGION} --version ${VERSION} \\ --machine-cidr $CIDR \\ --pod-cidr $POD_CIDR \\ --service-cidr $SERVICE_CIDR \\ --sts Create the Operator and OIDC Roles rosa create operator-roles -c $ROSA_CLUSTER_NAME_2 --mode auto --yes rosa create oidc-provider -c $ROSA_CLUSTER_NAME_2 --mode auto --yes Check the status of the Rosa cluster (40 mins wait until is in ready status) rosa describe cluster --cluster ${ROSA_CLUSTER_NAME_2} | grep State State: ready Set the admin user for the ROSA cluster rosa create admin --cluster=$ROSA_CLUSTER_NAME_2 Login into the rosa cluster and set the proper context oc login https://api.rosa-sbmr2.xxx.xxx.xxx.com:6443 --username cluster-admin --password xxx kubectl config rename-context $(oc config current-context) $ROSA_CLUSTER_NAME_2 kubectl config use $ROSA_CLUSTER_NAME_2 kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' Generate ROSA New nodes for submariner Create new node/s that will be used to run Submariner gateway using the following command rosa create machinepool --cluster $ROSA_CLUSTER_NAME_2 --name=sm-gw-mp --replicas=1 --labels='submariner.io/gateway=true' Check the machinepools requested, including the submariner machinepool requested: rosa list machinepools -c $ROSA_CLUSTER_NAME_2 ID AUTOSCALING REPLICAS INSTANCE TYPE LABELS TAINTS AVAILABILITY ZONES SPOT INSTANCES Default No 2 m5.xlarge us-east-2a N/A sm-gw-mp No 2 m5.xlarge submariner.io/gateway=true us-east-2a No After a couple of minutes, check the new nodes generated kubectl get nodes --show-labels | grep submariner Create a ManagedClusterSet In the Hub (where ACM is installed), create the ManagedClusterSet for the rosa-clusters: kubectl config use hub kubectl get mch -A cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1beta1 kind: ManagedClusterSet metadata: name: rosa-clusters EOF Import ROSA Sub1 We will import the cluster using the auto-import secret and using the Klusterlet Addon Config.\nRetrieve ROSA TOKEN the ROSA API from the first ROSA cluster kubectl config use $ROSA_CLUSTER_NAME_1 SUB1_TOKEN=$(oc whoami -t) echo $SUB1_TOKEN SUB1_API=$(oc whoami --show-server) echo $SUB1_API Config the Hub as the current context kubectl config use hub kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' kubectl get mch -A Create (in the Hub) ManagedCluster object defining the rosa-subm1 cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: $ROSA_CLUSTER_NAME_1 labels: name: $ROSA_CLUSTER_NAME_1 cluster.open-cluster-management.io/clusterset: rosa-clusters annotations: {} spec: hubAcceptsClient: true EOF Create (in the Hub) auto-import-secret.yaml secret defining the the token and server from first ROSA cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: auto-import-secret namespace: $ROSA_CLUSTER_NAME_1 stringData: autoImportRetry: \"5\" token: ${SUB1_TOKEN} server: ${SUB1_API} type: Opaque EOF Create and apply the klusterlet add-on configuration file for the first rosa cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: agent.open-cluster-management.io/v1 kind: KlusterletAddonConfig metadata: name: $ROSA_CLUSTER_NAME_1 namespace: $ROSA_CLUSTER_NAME_1 spec: clusterName: $ROSA_CLUSTER_NAME_1 clusterNamespace: $ROSA_CLUSTER_NAME_1 clusterLabels: name: $ROSA_CLUSTER_NAME_1 cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-clusters applicationManager: enabled: true certPolicyController: enabled: true iamPolicyController: enabled: true policyController: enabled: true searchCollector: enabled: true EOF Import ROSA sub2 (CLI) Retrieve ROSA TOKEN the ROSA API from the second ROSA cluster kubectl config use $ROSA_CLUSTER_NAME_2 SUB2_API=$(oc whoami --show-server) echo \"$ROSA_CLUSTER_NAME_2 API: $SUB2_API\\n\" SUB2_TOKEN=$(oc whoami -t) echo \"$ROSA_CLUSTER_NAME_2 Token: $SUB2_TOKEN\\n\" Config the Hub as the current context kubectl config use hub kubectl get mch -A Create (in the Hub) ManagedCluster object defining the second ROSA cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: name: $ROSA_CLUSTER_NAME_2 labels: name: $ROSA_CLUSTER_NAME_2 cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-clusters env: $ROSA_CLUSTER_NAME_2 annotations: {} spec: hubAcceptsClient: true EOF Create (in the Hub) auto-import-secret.yaml secret defining the the token and server from second ROSA cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: auto-import-secret namespace: $ROSA_CLUSTER_NAME_2 stringData: autoImportRetry: \"2\" token: \"${SUB2_TOKEN}\" server: \"${SUB2_API}\" type: Opaque EOF Create and apply the klusterlet add-on configuration file for the second rosa cluster cat \u003c\u003c EOF | kubectl apply -f - apiVersion: agent.open-cluster-management.io/v1 kind: KlusterletAddonConfig metadata: name: $ROSA_CLUSTER_NAME_2 namespace: $ROSA_CLUSTER_NAME_2 spec: clusterName: $ROSA_CLUSTER_NAME_2 clusterNamespace: $ROSA_CLUSTER_NAME_2 clusterLabels: name: $ROSA_CLUSTER_NAME_2 cloud: auto-detect vendor: auto-detect cluster.open-cluster-management.io/clusterset: rosa-clusters env: rosa-subm2 applicationManager: enabled: true policyController: enabled: true searchCollector: enabled: true certPolicyController: enabled: true iamPolicyController: enabled: true EOF Check the managed clusters and the managed cluster set kubectl config use hub kubectl get managedclusters NAME HUB ACCEPTED MANAGED CLUSTER URLS JOINED AVAILABLE AGE local-cluster true https://api.cluster-xxx.xxx.xxx.xxx.com:6443 True True 5h55m rosa-subm1 true https://api.rosa-subm1.xxx.p1.openshiftapps.com:6443 True True 133m rosa-subm2 true https://api.rosa-subm2.xxx.p1.openshiftapps.com:6443 True True 51m Now it’s time to deploy submariner in our Managed ROSA Clusters. Either deploy using the RHACM UI or with CLI (choose one).\nDeploy Submariner Addon in Managed ROSA clusters from the RHACM UI Inside of the ClusterSets tab, go to the rosa-aro-clusters generated.\nGo to Submariner add-ons and Click in “Install Submariner Add-Ons”\nConfigure the Submariner addons adding both ROSA clusters generated:\nDeploy Submariner Addon in ROSA clusters After the ManagedClusterSet is created, the submariner-addon creates a namespace called managed-cluster-set-name-broker and deploys the Submariner broker to it. $ kubectl get ns | grep broker default-broker Active 6h39m rosa-clusters-broker Active 13m Create the Broker configuration on the hub cluster in the managed-cluster-set-name-broker namespace cat \u003c\u003c EOF | kubectl apply -f - apiVersion: submariner.io/v1alpha1 kind: Broker metadata: name: submariner-broker namespace: rosa-clusters-broker spec: globalnetEnabled: false EOF NOTE: Set the the value of globalnetEnabled: true if you want to enable Submariner Globalnet in the ManagedClusterSet.\nCheck the Submariner Broker in the rosa-clusters-broker namespace: kubectl get broker -n rosa-clusters-broker NAME AGE submariner-broker 21s We don’t need to label the ManagedCluster because it was imported the proper labels within the proper ManagedClusterSet.\nDeploy SubmarinerConfig for the first rosa cluster imported:\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: submarineraddon.open-cluster-management.io/v1alpha1 kind: SubmarinerConfig metadata: name: submariner namespace: $ROSA_CLUSTER_NAME_1 spec: IPSecNATTPort: 4500 NATTEnable: true cableDriver: libreswan loadBalancerEnable: true EOF Deploy SubmarinerConfig for the second rosa cluster imported: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: submarineraddon.open-cluster-management.io/v1alpha1 kind: SubmarinerConfig metadata: name: submariner namespace: $ROSA_CLUSTER_NAME_2 spec: IPSecNATTPort: 4500 NATTEnable: true cableDriver: libreswan loadBalancerEnable: true EOF Deploy Submariner on the first ROSA cluster cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: addon.open-cluster-management.io/v1alpha1 kind: ManagedClusterAddOn metadata: name: submariner namespace: $ROSA_CLUSTER_NAME_1 spec: installNamespace: submariner-operator EOF Deploy Submariner on the second ROSA cluster cluster: cat \u003c\u003c EOF | kubectl apply -f - apiVersion: addon.open-cluster-management.io/v1alpha1 kind: ManagedClusterAddOn metadata: name: submariner namespace: $ROSA_CLUSTER_NAME_2 spec: installNamespace: submariner-operator EOF Check the submariner status of managedclusteraddons in order to check if submariner is deployed correctly kubectl get managedclusteraddon -A | grep submariner rosa-sbmr1 submariner True rosa-sbmr2 submariner True The Submariner Add-on installation will start, and will take up to 10 minutes to finish.\nCheck the Status of the Submariner Networking Add-On A few minutes (up to 10 minutes) after we can check that the app Connection Status and the Agent Status are Healthy:\nTesting Submariner Networking connectivity with an example app (Optional) This final step (totally optional), is an extra step to check if the Submariner networking tunnels are built and connected properly.\nThis example app deploy one FE (guestbook) in the first ROSA cluster, and two redis with active-backup replication.\nOne Redis will be in the first ROSA cluster and will sync and replicate the data inserted by the FE, to the second redis (in backup/passive mode) using the submariner tunnel (connecting both ROSA clusters).\nThe connection will be using the ServiceExport feature (DNS Discovery) from Submariner, that allows to call the Redis Service (Active or Passive) from within the Service CIDR.\nClone the example repo app git clone https://github.com/rh-mobb/acm-demo-app Deploy the GuestBook App in ROSA Cluster 1 kubectl config use hub oc apply -k guestbook-app/acm-resources Deploy the Redis Master App in ROSA Cluster 1 oc apply -k redis-master-app/acm-resources Apply relaxed scc only for this PoC kubectl config use $ROSA_CLUSTER_NAME oc adm policy add-scc-to-user anyuid -z default -n guestbook oc delete pod --all -n guestbook Deploy the Redis Slave App in ROSA Cluster 2 kubectl config use hub oc apply -k redis-slave-app/acm-resources Apply relaxed SCC only for this PoC kubectl config use $ROSA_CLUSTER_NAME_2 oc adm policy add-scc-to-user anyuid -z default -n guestbook oc delete pod --all -n guestbook Testing the Synchronization of the Redis Master-Slave between clusters and interacting with our FrontEnd using Submariner tunnels To test the sync between the data from the Redis Master\u003c-\u003eSlave, let’s write some data into our frontend. Access to the route of the guestbook App y write some data:\nNow let’s see the logs in the Redis Slave: The sync is automatic and almost instantaneous between Master-Slave.\nWe can check the data write in the redis-slave with the redis-cli and the following command: for key in $(redis-cli -p 6379 keys \\*); do echo \"Key : '$key'\" redis-cli -p 6379 GET $key; done Let’s do this in the redis-slave pod: And that’s how the Redis-Master in the ROSA cluster 1 sync properly the data to the redis-slave in the ROSA Cluster 2, using Submariner tunnels, all encrypted with IPSec.\n","description":"","tags":["Submariner","ROSA","ACM"],"title":"Deploy ACM Submariner for connect overlay networks of ROSA clusters","uri":"/docs/redhat/acm/submariner/rosa/"},{"content":"","description":"","tags":null,"title":"Submariner","uri":"/tags/submariner/"},{"content":"The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. With the release of OpenShift 4.10 the EFS CSI Driver is now GA and available.\nThis is a guide to quickly enable the EFS Operator on ROSA to a Red Hat OpenShift on AWS (ROSA) cluster with STS enabled.\nNote: The official supported installation instructions for the EFS CSI Driver on ROSA are available here .\nDynamic vs Static provisioning The CSI driver supports both Static and Dynamic provisioning. Dynamic provisioning should not be confused with the ability of the Operator to create EFS volumes.\nDynamic provisioning Dynamic provisioning provisions new PVs as subdirectories of a pre-existing EFS volume. The PVs are independent of each other. However, they all share the same EFS volume. When the volume is deleted, all PVs provisioned out of it are deleted too. The EFS CSI driver creates an AWS Access Point for each such subdirectory. Due to AWS AccessPoint limits, you can only dynamically provision 120 PVs from a single StorageClass/EFS volume.\nStatic provisioning Static provisioning mounts the entire volume to a pod.\nPrerequisites A Red Hat OpenShift on AWS (ROSA) 4.10 cluster The OC CLI The AWS CLI jq command watch command Set up environment export some environment variables\nexport CLUSTER_NAME=\"sts-cluster\" export AWS_REGION=\"your_aws_region\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Prepare AWS Account In order to use the AWS EFS CSI Driver we need to create IAM roles and policies that can be attached to the Operator.\nCreate an IAM Policy\ncat \u003c\u003c EOF \u003e $SCRATCH_DIR/efs-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:DescribeAccessPoints\", \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", \"ec2:DescribeAvailabilityZones\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:CreateAccessPoint\" ], \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/efs.csi.aws.com/cluster\": \"true\" } } }, { \"Effect\": \"Allow\", \"Action\": \"elasticfilesystem:DeleteAccessPoint\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/efs.csi.aws.com/cluster\": \"true\" } } } ] } EOF Create the Policy\nThis creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler.\nPOLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-rosa-efs-csi\" \\ --policy-document file://$SCRATCH_DIR/efs-policy.json \\ --query 'Policy.Arn' --output text) || \\ POLICY=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`rosa-efs-csi`].Arn' \\ --output text) echo $POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator\", \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa\" ] } } } ] } EOF Create Role for the EFS CSI Driver Operator\nROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Deploy and test the AWS EFS Operator Create a Secret to tell the AWS EFS Operator which IAM role to request.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-efs-cloud-credentials namespace: openshift-cluster-csi-drivers stringData: credentials: |- [default] role_arn = $ROLE web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install the EFS Operator\ncat \u003c\u003cEOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-cluster-csi-drivers- namespace: openshift-cluster-csi-drivers --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: \"\" name: aws-efs-csi-driver-operator namespace: openshift-cluster-csi-drivers spec: channel: stable installPlanApproval: Automatic name: aws-efs-csi-driver-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait until the Operator is running\nwatch oc get deployment aws-efs-csi-driver-operator -n openshift-cluster-csi-drivers Install the AWS EFS CSI Driver\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: ClusterCSIDriver metadata: name: efs.csi.aws.com spec: managementState: Managed EOF Wait until the CSI driver is running\nwatch oc get daemonset aws-efs-csi-driver-node -n openshift-cluster-csi-drivers Prepare an AWS EFS Volume for dynamic provisioning Run this set of commands to update the VPC to allow EFS access\nNODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ --region $AWS_REGION \\ | jq -r '.[0][0].VpcId') CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ --region $AWS_REGION \\ | jq -r '.[0]') SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ --region $AWS_REGION \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') echo \"CIDR - $CIDR, SG - $SG\" Assuming the CIDR and SG are correct, update the security group\naws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . At this point you can create either a single Zone EFS filesystem, or a Region wide EFS filesystem\nCreating a region-wide EFS Create a region-wide EFS File System\nEFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --region ${AWS_REGION} \\ --encrypted | jq -r '.FileSystemId') echo $EFS Configure a region-wide Mount Target for EFS (this will create a mount point in each subnet of your VPC by default)\nfor SUBNET in $(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ --region $AWS_REGION \\ | jq -r '.[].SubnetId'); do \\ MOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ --region $AWS_REGION \\ | jq -r '.MountTargetId'); \\ echo $MOUNT_TARGET; \\ done Creating a single-zone EFS Note: If you followed the instructions above to create a region wide EFS mount, skip the following steps and proceed to “Create a Storage Class for the EFS volume”\nSelect the first subnet that you will make your EFS mount in (this will by default select the same Subnet your first node is in)\nSUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ --region $AWS_REGION \\ | jq -r '.[0].SubnetId') AWS_ZONE=$(aws ec2 describe-subnets --filters Name=subnet-id,Values=$SUBNET \\ --region $AWS_REGION | jq -r '.Subnets[0].AvailabilityZone') Create your zonal EFS filesystem\nEFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --availability-zone-name $AWS_ZONE \\ --region $AWS_REGION \\ --encrypted | jq -r '.FileSystemId') echo $EFS Create your EFS mount point\nMOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ --region $AWS_REGION \\ | jq -r '.MountTargetId') echo $MOUNT_TARGET Create a Storage Class for the EFS volume\ncat \u003c\u003cEOF | oc apply -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: $EFS directoryPerms: \"700\" gidRangeStart: \"1000\" gidRangeEnd: \"2000\" basePath: \"/dynamic_provisioning\" EOF Test Create a namespace\noc new-project efs-demo Create a PVC\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-efs-volume spec: storageClassName: efs-sc accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Create a Pod to write to the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs \u0026\u0026 sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF It may take a few minutes for the pod to be ready. If you see errors such as Output: Failed to resolve \"fs-XXXX.efs.us-east-2.amazonaws.com\" it likely means its still setting up the EFS volume, just wait longer.\nWait for the Pod to be ready\nwatch oc get pod test-efs Create a Pod to read from the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume\noc logs test-efs-read You should see a stream of “hello efs”\nhello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs Cleanup Delete the Pods\noc delete pod -n efs-demo test-efs test-efs-read Delete the Volume\noc delete -n efs-demo pvc pvc-efs-volume Delete the Namespace\noc delete project efs-demo Delete the storage class\noc delete storageclass efs-sc Delete the EFS Shared Volume via AWS\naws efs delete-mount-target --mount-target-id $MOUNT_TARGET --region $AWS_REGION aws efs delete-file-system --file-system-id $EFS --region $AWS_REGION Note: if you receive the error An error occurred (FileSystemInUse) wait a few minutes and try again.\nNote: if you created additional mount points for a regional EFS filesystem, remember to delete all of them before removing the file system\nDetach the Policies to the Role\naws iam detach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Delete the Role\naws iam delete-role --role-name \\ ${CLUSTER_NAME}-aws-efs-csi-operator Delete the Policy\naws iam delete-policy --policy-arn \\ $POLICY ","description":"","tags":["AWS","ROSA"],"title":"Enabling the AWS EFS CSI Driver Operator on ROSA","uri":"/docs/rosa/aws-efs/"},{"content":"The steps to add Azure AD as an identity provider for Azure Red Hat OpenShift (ARO) via cli are:\nPrerequisites Have Azure cli installed Login to Azure Azure Define needed variables Get oauthCallbackURL Create \u003ccode\u003emanifest.json\u003c/code\u003e file to configure the Azure Active Directory application Register/create app Add Service Principal for the new app Make Service Principal an Enterprise Application Create the client secret Update the Azure AD application scope permissions Get Tenant ID OpenShift Login to OpenShift as kubeadmin Create an OpenShift secret### Apply OpenShift OpenID authentication Wait for authentication operator to roll out Verify login through Azure Active Directory Last steps Prerequisites Have Azure cli installed Follow the Microsoft instuctions: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli Note This has been written for az cli verion 2.37.0 some commands will not work with previous versions, however, there is a known issue https://github.com/Azure/azure-cli/issues/23027 where we will use an older version via podman run -it mcr.microsoft.com/azure-cli:2.36.0 . In case you’re using docker, just replace podman command by docker . For podman installation on Mac, Windows \u0026 Linux, please refer to https://podman.io/getting-started/installation Login to Azure Login to Azure as follows:\naz login If you’re logging in from a system you have no access to your browser you can authenticate, you can also use\naz login --use-device-code Azure Define needed variables To simplly follow along, first define the following variables according to your set-up:\nRESOURCEGROUP=\u003ccluster-dmoessne-aro01\u003e # replave with your name CLUSTERNAME=\u003crg-dmoessne-aro01\u003e # replave with your name Get oauthCallbackURL To get the oauthCallbackURL for the Azure AD integration, run the following commands:\nDOMAIN=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query clusterProfile.domain -o tsv) APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query apiserverProfile.url -o tsv) oauthCallbackURL=https://oauth-openshift.apps.$DOMAIN/oauth2callback/AAD echo $oauthCallbackURL Note oauthCallbackURL, in particular AAD can be changed but must match the name in the oauth providerwhen creating the OpenShift OpenID authentication\nCreate manifest.json file to configure the Azure Active Directory application Configure OpenShift to use the email claim and fall back to upn to set the Preferred Username by adding the upn as part of the ID token returned by Azure Active Directory.\nCreate a manifest.json file to configure the Azure Active Directory application.\ncat \u003c\u003c EOF \u003e manifest.json { \"idToken\": [ { \"name\": \"preferred_username\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] } ] } EOF Register/create app Create an Azure AD application and retrieve app id:\nDISPLAYNAME=\u003cauth-dmoessne-aro01\u003e # set you name accordingly az ad app create \\ --display-name $DISPLAYNAME \\ --web-redirect-uris $oauthCallbackURL \\ --sign-in-audience AzureADMyOrg \\ --optional-claims @manifest.json APPID=$(az ad app list --display-name $DISPLAYNAME --query '[].appId' -o tsv) Add Service Principal for the new app Create Service Principal for the app created:\naz ad sp create --id $APPID Make Service Principal an Enterprise Application We need this Service Principal to be an Enterprise Application to be able to add users and groups, so we add the needed tag (az cli \u003e= 2.38.0)\naz ad sp update --id $APPID --set 'tags=[\"WindowsAzureActiveDirectoryIntegratedApp\"]' Note In case you get a trace back (az cli = 2.37.0) check out https://github.com/Azure/azure-cli/issues/23027 To overcome that issue, we’ll do the following\n# APP_ID=$(az ad app list --display-name $DISPLAYNAME --query [].id -o tsv) # az rest --method PATCH --url https://graph.microsoft.com/v1.0/applications/$APP_ID --body '{\"tags\":[\"WindowsAzureActiveDirectoryIntegratedApp\"]}' Create the client secret The password for the app created is retrieved by resetting the same:\nPASSWD=$(az ad app credential reset --id $APPID --query password -o tsv) Note The password generated with above command is by default valid for one year and you may want to change that by adding either and end date via --end-date or set validity in years with --years. For details consult the documentation Update the Azure AD application scope permissions To be able to read the user information from Azure Active Directory, we need to add the following Azure Active Directory Graph permissions\nAdd permission for the Azure Active Directory as follows:\nread email az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 64a6cdd6-aab1-4aaf-94b8-3cc8405e90d0=Scope \\ --id $APPID read profile az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 14dad69e-099b-42c9-810b-d002981feec1=Scope \\ --id $APPID User.Read az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions e1fe6dd8-ba31-4d61-89e7-88639da4683d=Scope \\ --id $APPID Note If you see a message that you need to grant consent you can safely ignore it, unless you are authenticated as a alobal administrator for this Azure Active Directory. Standard domain users will be asked to grant consent when they first login to the cluster using their AAD credentials.\nGet Tenant ID We do need the Tenant ID for setting up the Oauth provider later on:\nTENANTID=$(az account show --query tenantId -o tsv) Note Now we can switch over to our OpenShift installation and apply the needed configuraion. Please refer to https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html to get the latest oc cli\nOpenShift Login to OpenShift as kubeadmin Fetch kubeadmin password and login to your cluster via oc cli (you can use any other cluster-admin user in case you have already created/added other oauth providers)\nKUBEPW=$(az aro list-credentials \\ --name $CLUSTERNAME \\ --resource-group $RESOURCEGROUP \\ --query kubeadminPassword --output tsv) oc login $APISERVER -u kubeadmin -p $KUBEPW Create an OpenShift secret### Create an OpenShift secret to store the Azure Active Directory application secret from the application password we created/reset earlier:\noc create secret generic openid-client-secret-azuread \\ -n openshift-config \\ --from-literal=clientSecret=$PASSWD Apply OpenShift OpenID authentication As a last step we need to apply the OpenShift OpenID authentication for Azure Active Directory:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: $APPID clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - preferred_username name: - name email: - email issuer: https://login.microsoftonline.com/$TENANTID/v2.0 EOF Wait for authentication operator to roll out Before we move over to the OpenShift login, let’s wait for the new version of the authentication cluster operator to be rolled out\nwatch -n 5 oc get co authentication Note it may take some time until the rollout starts\nVerify login through Azure Active Directory Get console url to login:\naz aro show --name $CLUSTERNAME --resource-group $RESOURCEGROUP --query \"consoleProfile.url\" -o tsv Opening the url in a browser, we can see the login to Azure AD is available\nAt first login you may have to accept application permissions\nLast steps As a last step you may want to grant a user or group cluster-admin permissions and remove kubeadmin user, see\nhttps://docs.openshift.com/container-platform/4.10/authentication/using-rbac.html#cluster-role-binding-commands_using-rbac https://docs.openshift.com/container-platform/4.10/authentication/remove-kubeadmin.html ","description":"","tags":["Azure","ARO"],"title":"Configure Azure AD as an OIDC identity provider for ARO with cli","uri":"/docs/idp/azuread-aro-cli/"},{"content":"","description":"","tags":null,"title":"Cost","uri":"/tags/cost/"},{"content":"Adopted from Official Documentation for Cost Management Service Red Hat Cost Management is a software as a service (SaaS) offering available free of charge as part of your Red Hat subscriptions. Cost management helps you monitor and analyze your OpenShift Container Platform and Public cloud costs in order to improve the management of your business.\nSome capabilities of cost management are :\nVisualize costs across hybrid cloud infrastructure Track cost trends Map charges to projects and organizations Normalize data and add markups with cost models Generate showback and chargeback information In this document, I will show you how to connect your OpenShift and Cloud provider sources to Cost Management in order to collect cost and usage.\nPrerequisites A Public Cloud subscritption (Azure Subscription) An OpenShift Cluster (to create an Azure Red Hat OpenShift (ARO) cluster, click here ) Adding your OpenShift source to Cost Management Installing the Cost Management Metric Operator Log into the Openshift cluster web console with cluster-admin credentials\nOn the left navigation pane under Administator perspective, select Operators –\u003e OperatorHub\nSearch for and locate cost management metrics operator. Click on the displayed Cost Management Metrics Operator\nWhen the Install Operator window appears, you must select the costmanagement-metrics-operator namespace for installation. If it does not exist, it will be created for you. Click on install button.\nAfter a short wait, Cost Management Metrics Operator appears in the Installed Operators tab under Project: all Projects or Project: costmanagement-metrics-operator\nConfiguring the Operator instance for a new installation Once installed, click on the Cost Management\nIn the detail window, click + Create Instance\nA Cost Management Metrics Operator \u003e Create CostManagementMetricsConfig window appears\nClick the YAML view radio button to view and modify the contents of the YAML configuration file\nModify the following two lines in the YAML file to look like the following\ncreate_source: true name: \u003cSOURCE-NAME\u003e Change SOURCE-NAME to the new name of your source (ex. my-openshift-cost-source) Change false to true\nClick the Create button. This creates a new source for cost management that will appear in the console.redhat.com Cost Management applications\nAdding your Microsoft Azure source to Cost Management 1. Configuring your Microsoft Azure The following steps are required to configure your Azure account to be a cost management source\nCreating a storage account and resource group Configuring a storage account contributor and reader roles for access Scheduling daily exports 1.1 Creating an Azure resource group and storage account using Azure CLI First create a new resource group\naz group create \\ --name storage-resource-group \\ --location eastus If you’re not sure which region to specify for the --location parameter, you can retrieve a list of supported regions for your subscription with the az account list-locations command.\naz account list-locations \\ --query \"[].{Region:name}\" \\ --out table Next, create a standard general-purpose v2 storage account with read-access geo-redundant storage. Ensure the name of your storage account is unique across Azure\naz storage account create \\ --name \u003caccount-name\u003e \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_RAGRS \\ --kind StorageV2 Make note of the resource group and storage account. We will need them in the subsequent steps\n2. Creating a Microsoft Azure Source in your Red Hat account In the console.redhat.com click on All apps and services tab in the left top corner of the screen to navigate to this window. Click on Sources under Settings\nOn Sources page, click on Cloud sources tab and then click Add a source. This opens up the Sources Wizard Select Microsoft Azure as the source type and click next.\nEnter a name for your source and click next.\nSelect cost management as the application click next.\nEnter resource group and storage account name created in the last step to collect cost data and metrics for cost management click next.\nEnter Subscription ID for your Azure account click next. Use the command given by the wizard to get Subscription ID.\nUse the command given by the wizard to create Service Principal and enter Tenant ID, Application ID and Application Secret.\nClick next, review the information and click add.\n2.3 Configuring a Daily Azure data export schedule using Azure Portal Cost management requires a data export from a Subscription level scope\nIn the Azure Portal home page, click on Subscriptions\nSelect the Subscription you want to track from the list, and then select Cost Analysis in the menu. At the top of the Cost analysis page, select configure subscription\nClick on the Export tab, and then Schedule export\nIn the Exports wizard, fill out the Export details\nFor Export Type, select Daily export of billing-period-to-date costs For Storage account, select the account you created earlier Enter any value for the Container name and Directory path for the export. These values provide the tree structure in the storage account where report files are stored.\nClick Create to start exporting data to the Azure storage container.\nReturn to Sources wizard after creating the export schedule and click Next. Review the source details\nClick Finish to complete adding the Azure source to cost management\nCost management will begin polling Azure for cost data, which will appear on the cost management dashboard (console.redhat.com/openshift/cost-management/).\nAdding your Amazon AWS source to Cost Management In the console.redhat.com click on All apps and services tab in the left top corner of the screen to navigate to this window. Click on Sources under Settings\nOn Sources page, click on Cloud sources tab and then click Add a source. This opens up the Sources Wizard Select Amazon Web Services as the source type and click next.\nEnter a name for your source and click next.\nTo automatically configure your AWS account, select Account authorization option and provide Access Key ID and Access Secret Key. Click next.\nSelect Cost Management option and click next.\nReview the information and click add.\nManaging your Costs After adding your Openshift Container Platform and Cloud Provider sources, Cost management will show cost data by\nSource\nCloud provider cost and usage related to running your OpenShift Container Platform clusters on their platform\nSee the following video for a quick overview of Cost Management for OpenShift followed by a demo of the product on YouTube Next steps for managing your costs Limiting access to cost management resources - Use role-based access control to limit visibility of resources in cost management reports.\nManaging cost data using tagging - Tags allow you to organize your resources by cost and allocate the costs to different parts of your cloud infrastructure\nUsing cost models - Configure cost models to associate prices to metrics and usage.\nVisualizing your costs using Cost Explorer - Allows you to see your costs through time.\n","description":"","tags":["Cost","Azure","ARO"],"title":"Red Hat Cost Management for Cloud Services","uri":"/docs/misc/cost-management/"},{"content":"Securing exposing an Internet facing application with a private ARO Cluster.\nWhen you create a cluster on ARO you have several options in making the cluster public or private. With a public cluster you are allowing Internet traffic to the api and *.apps endpoints. With a private cluster you can make either or both the api and .apps endpoints private.\nHow can you allow Internet access to an application running on your private cluster where the .apps endpoint is private? This document will guide you through using Azure Frontdoor to expose your applications to the Internet. There are several advantages of this approach, namely your cluster and all the resources in your Azure account can remain private, providing you an extra layer of security. Azure FrontDoor operates at the edge so we are controlling traffic before it even gets into your Azure account. On top of that, Azure FrontDoor also offers WAF and DDoS protection, certificate management and SSL Offloading just to name a few benefits.\nAdopted from ARO Reference Architecture Prerequisites az cli oc cli a custom domain a DNS zone that you can easily modify To build and deploy the application\nmaven cli quarkus cli OpenJDK Java 8 Make sure to use the same terminal session while going through guide for all commands as we will reference envrionment variables set or created through the guide.\nGet Started Create a private ARO cluster.\nFollow this guide to Create a private ARO cluster or simply run this bash script Set Evironment Variables Manually set environment variables\nAROCLUSTER=\u003ccluster name\u003e ARORG=\u003cresource group for the cluster\u003e AFD_NAME=\u003cname you want to use for the front door instance\u003e DOMAIN='e.g. aro.kmobb.com' This is the domain that you will be adding to Azure DNS to manage. ARO_APP_FQDN='e.g. minesweeper.aro.kmobb.com' (note - we will be deploying an application called minesweeper to test front door. Select a domain you would like to use for the application. For example minesweeper.aro.kmobb.com ... where aro.kmobb.com is the domain you manage and have DNS access to.) AFD_MINE_CUSTOM_DOMAIN_NAME='minesweeper-aro-kmobb-com' (note - this should be your domain name without and .'s for example minesweeper-aro-kmobb-com) PRIVATEENDPOINTSUBNET_PREFIX= subnet in the VNET you cluster is in. If you following the example above to create a custer where you virtual network is 10.0.0.0/20 then you can use '10.0.6.0/24' PRIVATEENDPOINTSUBNET_NAME='PrivateEndpoint-subnet' Set environment variables with Bash\nUNIQUEID=$RANDOM ARO_RGNAME=$(az aro show -n $AROCLUSTER -g $ARORG --query \"clusterProfile.resourceGroupId\" -o tsv | sed 's/.*\\///') LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) INTERNAL_LBNAME=$(az network lb list --resource-group $ARO_RGNAME --query \"[? contains(name, 'internal')].name\" -o tsv) WORKER_SUBNET_NAME=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv | sed 's/.*\\///') WORKER_SUBNET_ID=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) LBCONFIG_ID=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].id\" -o tsv) LBCONFIG_IP=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].privateIPAddress\" -o tsv) Create a Private Link Service After we have the cluster up and running, we need to create a private link service. The private link service will provide private and secure connectivity between the Front Door Service and our cluster.\nDisable the worker subnet private link service network policy for the worker subnet\naz network vnet subnet update \\ --disable-private-link-service-network-policies true \\ --name $WORKER_SUBNET_NAME \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME Create a private link service targeting the worker subnets\naz network private-link-service create \\ --name $AROCLUSTER-pls \\ --resource-group $ARORG \\ --private-ip-address-version IPv4 \\ --private-ip-allocation-method Dynamic \\ --vnet-name $VNET_NAME \\ --subnet $WORKER_SUBNET_NAME \\ --lb-frontend-ip-configs $LBCONFIG_ID privatelink_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'id' -o tsv) Create and Configure an instance of Azure Front Door Create a Front Door Instance\naz afd profile create \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --sku Premium_AzureFrontDoor afd_id=$(az afd profile show -g $ARORG --profile-name $AFD_NAME --query 'id' -o tsv) Create an endpoint for the ARO Internal Load Balancer\naz afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-ilb'$UNIQUEID \\ --profile-name $AFD_NAME Create a Front Door Origin Group that will point to the ARO Internal Loadbalancer\naz afd origin-group create \\ --origin-group-name 'afdorigin' \\ --probe-path '/' \\ --probe-protocol Http \\ --probe-request-type GET \\ --probe-interval-in-seconds 100 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --probe-interval-in-seconds 120 \\ --sample-size 4 \\ --successful-samples-required 3 \\ --additional-latency-in-milliseconds 50 Create a Front Door Origin with the above Origin Group that will point to the ARO Internal Loadbalancer\naz afd origin create \\ --enable-private-link true \\ --private-link-resource $privatelink_id \\ --private-link-location $LOCATION \\ --private-link-request-message 'Private link service from AFD' \\ --weight 1000 \\ --priority 1 \\ --http-port 80 \\ --https-port 443 \\ --origin-group-name 'afdorigin' \\ --enabled-state Enabled \\ --host-name $LBCONFIG_IP \\ --origin-name 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Approve the private link connection\nprivatelink_pe_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'privateEndpointConnections[0].id' -o tsv) az network private-endpoint-connection approve \\ --description 'Approved' \\ --id $privatelink_pe_id Add your custom domain to Azure Front Door\naz afd custom-domain create \\ --certificate-type ManagedCertificate \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --host-name $ARO_APP_FQDN \\ --minimum-tls-version TLS12 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Create an Azure Front Door endpoint for your custom domain\naz afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --profile-name $AFD_NAME Add an Azure Front Door route for your custom domain\naz afd route create \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --forwarding-protocol HttpOnly \\ --https-redirect Disabled \\ --origin-group 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --route-name 'aro-mine-route' \\ --supported-protocols Http Https \\ --patterns-to-match '/*' \\ --custom-domains $AFD_MINE_CUSTOM_DOMAIN_NAME Update DNS\nGet a validation token from Front Door so Front Door can validate your domain\nafdToken=$(az afd custom-domain show \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --query \"validationProperties.validationToken\") Create a DNS Zone\naz network dns zone create -g $ARORG -n $DOMAIN You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar.\nCreate a new text record in your DNS server\naz network dns record-set txt add-record -g $ARORG -z $DOMAIN -n _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') --value $afdToken --record-set-name _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') Check if the domain has been validated:\nNote this can take several hours Your FQDN will not resolve until Front Door validates your domain.\naz afd custom-domain list -g $ARORG --profile-name $AFD_NAME --query \"[? contains(hostName, '$ARO_APP_FQDN')].domainValidationState\" Add a CNAME record to DNS\nGet the Azure Front Door endpoint:\nafdEndpoint=$(az afd endpoint show -g $ARORG --profile-name $AFD_NAME --endpoint-name aro-mine-$UNIQUEID --query \"hostName\" -o tsv) Create a cname record for the application\naz network dns record-set cname set-record -g $ARORG -z $DOMAIN \\ -n $(echo $ARO_APP_FQDN | sed 's/\\..*//') -z $DOMAIN -c $afdEndpoint Deploy an application Now the fun part, let’s deploy an application! We will be deploying a Java based application called microsweeper . This is an application that runs on OpenShift and uses a PostgreSQL database to store scores. With ARO being a first class service on Azure, we will create an Azure Database for PostgreSQL service and connect it to our cluster with a private endpoint.\nCreate a Azure Database for PostgreSQL servers service\naz postgres server create --name microsweeper-database --resource-group $ARORG --location $LOCATION --admin-user quarkus --admin-password r3dh4t1! --sku-name GP_Gen5_2 POSTGRES_ID=$(az postgres server show -n microsweeper-database -g $ARORG --query 'id' -o tsv) Create a private endpoint connection for the database\naz network vnet subnet create \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --name $PRIVATEENDPOINTSUBNET_NAME \\ --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \\ --disable-private-endpoint-network-policies true az network private-endpoint create \\ --name 'postgresPvtEndpoint' \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --subnet $PRIVATEENDPOINTSUBNET_NAME \\ --private-connection-resource-id $POSTGRES_ID \\ --group-id 'postgresqlServer' \\ --connection-name 'postgresdbConnection' Create and configure a private DNS Zone for the Postgres database\naz network private-dns zone create \\ --resource-group $ARORG \\ --name 'privatelink.postgres.database.azure.com' az network private-dns link vnet create \\ --resource-group $ARORG \\ --zone-name 'privatelink.postgres.database.azure.com' \\ --name 'PostgresDNSLink' \\ --virtual-network $VNET_NAME \\ --registration-enabled false az network private-endpoint dns-zone-group create \\ --resource-group $ARORG \\ --name 'PostgresDb-ZoneGroup' \\ --endpoint-name 'postgresPvtEndpoint' \\ --private-dns-zone 'privatelink.postgres.database.azure.com' \\ --zone-name 'postgresqlServer' NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) POSTGRES_IP=$(az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv) az network private-dns record-set a create --name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG az network private-dns record-set a add-record --record-set-name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG -a $POSTGRES_IP Create a postgres database that will contain scores for the minesweeper application\naz postgres db create \\ --resource-group $ARORG \\ --name score \\ --server-name microsweeper-database Deploy the minesweeper application Clone the git repository\ngit clone https://github.com/rh-mobb/aro-workshop-app.git change to the root directory\ncd aro-workshop-app Ensure Java 1.8 is set at your Java version\nmvn --version Look for Java version - 1.8XXXX if not set to Java 1.8 you will need to set your JAVA_HOME variable to Java 1.8 you have installed. To find your java versions run:\njava -version then export your JAVA_HOME variable\nexport JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_332` Log into your openshift cluster\nBefore you deploy your application, you will need to be connected to a private network that has access to the cluster.\nA great way to establish this connectity is with a VPN connection. Follow this guide to setup a VPN connection with your Azure account.\nkubeadmin_password=$(az aro list-credentials --name $AROCLUSTER --resource-group $ARORG --query kubeadminPassword --output tsv) apiServer=$(az aro show -g $ARORG -n $AROCLUSTER --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password Create a new OpenShift Project\noc new-project minesweeper add the openshift extension to quarkus\nquarkus ext add openshift Edit microsweeper-quarkus/src/main/resources/application.properties\nMake sure your file looks like the one below, changing the IP address on line 3 to the private ip address of your postgres instance.\nTo find your Postgres private IP address run the following commands:\nNETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv Sample microsweeper-quarkus/src/main/resources/application.properties\n# Database configurations %prod.quarkus.datasource.db-kind=postgresql %prod.quarkus.datasource.jdbc.url=jdbc:postgresql://10.1.6.9:5432/score %prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver %prod.quarkus.datasource.username=quarkus@microsweeper-database %prod.quarkus.datasource.password=r3dh4t1! %prod.quarkus.hibernate-orm.database.generation=drop-and-create %prod.quarkus.hibernate-orm.database.generation=update # OpenShift configurations %prod.quarkus.kubernetes-client.trust-certs=true %prod.quarkus.kubernetes.deploy=true %prod.quarkus.kubernetes.deployment-target=openshift %prod.quarkus.openshift.build-strategy=docker Build and deploy the quarkus application to OpenShift\nquarkus build --no-tests Create a route to your custom domain Change the snippet below replacing your hostname for the host:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app.kubernetes.io/name: microsweeper-appservice app.kubernetes.io/version: 1.0.0-SNAPSHOT app.openshift.io/runtime: quarkus name: microsweeper-appservice namespace: minesweeper spec: host: minesweeper.aro.kmobb.com to: kind: Service name: microsweeper-appservice weight: 100 targetPort: port: 8080 wildcardPolicy: None EOF Check the dns settings of your application.\nnotice that the application URL is routed through Azure Front Door at the edge. The only way this application that is running on your cluster can be access is through Azure Front Door which is connected to your cluster through a private endpoint.\nnslookup $ARO_APP_FQDN sample output:\nServer:\t2600:1700:850:d220::1 Address:\t2600:1700:850:d220::1#53 Non-authoritative answer: minesweeper.aro.kmobb.com\tcanonical name = aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net. aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net\tcanonical name = star-azurefd-prod.trafficmanager.net. star-azurefd-prod.trafficmanager.net\tcanonical name = dual.part-0013.t-0009.t-msedge.net. dual.part-0013.t-0009.t-msedge.net\tcanonical name = part-0013.t-0009.t-msedge.net. Name:\tpart-0013.t-0009.t-msedge.net Address: 13.107.213.41 Name:\tpart-0013.t-0009.t-msedge.net Address: 13.107.246.41 Test the application Point your broswer to your domain!! Clean up To clean up everything you created, simply delete the resource group\naz group delete -g $ARORG ","description":"","tags":["ARO","Azure"],"title":"Azure Front Door with ARO ( Azure Red Hat OpenShift )","uri":"/docs/aro/frontdoor/"},{"content":"When you configure an Azure Red Hat OpenShift (ARO) cluster with a private only configuration, you will need connectivity to this private network in order to access your cluster. This guide will show you how to configute a point-to-site VPN connection so you won’t need to setup and configure Jump Boxes.\nPrerequisites a private ARO Cluster git openssl Create certificates to use for your VPN Connection There are many ways and methods to create certificates for VPN, the guide below is one of the ways that works well. Note, that whatever method you use, make sure it supports “X509v3 Extended Key Usage”.\nClone OpenVPN/easy-rsa\ngit clone https://github.com/OpenVPN/easy-rsa.git Change to the easyrsa directory\ncd easy-rsa/easyrsa3 Initialize the PKI\n./easyrsa init-pki Edit certificate parameters\nUncomment and edit the copied template with your values\nvim pki/vars set_var EASYRSA_REQ_COUNTRY \"US\" set_var EASYRSA_REQ_PROVINCE \"California\" set_var EASYRSA_REQ_CITY \"San Francisco\" set_var EASYRSA_REQ_ORG \"Copyleft Certificate Co\" set_var EASYRSA_REQ_EMAIL \"me@example.net\" set_var EASYRSA_REQ_OU \"My Organizational Unit\" Uncomment (remove the #) the folowing field\n#set_var EASYRSA_KEY_SIZE 2048 Create the CA:\n./easyrsa build-ca nopass Generate the Server Certificate and Key\n./easyrsa build-server-full server nopass Generate Diffie-Hellman (DH) parameters\n./easyrsa gen-dh Generate client credentials\n./easyrsa build-client-full azure nopass Set environment variables for the CA certificate you just created.\nCACERT=$(openssl x509 -in pki/ca.crt -outform der | base64) Set Envrionment Variables AROCLUSTER=\u003ccluster name\u003e ARORG=\u003cresource group the cluster is in\u003e UNIQUEID=$RANDOM LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) GW_NAME=${USER}_${VNET_NAME} GW_SUBNET_PREFIX=e.g. 10.0.7.0/24 choose a new available subnet in the VNET your cluster is in. VPN_PREFIX=172.18.0.0/24 Create an Azure Virtual Network Gateway Request a public IP Address\naz network public-ip create \\ -n $USER-pip-$UNIQUEID \\ -g $ARORG \\ --allocation-method Static \\ --sku Standard \\ --zone 1 2 3 pip=$(az network public-ip show -g $ARORG --name $USER-pip-$UNIQUEID --query \"ipAddress\" -o tsv) Create a Gateway Subnet\naz network vnet subnet create \\ --vnet-name $VNET_NAME \\ -n GatewaySubnet \\ -g $ARORG \\ --address-prefix $GW_SUBNET_PREFIX Create a virtual network gateway\naz network vnet-gateway create \\ --name $GW_NAME \\ --location $LOCATION \\ --public-ip-address $USER-pip-$UNIQUEID \\ --resource-group $ARORG \\ --vnet $VNET_NAME \\ --gateway-type Vpn \\ --sku VpnGw3AZ \\ --address-prefixes $VPN_PREFIX \\ --root-cert-data pki/ca.crt \\ --root-cert-name $USER-p2s \\ --vpn-type RouteBased \\ --vpn-gateway-generation Generation2 \\ --client-protocol IkeV2 OpenVPN go grab a coffee, this takes about 15 - 20 minutes\nConfigure your OpenVPN Client Retrieve the VPN Settings\nFrom the Azure Portal - navigate to your Virtual Network Gateway, point to site configuration, and then click Download VPN Client. This will download a zip file containing the VPN Client\nCreate a VPN Client Configuration\nUncompress the file you downloaded in the previous step and edit the OpenVPN\\vpnconfig.ovpn file.\nNote: The next two commands assume you are still in the easyrsa3 directory.\nIn the vpnconfig.ovpn replace the $CLIENTCERTIFICATE line with the entire contents of:\nopenssl x509 -in pki/issued/azure.crt Make sure to copy the —–BEGIN CERTIFICATE—– and the —–END CERTIFICATE—– lines.\nalso replace $PRIVATEKEY line with the output of:\ncat pki/private/azure.key Make sure to copy the —–BEGIN PRIVATE KEY—– and the —–END PRIVATE KEY—– lines.\nadd the new OpenVPN configuration file to your OpenVPN client.\nmac users - just double click on the vpnserver.ovpn file and it will be automatically imported.\nConnect your VPN.\n","description":"","tags":["ARO","Azure"],"title":"Setup a VPN Connection into an ARO Cluster with OpenVPN","uri":"/docs/aro/vpn/"},{"content":" Deploy ACM Submariner for connecting overlay networks of ROSA clusters Deploy ACM Submariner for connect overlay networks ARO - ROSA clusters ","description":"","tags":null,"title":"Advanced Cluster Management - Submariner","uri":"/docs/redhat/acm/submariner/"},{"content":"In Azure Red Hat OpenShift (ARO) you can fairly easily set up cluster logging to an in-cluster Elasticsearch using the OpenShift Elasticsearch Operator and the Cluster Logging Operator, but what if you want to use the Azure native Log Analytics service?\nThere’s a number of ways to do this, for example installing agents onto the VMs (in this case, it would be a DaemonSet with hostvar mounts) but that isn’t ideal in a managed system like ARO.\nFluentd is the log collection and forwarding tool used by OpenShift, however it does not have native support for Azure Log Analytics. However Fluent-bit which supports many of the same protocols as Fluentd does have native support for Azure Log Analytics.\nArmed with this knowledge we can create a fluent-bit service on the cluster to accept logs from fluentd and forward them to Azure Log Analytics.\nPrepare your ARO cluster Deploy an ARO cluster\nSet some environment variables\nexport NAMESPACE=aro-clf-am export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift # this value must be unique export AZR_LOG_APP_NAME=$AZR_RESOURCE_GROUP-$AZR_RESOURCE_LOCATION Set up ARO Monitor workspace Add the Azure CLI log extensions\naz extension add --name log-analytics Create resource group\nIf you plan to reuse the same group as your cluster skip this step\naz group create -n $AZR_RESOURCE_GROUP -l $AZR_RESOURCE_LOCATION Create workspace\naz monitor log-analytics workspace create \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ -l $AZR_RESOURCE_LOCATION Create a secret for your Azure workspace\nWORKSPACE_ID=$(az monitor log-analytics workspace show \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query customerId -o tsv) SHARED_KEY=$(az monitor log-analytics workspace get-shared-keys \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query primarySharedKey -o tsv) Configure OpenShift Create a Project to run the log forwarding in\noc new-project $NAMESPACE Create namespaces for logging operators\nkubectl create ns openshift-logging kubectl create ns openshift-operators-redhat Add the MOBB chart repository to Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories\nhelm repo update Deploy the OpenShift Elasticsearch Operator and the Red Hat OpenShift Logging Operator\n\u003e Note: You can skip this if you already have them installed, or install them via the OpenShift Console.\nhelm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-clf-am/files/operators.yaml Configure cluster logging forwarder\nhelm upgrade -n $NAMESPACE clf \\ mobb/aro-clf-am --install \\ --set \"azure.workspaceId=$WORKSPACE_ID\" --set \"azure.sharedKey=$SHARED_KEY\" Check for logs in Azure Wait 5 to 15 minutes\nQuery our new Workspace\naz monitor log-analytics query -w $WORKSPACE_ID \\ --analytics-query \"openshift_CL | take 10\" --output tsv or\nLog into AzureAzure Log Insights or you can login into portal and search for Log Analytics workspace\n! screenshot of Log analytics workspace Select your workspace\nRun the Query\nopenshift_CL | take 10 ","description":"","tags":["ARO","Azure"],"title":"Using Cluster Logging Forwarder in ARO with Azure Monitor","uri":"/docs/aro/clf-to-azure/"},{"content":"","description":"","tags":null,"title":"ADO","uri":"/tags/ado/"},{"content":"Author: Kevin Collins\nLast edited: 03/14/2023\nAdopted from Hosting an Azure Pipelines Build Agent in OpenShift and Kevin Chung Azure Pipelines OpenShift example Azure DevOps is a very popular DevOps tool that has a host of features including the ability for developers to create CI/CD pipelines.\nIn this document, I will show you how to connect your Managed OpenShift Cluster to Azure DevOps end-to-end including running the pipeline build process in the cluster, setting up the OpenShift internal image registry to store the images, and then finally deploy a sample application. To demonstrate the flexibility of Azure DevOps, I will be deploying to a ROSA cluster, however the same procudures will apply to if you choose to deploy to any other OCP Cluster.\nPrerequisites A Public Cloud subscription (Azure Subscription) Azure Dev Ops instance An OpenShift Cluster (to create an Azure Red Hat OpenShift (ROSA) cluster, click here ) Clone example application and configuration files git clone https://github.com/rh-mobb/azure-pipelines-openshift cd azure-pipelines-openshift Configure Azure Devops While logged into your Azure DevOps Organization , create a new project. The examples in this document will assume the project is named azure-pipelines-openshift Obtain a personal access token. While in Azure DevOPs, select Personal Access Token under User Settings. On the next screen, create a New Token. In this example, we will create a token with Full Access. Once you click create your token will be displayed. Make sure to store it somewhere safe as you won’t be able to see it again.\nSet Azure DevOps environment variables for your Azure DevOps instance AZP_URL=https://dev.azure.com/\u003cyourOrg\u003e AZP_TOKEN=\u003cToken you retrieved above\u003e AZP_POOL=Default Host an Azure Pipelines Build Agent in OpenShift note: this is an abreviated version of this blog by Kevin Chung and Mark Dunnett.\nIn this step, we will configure OpenShift to build our container image leveraging Universal Base Images ( UBI )\nConfigure the Azure DevOps build agent with OpenShift Start by creating a new project\noc new-project azure-build Create the following artifacts that include a wrapper script for the build agent and an example BuildConfig that will build a .NET application using the Red Hat Universal Based Image for .NET\noc create configmap start-sh --from-file=start.sh=assets/start.sh oc create imagestream azure-build-agent oc create -f assets/buildconfig.yaml Update the azure build agent image\noc set env bc/azure-build-agent AZP_AGENT_PACKAGE_LATEST_URL=https://vstsagentpackage.azureedge.net/agent/2.218.1/vsts-agent-linux-x64-2.218.1.tar.gz oc start-build azure-build-agent As a cluster admin, create a service account for the build agent\noc create serviceaccount azure-build-sa oc create -f assets/nonroot-builder.yaml oc adm policy add-scc-to-user nonroot-builder -z azure-build-sa Make sure to be in the root directory of the Azure Pipelines OpenShift git repository you cloned in the previous step.\nCreate a secret with your Azure DevOps credentials\noc create secret generic azdevops \\ --from-literal=AZP_URL=${AZP_URL} \\ --from-literal=AZP_TOKEN=${AZP_TOKEN} \\ --from-literal=AZP_POOL=${AZP_POOL} Deploy the azure build agent\noc create -f assets/deployment.yaml Make sure the build agent is running:\noc get pods expected output:\nNAME READY STATUS RESTARTS AGE azure-build-agent-1-build 0/1 Completed 0 18m azure-build-agent-2-build 0/1 Completed 0 15m azure-build-agent-5d7c455ffd-d2pcc 1/1 Running 0 46s Create an Azure DevOps Pipeline The pipeline we will create has three steps.\nbuild the image on the OpenShift cluster push the image to the internal registry of the OpenShift cluster deploy the application to the cluster Before we can do steps 2 and 3, we need to create a service account in OpenShift to both authenticate with OpenShift to push an image and also the internal image registry.\nCreate a new project for our application\noc new-project ado-openshift Create a service account and grant it cluster-admin privileges\noc create sa azure-sa oc adm policy add-cluster-role-to-user cluster-admin -z azure-sa Create a secret token for the service account\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: azure-sa-secret annotations: kubernetes.io/service-account.name: \"azure-sa\" type: kubernetes.io/service-account-token EOF Retrieve the secret name that we just created that was a token associated with it.\noc get secrets | grep azure-sa-token | awk '{ print $1 }' expected output:\nazure-sa-token-2qrgw note: your output will have a different name\nDescribe the secret to retrieve the token:\noc describe secret \u003csecret name\u003e expected output:\nName: azure-sa-token-2qrgw Namespace: ado-openshift Labels: \u003cnone\u003e Annotations: kubernetes.io/created-by: openshift.io/create-dockercfg-secrets kubernetes.io/service-account.name: azure-sa kubernetes.io/service-account.uid: d361f12e-db7d-412b-9ab8-8ac3a0ba459b Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlFBcmE2b1N5NnA2OUJZcEh2WUVad1BCSGozck9fa2tpaG83bnctM0hUd00ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJhZG8tb3BlbnNoaWZ0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImF6dXJlLXNhLXRva2VuLTJxcmd3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImF6dXJlLXNhIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZDM2MWYxMmUtZGI3ZC00MTJiLTlhYjgtOGFjM2EwYmE0NTliIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmFkby1vcGVuc2hpZnQ6YXp1cmUtc2EifQ.GhLVRAJcG_CHuUxPaz3H_d_E_tGkFK6VaaFv_4UGZiwLLE1Hx-nSIYOA7YsOUvKOkdY2B6fIJrcLUBe5SUjiK0ZePSJZQNry_oZ9xKqhgSRFntxHT5mUR_BXT4cnF5zv0zrT3dvqWcM11mTSs2xfmCx8eACt-uEz2CtLHmxqkvpsiZA2wFQfxekInFFwFhbZSeQk6YBRGFu5f-eawP7qzzDibmo_GmMLHH4uLnpR1CJQFYzI09fEdzSf7IK2UzBgn6dmqpSzHnxLMjgHJVkX66FztJochlGUV8bE4acZk54lu_Xo7OhKxjhiqdeMHFBzq2PeSyvdvSspFME9y6_gXcy1-4QjxLM3t_K9yj7LsJSZKWn8HcmTJy_HoTvpbPtDznz_KEYJH1yX4zdK36D0ocUAb3gBNgfXlsEPAVXYV2o75ZL-AEwpumBv49rRNs_-wZKRO_3eR5zgZWGjZpVoDRb1F_QoFkxy-pnF2sSMQXZOEjwFTapESP182mWZtzzdU8TMOcdK44cr9mYB5IYBmJ2JTRQR2K_iTLfgK-im8O2K5n6OAwWBl4w8mpZDx0eHDp4IBfCBJk2AlopGrQ4TOf-l2bkcEnbJco7ei4D39tRR6xQcPddPEPEDbwIudI9IEzNhyJmHztUnjMV5NaC17hJ05AXWS83nPxFhH_a7pN8 ca.crt: 8717 bytes namespace: 13 bytes service-ca.crt: 9930 bytes Copy the value of the token.\nRetrieve the host of your cluster image regstry.\noc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}' Expected Output:\ndefault-route-openshift-image-registry.apps.ado-rosa.9s68.p1.openshiftapps.com Get the cluster api endpoint\noc cluster-info expected output:\nKubernetes control plane is running at https://api.ado-rosa.9s68.p1.openshiftapps.com:6443 note the api server url for usage later\nConfigure Azure DevOps service connections for the registry and OpenShift From Azure DevOps, click on Project settings ( the gear icon ), then Service Connections, and finally Create service connection.\nSelect Docker registry\nEnter the settings we retrieved in the previous step:\nSelect Others for Docker Registry Docker Registry - make sure to add https:// in front of the hostname you retrieved Docker ID - the service account you created Docker Password - the service account token Service connection name - enter openshift-registry Next, let’s create a another serivce connection for our cluster.\nClick New service connection:\nSelect Kubernetes\nServer Url - the api server you retrieved in the previous step\nSecret - using the name of the secret that we retieved earlier that has token in the name run:\noc get secret azure-sa-token-2qrgw -o json Copy the entire json results and paste it in the Secret field\nService connection name - select openshift\nCreate an Azure DevOps Pipeline We will be deploying an application from a GitHub repo. Fork the following Git Repo: https://github.com/rh-mobb/azure-pipelines-openshift In Azure DevOPs, click on Pipelines and then Create Pipeline\nOn the next screen, select GitHub\nOn the next screen, select your github repo that you forked to.\nReview the azure-pipelines.yml file and then click run.\nIf this is the first time running, you might see a message like the following:\nClickad on the Build and Push Image tile, and then view permissions, and grat permissions to both the Default queue and the openshift-registry service connection.\nLike we saw with permissions on the build and push, we also need to give permissions to deploy.\nOn the next screen, click on Permit to give access to the OpenShift connection.\nAfter a few minutes, you should see both the Build and Push Image and Deploy to DEV stages complete.\nTo verify the application was successfully deployed in openshift run:\noc get pods -n ado-openshift expected output:\nNAME READY STATUS RESTARTS AGE ado-dotnet-74b64db7d5-p8vr7 1/1 Running 0 98s ","description":"","tags":["DevOps","ADO","Azure","ARO","ROSA"],"title":"Azure DevOps with Managed OpenShift","uri":"/docs/misc/azure-dev-ops-with-managed-openshift/"},{"content":"","description":"","tags":null,"title":"DevOps","uri":"/tags/devops/"},{"content":"Background One of the great features of ARO is that you can create ‘disconnected’ clusters with no connectivity to the Internet. Out of the box, the ARO service mirrors all the code repositories to build OpenShift clusters to Azure Container Registry. This means ARO is built without having to reach out to the Internet as the images to build OpenShift are pulled via the Azure private network.\nWhen you upgrade a cluster, OpenShift needs to call out to the Internet to get an upgrade graph to see what options you have to upgrade the cluster. This of course breaks the concept of having a disconnected cluster. This guide goes through how to upgrade ARO without having the cluster reach out to the Internet and maintaining the disconnected nature of an ARO cluster.\nPrerequisites A Private Azure Red Hat OpenShift cluster with no Internet Connectivity Check upgrade path NOTE: This step is VERY important. In a future step, you need to have already validated that the version you are upgrading to is safe to do so. First check which version your cluster is at:\noc get clusterversion version Note the server version.\nNAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.10.40 True False 14h Cluster version is 4.10.40 Verify you are selecting a valid version to upgrade to. Go to https://access.redhat.com/labsinfo/ocpupgradegraph Under Channel, select the stable minor version that you want to upgrade the cluster to. In this example, we have a 4.10 cluster that is at patch level 40 and we want to upgrade it to 4.11. Note that you can also update patch versions.\nOn the next screen, start by selecting the version your cluster is at. In the example below, we’ll select 4.10.40.\nThen select the version you want to upgrade to ensure there is a green line showing the upgrade path is recommended. In the example, we select version 4.11.28.\nUpgrade the cluster NOTE: In step 2 below, you are explicitly telling the cluster to upgrade to an image digest value and must use the --force flag because the cluster has no ability to validate the image digest value without Internet connectivity. Please ensure you have completed the step to check the upgrade path so that you are upgrading the cluster to a version with a supported path from the current cluster version you’re on. Retrieve the image digest of the OpenShift version you want to upgrade to:\nexport VERSION=4.11.28 # Update to your desired version curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/\"${VERSION}\"/release.txt | grep \"Pull From:\" Expected Output:\nPull From: quay.io/openshift-release-dev/ocp-release@sha256:85238bc3eddb88e958535597dbe8ec6f2aa88aa1713c2e1ee7faf88d1fefdac0 Perform the Upgrade\nSet the image to the desired values from the above command.\noc adm upgrade --allow-explicit-upgrade --to-image=quay.io/openshift-release-dev/ocp-release@sha256:1c3913a65b0a10b4a0650f54e545fe928360a94767acea64c0bd10faa52c945a --force Check the status of the scheduled upgrade\noc get clusterversion version When the upgrade is complete you will see the following:\nNAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.11.28 True False 161m Cluster version is 4.11.28 ","description":"","tags":["ARO","Azure"],"title":"Upgrade a disconnected ARO cluster","uri":"/docs/aro/upgrade-disconnected-aro/"},{"content":"It may be desirable to assign a consistent IP address for traffic that leaves the cluster when configuring items such as security groups or other sorts of security controls which require an IP-based configuration. By default, Kubernetes via the OVN-Kubernetes CNI will assign random IP addresses from a pool which will make configuring security lockdowns unpredictable or unnecessarily open. This guide shows you how to configure a set of predictable IP addresses for egress cluster traffic to meet common security standards and guidance and other potential use cases.\nSee the OpenShift documentation on this topic for more information.\nPrerequisites ROSA Cluster openshift-cli (oc) rosa-cli (rosa) jq Demo Set Environment Variables This sets environment variables for the demo so that you do not need to copy/paste in your own. Be sure to replace the values for your desired values for this step:\nexport ROSA_CLUSTER_NAME=cluster export ROSA_MACHINE_POOL_NAME=Default Ensure Capacity For each public cloud provider, there is a limit on the number of IP addresses that may be assigned per node. This may affect the ability to assign an egress IP address. To verify sufficient capacity, you can run the following command to print out the currently assigned IP addresses versus the total capacity in order to identify any nodes which may affected:\noc get node -o json | \\ jq '.items[] | { \"name\": .metadata.name, \"ips\": (.status.addresses | map(select(.type == \"InternalIP\") | .address)), \"capacity\": (.metadata.annotations.\"cloud.network.openshift.io/egress-ipconfig\" | fromjson[] | .capacity.ipv4) }' Example Output:\n{ \"name\": \"ip-10-10-145-88.ec2.internal\", \"ips\": [ \"10.10.145.88\" ], \"capacity\": 14 } { \"name\": \"ip-10-10-154-175.ec2.internal\", \"ips\": [ \"10.10.154.175\" ], \"capacity\": 14 } ... NOTE: the above example uses jq as a friendly filter. If you do not have jq installed, you can review the metadata.annotations['cloud.network.openshift.io/egress-ipconfig'] field of each node manually to verify node capacity.\nCreate the Egress IP Rule(s) NOTE: generally speaking it would be ideal to label the nodes prior to assigning the egress IP addresses, however there is a bug that exists which needs to be fixed first. Once this is fixed, the process and documentation will be re-ordered to address this. See https://issues.redhat.com/browse/OCPBUGS-4969 Identify the Egress IPs Before creating the rules, we should identify which egress IPs that we will use. It should be noted that the egress IPs that you select should exist as a part of the subnets in which the worker nodes are provisioned into.\nReserve the Egress IPs It is recommended, but not required, to reserve the egress IPs that you have requested to avoid conflicts with the AWS VPC DHCP service. To do so, you can request explicit IP reservations by following the AWS documentation for CIDR reservations .\nExample: Assign Egress IP to a Namespace Create a project to demonstrate assigning egress IP addresses based on a namespace selection:\noc new-project demo-egress-ns Create the egress rule. This rule will ensure that egress traffic will be applied to all pods within the namespace that we just created via the spec.namespaceSelector field:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: k8s.ovn.org/v1 kind: EgressIP metadata: name: demo-egress-ns spec: # NOTE: these egress IPs are within the subnet range(s) in which my worker nodes # are deployed. egressIPs: - 10.10.100.253 - 10.10.150.253 - 10.10.200.253 namespaceSelector: matchLabels: kubernetes.io/metadata.name: demo-egress-ns EOF Example: Assign Egress IP to a Pod Create a project to demonstrate assigning egress IP addresses based on a pod selection:\noc new-project demo-egress-pod Create the egress Rule. This rule will ensure that egress traffic will be applied to the pod which we just created using the spec.podSelector field. It should be noted that spec.namespaceSelector is a mandatory field:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: k8s.ovn.org/v1 kind: EgressIP metadata: name: demo-egress-pod spec: # NOTE: these egress IPs are within the subnet range(s) in which my worker nodes # are deployed. egressIPs: - 10.10.100.254 - 10.10.150.254 - 10.10.200.254 namespaceSelector: matchLabels: kubernetes.io/metadata.name: demo-egress-pod podSelector: matchLabels: run: demo-egress-pod EOF Label the Nodes You can run oc get egressips and see that the egress IP assignments are pending. This is due to bug https://issues.redhat.com/browse/OCPBUGS-4969 and will not be an issue once fixed:\nNAME EGRESSIPS ASSIGNED NODE ASSIGNED EGRESSIPS demo-egress-ns 10.10.100.253 demo-egress-pod 10.10.100.254 To complete the egress IP assignment, we need to assign a specific label to the nodes. The egress IP rule that you created in a previous step only applies to nodes with the k8s.ovn.org/egress-assignable label. We want to ensure that label exists on only a specific machinepool as set via an environment variable in the set environment variables step.\nFor ROSA clusters, you can assign labels via the following rosa command:\nWARNING: if you are reliant upon any node labels for your machinepool, this command will replace those labels. Be sure to input your desired labels into the --labels field to ensure your node labels persist.\nrosa update machinepool ${ROSA_MACHINE_POOL_NAME} \\ --cluster=\"${ROSA_CLUSTER_NAME}\" \\ --labels \"k8s.ovn.org/egress-assignable=\" Review the Egress IPs You can review the egress IP assignments by running oc get egressips which will produce output as follows:\nNAME EGRESSIPS ASSIGNED NODE ASSIGNED EGRESSIPS demo-egress-ns 10.10.100.253 ip-10-10-156-122.ec2.internal 10.10.150.253 demo-egress-pod 10.10.100.254 ip-10-10-156-122.ec2.internal 10.10.150.254 Test the Egress IP Rule Create the Demo Service To test the rule, we will create a service which is locked down only to the egress IP addresses in which we have specified. This will simulate an external service which is expecting a small subset of IP addresses\nRun the echoserver which gives us some helpful information:\noc -n default run demo-service --image=gcr.io/google_containers/echoserver:1.4 Expose the pod as a service, limiting the ingress (via the .spec.loadBalancerSourceRanges field) to the service to only the egress IP addresses in which we specified our pods should be using:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Service metadata: name: demo-service namespace: default annotations: service.beta.kubernetes.io/aws-load-balancer-scheme: \"internal\" service.beta.kubernetes.io/aws-load-balancer-internal: \"true\" spec: selector: run: demo-service ports: - port: 80 targetPort: 8080 type: LoadBalancer externalTrafficPolicy: Local # NOTE: this limits the source IPs that are allowed to connect to our service. It # is being used as part of this demo, restricting connectivity to our egress # IP addresses only. # NOTE: these egress IPs are within the subnet range(s) in which my worker nodes # are deployed. loadBalancerSourceRanges: - 10.10.100.254/32 - 10.10.150.254/32 - 10.10.200.254/32 - 10.10.100.253/32 - 10.10.150.253/32 - 10.10.200.253/32 EOF Retrieve the load balancer hostname as the LOAD_BALANCER_HOSTNAME environment variable which you can copy and use for following steps:\nexport LOAD_BALANCER_HOSTNAME=$(oc get svc -n default demo-service -o json | jq -r '.status.loadBalancer.ingress[].hostname') Test Namespace Egress Test the namespace egress rule which was created previously. The following starts an interactive shell which allows you to run curl against the demo service:\noc run \\ demo-egress-ns \\ -it \\ --namespace=demo-egress-ns \\ --env=LOAD_BALANCER_HOSTNAME=$LOAD_BALANCER_HOSTNAME \\ --image=registry.access.redhat.com/ubi9/ubi -- \\ bash Once inside the pod, you can send a request to the load balancer, ensuring that you can successfully connect:\ncurl -s http://$LOAD_BALANCER_HOSTNAME You should see output similar to the following, indicating a successful connection. It should be noted that that client_address below is the internal IP address of the load balancer rather than our egress IP. Successful connectivity (by limiting the service to .spec.loadBalancerSourceRanges) is what provides a successful demonstration:\nCLIENT VALUES: client_address=10.10.207.247 command=GET real path=/ query=nil request_version=1.1 request_uri=http://internal-a3e61de18bfca4a53a94a208752b7263-148284314.us-east-1.elb.amazonaws.com:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=internal-a3e61de18bfca4a53a94a208752b7263-148284314.us-east-1.elb.amazonaws.com user-agent=curl/7.76.1 BODY: -no body in request- You can safely exit the pod once you are done:\nexit Test Pod Egress Test the pod egress rule which was created previously. The following starts an interactive shell which allows you to run curl against the demo service:\noc run \\ demo-egress-pod \\ -it \\ --namespace=demo-egress-pod \\ --env=LOAD_BALANCER_HOSTNAME=$LOAD_BALANCER_HOSTNAME \\ --image=registry.access.redhat.com/ubi9/ubi -- \\ bash Once inside the pod, you can send a request to the load balancer, ensuring that you can successfully connect:\ncurl -s http://$LOAD_BALANCER_HOSTNAME You should see output similar to the following, indicating a successful connection. It should be noted that that client_address below is the internal IP address of the load balancer rather than our egress IP. Successful connectivity (by limiting the service to .spec.loadBalancerSourceRanges) is what provides a successful demonstration:\nCLIENT VALUES: client_address=10.10.207.247 command=GET real path=/ query=nil request_version=1.1 request_uri=http://internal-a3e61de18bfca4a53a94a208752b7263-148284314.us-east-1.elb.amazonaws.com:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=internal-a3e61de18bfca4a53a94a208752b7263-148284314.us-east-1.elb.amazonaws.com user-agent=curl/7.76.1 BODY: -no body in request- You can safely exit the pod once you are done:\nexit Test Blocked Egress Alternatively to a successful connection, you can see that the traffic is successfully blocked when the egress rules do not apply. Unsuccessful connectivity (by limiting the service to .spec.loadBalancerSourceRanges) is what provides a successful demonstration in this scenario:\noc run \\ demo-egress-pod-fail \\ -it \\ --namespace=demo-egress-pod \\ --env=LOAD_BALANCER_HOSTNAME=$LOAD_BALANCER_HOSTNAME \\ --image=registry.access.redhat.com/ubi9/ubi -- \\ bash Once inside the pod, you can send a request to the load balancer:\ncurl -s http://$LOAD_BALANCER_HOSTNAME The above command should hang. You can safely exit the pod once you are done:\nexit Cleanup You can cleanup your cluster by running the following commands:\noc delete svc demo-service -n default; \\ oc delete pod demo-service -n default; \\ oc delete project demo-egress-ns; \\ oc delete project demo-egress-pod; \\ oc delete egressip demo-egress-ns; \\ oc delete egressip demo-egress-pod You can cleanup the assigned node labels by running the following commands:\nWARNING: if you are reliant upon any node labels for your machinepool, this command will replace those labels. Be sure to input your desired labels into the --labels field to ensure your node labels persist.\nrosa update machinepool ${ROSA_MACHINE_POOL_NAME} \\ --cluster=\"${ROSA_CLUSTER_NAME}\" \\ --labels \"\" ","description":"","tags":["OSD","ROSA","ARO"],"title":"Assign Consistent Egress IP for External Traffic","uri":"/docs/rosa/egress-ip/"},{"content":"","description":"","tags":null,"title":"OSD","uri":"/tags/osd/"},{"content":"","description":"","tags":null,"title":"GPU","uri":"/tags/gpu/"},{"content":"ROSA guide to running Nvidia GPU workloads.\nPrerequisites ROSA Cluster (4.10+) rosa cli #logged-in oc cli #logged-in-cluster-admin jq If you need to install a ROSA cluster, please read our ROSA Quickstart Guide . Please be sure you are installing or using an existing ROSA cluster that it is 4.10.x or higher.\nAs of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads.\nEnter the oc login command, username, and password from the output of the previous command:\nExample login:\noc login https://api.cluster_name.t6k4.i1.organization.org:6443 \\ \u003e --username cluster-admin \\ \u003e --password mypa55w0rd Login successful. You have access to 77 projects, the list has been suppressed. You can list all projects with ' projects' Linux:\nsudo dnf install jq MacOS\nbrew install jq Helm Prerequisites If you plan to use Helm to deploy the GPU operator, you will need to do the following\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update GPU Quota View the list of supported GPU instance types in ROSA\nrosa list instance-types | grep accelerated Select a GPU instance type\nThe guide uses g5.xlarge as an example. Please be mindful of the GPU cost of the type you choose.\nexport GPU_INSTANCE_TYPE='g5.xlarge' Login to AWS\nLogin to AWS Console , type “quotas” in search by, click on “Service Quotas” -\u003e “AWS services” -\u003e “Amazon Elastic Compute Cloud (Amazon EC2). Search for “Running On-Demand [instance-family] instances” (e.g. Running On-Demand G and VT instances).\nPlease remember that when you request quota that AWS is per core. As an example, to request a single g5.xlarge, you will need to request quota in groups of 4; to request a single g5.8xlarge, you will need to request quota in groups of 32.\nVerify quota and request increase if necessary\nGPU Machine Pool Set environment variables\nexport CLUSTER_NAME=\u003cYOUR-CLUSTER\u003e export MACHINE_POOL_NAME=nvidia-gpu-pool export MACHINE_POOL_REPLICA_COUNT=1 Create GPU machine pool\nrosa create machinepool --cluster=$CLUSTER_NAME \\ --name=$MACHINE_POOL_NAME \\ --replicas=$MACHINE_POOL_REPLICA_COUNT \\ --instance-type=$GPU_INSTANCE_TYPE Verify GPU machine pool\nIt may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the AWS Console and ensure you didn’t run across availability issues. You can go to EC2 and search for instances by cluster name to see the instance state.\noc wait --for=jsonpath='{.status.readyReplicas}'=1 machineset \\ -l hive.openshift.io/machine-pool=$MACHINE_POOL_NAME \\ -n openshift-machine-api --timeout=600s Install and Configure Nvidia GPU This section configures the Node Feature Discovery Operator (to allow OpenShift to discover the GPU nodes) and the Nvidia GPU Operator.\nTwo options: Helm or Manual Helm Create namespaces\noc create namespace openshift-nfd oc create namespace nvidia-gpu-operator Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \\ mobb/operatorhub --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml Wait until the two operators are running\noc rollout status deploy/nfd-controller-manager -n openshift-nfd --timeout=300s oc rollout status deploy/gpu-operator -n nvidia-gpu-operator --timeout=300s Install the Nvidia GPU Operator chart\nhelm upgrade --install -n nvidia-gpu-operator nvidia-gpu \\ mobb/nvidia-gpu --disable-openapi-validation Wait until NFD instances are ready\nNOTE: If you are deploying ROSA in single-AZ change the replicas from 3 to 1 nfd-master\noc wait --for=jsonpath='{.status.availableReplicas}'=3 -l app=nfd-master deployment -n openshift-nfd oc wait --for=jsonpath='{.status.numberReady}'=5 -l app=nfd-worker ds -n openshift-nfd Wait until Cluster Policy is ready\noc wait --for=jsonpath='{.status.state}'=ready clusterpolicy \\ gpu-cluster-policy -n nvidia-gpu-operator --timeout=600s Skip to Validate GPU Manually Install Nvidia GPU Operator Create Nvidia namespace\noc create namespace nvidia-gpu-operator Create Operator Group\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Get latest nvidia channel\nCHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}') Get latest nvidia package\nPACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"'$CHANNEL'\") | .currentCSV') Create Subscription\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"$CHANNEL\" installPlanApproval: Automatic name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"$PACKAGE\" EOF Wait for Operator to finish installing\noc rollout status deploy/gpu-operator -n nvidia-gpu-operator --timeout=300s Install Node Feature Discovery Operator The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads. We’ll install the NFD operator into the opneshift-ndf namespace and create the “subscription” which is the configuration for NFD.\nOfficial Documentation for Installing Node Feature Discovery Operator Set up namespace\noc create namespace openshift-nfd Create OperatorGroup\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd EOF Create Subscription\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait for Node Feature discovery to complete installation\noc rollout status deploy/nfd-controller-manager -n openshift-nfd --timeout=300s Create NFD Instance\ncat \u003c\u003cEOF | oc apply -f - kind: NodeFeatureDiscovery apiVersion: nfd.openshift.io/v1 metadata: name: nfd-instance namespace: openshift-nfd spec: customConfig: configData: | # - name: \"more.kernel.features\" # matchOn: # - loadedKMod: [\"example_kmod3\"] # - name: \"more.features.by.nodename\" # value: customValue # matchOn: # - nodename: [\"special-.*-node-.*\"] operand: image: \u003e- registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b servicePort: 12000 workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time ## configurable and require a nfd-worker restart to take effect ## after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: # cpu: # cpuid: ## NOTE: whitelist has priority over blacklist # attributeBlacklist: # - \"BMI1\" # - \"BMI2\" # - \"CLMUL\" # - \"CMOV\" # - \"CX16\" # - \"ERMS\" # - \"F16C\" # - \"HTT\" # - \"LZCNT\" # - \"MMX\" # - \"MMXEXT\" # - \"NX\" # - \"POPCNT\" # - \"RDRAND\" # - \"RDSEED\" # - \"RDTSCP\" # - \"SGX\" # - \"SSE\" # - \"SSE2\" # - \"SSE3\" # - \"SSE4.1\" # - \"SSE4.2\" # - \"SSSE3\" # attributeWhitelist: # kernel: # kconfigFile: \"/path/to/kconfig\" # configOpts: # - \"NO_HZ\" # - \"X86\" # - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: # - \"class\" - \"vendor\" # - \"device\" # - \"subsystem_vendor\" # - \"subsystem_device\" # usb: # deviceClassWhitelist: # - \"0e\" # - \"ef\" # - \"fe\" # - \"ff\" # deviceLabelFields: # - \"class\" # - \"vendor\" # - \"device\" # custom: # - name: \"my.kernel.feature\" # matchOn: # - loadedKMod: [\"example_kmod1\", \"example_kmod2\"] # - name: \"my.pci.feature\" # matchOn: # - pciId: # class: [\"0200\"] # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # - pciId : # vendor: [\"8086\"] # device: [\"1000\", \"1100\"] # - name: \"my.usb.feature\" # matchOn: # - usbId: # class: [\"ff\"] # vendor: [\"03e7\"] # device: [\"2485\"] # - usbId: # class: [\"fe\"] # vendor: [\"1a6e\"] # device: [\"089a\"] # - name: \"my.combined.feature\" # matchOn: # - pciId: # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"] EOF Wait until NFD instances are ready\noc wait --for=jsonpath='{.status.numberReady}'=3 -l app=nfd-master ds -n openshift-nfd oc wait --for=jsonpath='{.status.numberReady}'=5 -l app=nfd-worker ds -n openshift-nfd Apply nVidia Cluster Config We’ll now apply the nvidia cluster config. Please read the nvidia documentation on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete.\nCreate cluster config\ncat \u003c\u003cEOF | oc create -f - apiVersion: nvidia.com/v1 kind: ClusterPolicy metadata: name: gpu-cluster-policy spec: migManager: enabled: true operator: defaultRuntime: crio initContainer: {} runtimeClass: nvidia deployGFD: true dcgm: enabled: true gfd: {} dcgmExporter: config: name: '' driver: licensingConfig: nlsEnabled: false configMapName: '' certConfig: name: '' kernelModuleConfig: name: '' repoConfig: configMapName: '' virtualTopology: config: '' enabled: true use_ocp_driver_toolkit: true devicePlugin: {} mig: strategy: single validator: plugin: env: - name: WITH_WORKLOAD value: 'true' nodeStatusExporter: enabled: true daemonsets: {} toolkit: enabled: true EOF Wait until Cluster Policy is ready\noc wait --for=jsonpath='{.status.state}'=ready clusterpolicy \\ gpu-cluster-policy -n nvidia-gpu-operator --timeout=600s Validate GPU Verify NFD can see your GPU(s)\noc describe node -l node.kubernetes.io/instance-type=$GPU_INSTANCE_TYPE \\ | egrep 'Roles|pci-10de' | grep -v master You should see output like:\nRoles: worker feature.node.kubernetes.io/pci-10de.present=true Verify GPU Operator added node label to your GPU nodes\noc get node -l nvidia.com/gpu.present [Optional] Test GPU access using Nvidia SMI\noc project nvidia-gpu-operator for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\\n' ; done You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type)\nCreate Pod to run a GPU workload\noc project nvidia-gpu-operator cat \u003c\u003cEOF | oc create -f - apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"nvidia/samples:vectoradd-cuda11.2.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: nvidia.com/gpu.present: true EOF View logs\noc logs cuda-vector-add --tail=-1 Please note, if you get an error “Error from server (BadRequest): container “cuda-vector-add” in pod “cuda-vector-add” is waiting to start: ContainerCreating” try running “oc delete pod cuda-vector-add” and then re-run the create statement above. We’ve seen issues where if this step is ran before all of the operator consolidation is done it may just sit there.\nYou should see Output like the following (mary vary depending on GPU):\n[Vector addition of 5000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done If successful, the pod can be deleted\noc delete pod cuda-vector-add ","description":"","tags":["AWS","ROSA","GPU"],"title":"ROSA with Nvidia GPU Workloads","uri":"/docs/rosa/gpu/"},{"content":"Configuring the Custom Domain Operator requires a wildcard CNAME DNS record in your Route53 Hosted Zone. If you do not wish to use a wildcard record, you can use the External DNS Operator to create individual entries for routes.\nThis document will guide you through deploying and configuring the External DNS Operator with a Custom Domain in ROSA.\nImportant Note: The ExternalDNS Operator does not support STS yet and uses long lived IAM credentials. This guide will be updated once STS is supported.\nPrerequisites ROSA Cluster AWS CLI Route53 Hosted Zone A domain Deploy Setup Environment Set your email and domain export EMAIL=\u003cYOUR-EMAIL\u003e export DOMAIN=\u003cYOUR-DOMAIN\u003e Set remaining environment variables export SCRATCH_DIR=/tmp/scratch export ZONE_ID=$(aws route53 list-hosted-zones-by-name --output json \\ --dns-name \"$DOMAIN.\" --query 'HostedZones[0]'.Id --out text | sed 's/\\/hostedzone\\///') mkdir -p $SCRATCH_DIR Custom Domain Check out the dynamic certificates guide if you do not want to use a wildcard certificate.\nCreate TLS Key Pair for custom domain using certbot:\nSkip this if you already have a key pair.\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" \\ -d \"*.$DOMAIN\" Create TLS secret for custom domain:\nNote use your own keypair paths if not using certbot.\nCERTS=/tmp/scratch/config/live/$DOMAIN oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem Create Custom Domain resource:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait for the domain to be ready:\noc wait --for=condition=Ready customdomains/acme --timeout=300s External DNS Deploy the External DNS Operator:\noc new-project external-dns-operator cat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: external-dns-group namespace: external-dns-operator spec: targetNamespaces: - external-dns-operator --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: external-dns-operator namespace: external-dns-operator spec: channel: stable-v1 installPlanApproval: Automatic name: external-dns-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait until the Operator is running:\noc rollout status deploy external-dns-operator --timeout=300s Create IAM Policy document that allows ExternalDNS to update Route53 only in your hosted zone:\ncat \u003c\u003c EOF \u003e $SCRATCH_DIR/externaldns-r53-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\" ], \"Resource\": [ \"arn:aws:route53:::hostedzone/$ZONE_ID\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ListHostedZones\", \"route53:ListResourceRecordSets\" ], \"Resource\": [ \"*\" ] } ] } EOF Create IAM Policy:\nPOLICY_ARN=$(aws iam create-policy --policy-name \"AllowExternalDNSUpdates\" \\ --policy-document file://$SCRATCH_DIR/externaldns-r53-policy.json \\ --query 'Policy.Arn' --output text) Create IAM user and attach policy:\nNote: This will be changed to STS using IRSA in the future.\naws iam create-user --user-name \"externaldns\" aws iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN Create aws keys for IAM user:\nSECRET_ACCESS_KEY=$(aws iam create-access-key --user-name \"externaldns\") Create static credentials:\ncat \u003c\u003c EOF \u003e $SCRATCH_DIR/credentials [default] aws_access_key_id = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId') aws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey') EOF Create secret from credentials:\noc create secret generic external-dns \\ --namespace external-dns-operator --from-file $SCRATCH_DIR/credentials Deploy ExternalDNS controller:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: externaldns.olm.openshift.io/v1beta1 kind: ExternalDNS metadata: name: $DOMAIN spec: domains: - filterType: Include matchType: Exact name: $DOMAIN provider: aws: credentials: name: external-dns type: AWS source: openshiftRouteOptions: routerName: acme type: OpenShiftRoute zones: - $ZONE_ID EOF Wait until the controller is running:\noc rollout status deploy external-dns-$DOMAIN --timeout=300s Test Create a new route to OpenShift console using your domain:\noc create route reencrypt --service=console console-acme \\ --hostname console.$DOMAIN -n openshift-console Check if DNS record was created automatically by ExternalDNS:\nIt may take a few minutes for the record to appear in Route53\naws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Type == 'CNAME']\" | grep console You can also view the TXT records that indicate they were created by ExternalDNS:\naws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Type == 'TXT']\" | grep $DOMAIN Navigate to your custom console domain in the browser and you should see OpenShift login.\necho console.$DOMAIN ","description":"","tags":["AWS","ROSA"],"title":"External DNS for ROSA Custom Domain","uri":"/docs/rosa/external-dns/"},{"content":"AWS Load Balancer Controller is a controller to help manage Elastic Load Balancers for a Kubernetes cluster.\nIt satisfies Kubernetes Ingress resources by provisioning Application Load Balancers . It satisfies Kubernetes Service resources by provisioning Network Load Balancers . Compared with default AWS In Tree Provider, this controller is actively developed with advanced annotations for both ALB and NLB . Some advanced usecases are:\nUsing native kubernetes ingress with ALB Integrate ALB with WAF Specify NLB source IP ranges Specify NLB internal IP address AWS Load Balancer Operator is used to used to install, manage and configure an instance of aws-load-balancer-controller in a OpenShift cluster.\nPrerequisites A multi AZ ROSA cluster deployed with STS AWS CLI OC CLI Environment Prepare the environment variables\nexport AWS_PAGER=\"\" export ROSA_CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\" | sed 's/-[a-z0-9]\\{5\\}$//') export REGION=$(oc get infrastructure cluster -o=jsonpath=\"{.status.platformStatus.aws.region}\") export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o jsonpath='{.spec.serviceAccountIssuer}' | sed 's|^https://||') export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/alb-operator\" mkdir -p ${SCRATCH} echo \"Cluster: ${ROSA_CLUSTER_NAME}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" AWS VPC / Subnets Note: This section only applies to BYO VPC clusters, if you let ROSA create your VPCs you can skip to the following Installation section.\nSet Variables describing your VPC and Subnets:\nexport VPC_ID=\u003cvpc-id\u003e export PUBLIC_SUBNET_IDS=\u003cpublic-subnets\u003e export PRIVATE_SUBNET_IDS=\u003cprivate-subnets\u003e export CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\") Tag VPC with the cluster name\naws ec2 create-tags --resources ${VPC_ID} --tags Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=owned --region ${REGION} Add tags to Public Subnets\naws ec2 create-tags \\ --resources ${PUBLIC_SUBNET_IDS} \\ --tags Key=kubernetes.io/role/elb,Value='' \\ --region ${REGION} Add tags to Private Subnets\naws ec2 create-tags \\ --resources \"${PRIVATE_SUBNET_IDS}\" \\ --tags Key=kubernetes.io/role/internal-elb,Value='' \\ --region ${REGION} Installation Create Policy for the aws load balancer controller\nNote: Policy is from AWS Load Balancer Controller Policy plus subnet create tags permission (required by the operator)\noc new-project aws-load-balancer-operator POLICY_ARN=$(aws iam list-policies --query \\ \"Policies[?PolicyName=='aws-load-balancer-operator-policy'].{ARN:Arn}\" \\ --output text) if [[ -z \"${POLICY_ARN}\" ]]; then wget -O \"${SCRATCH}/load-balancer-operator-policy.json\" \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/content/docs/rosa/aws-load-balancer-operator/load-balancer-operator-policy.json POLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name aws-load-balancer-operator-policy \\ --policy-document \"file://${SCRATCH}/load-balancer-operator-policy.json\") fi echo $POLICY_ARN Create trust policy for ALB Operator\ncat \u003c\u003cEOF \u003e \"${SCRATCH}/trust-policy.json\" { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Condition\": { \"StringEquals\" : { \"${OIDC_ENDPOINT}:sub\": [\"system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-operator-controller-manager\", \"system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-controller-cluster\"] } }, \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create Role for ALB Operator\nROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --assume-role-policy-document \"file://${SCRATCH}/trust-policy.json\" \\ --query Role.Arn --output text) echo $ROLE_ARN aws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --policy-arn $POLICY_ARN Create secret for ALB Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator stringData: credentials: | [default] role_arn = $ROLE_ARN web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install Red Hat AWS Load Balancer Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator spec: upgradeStrategy: Default --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator spec: channel: stable-v1.0 installPlanApproval: Automatic name: aws-load-balancer-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: aws-load-balancer-operator.v1.0.0 EOF Install Red Hat AWS Load Balancer Controller\nNote: If you get an error here wait a minute and try again, it likely means the Operator hasn’t completed installing yet.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: networking.olm.openshift.io/v1 kind: AWSLoadBalancerController metadata: name: cluster spec: credentials: name: aws-load-balancer-operator EOF Check the Operator and Controller pods are both running\noc -n aws-load-balancer-operator get pods You should see the following, if not wait a moment and retry.\nNAME READY STATUS RESTARTS AGE aws-load-balancer-controller-cluster-6ddf658785-pdp5d 1/1 Running 0 99s aws-load-balancer-operator-controller-manager-577d9ffcb9-w6zqn 2/2 Running 0 2m4s Validate the deployment with Echo Server application Deploy Echo Server Ingress with ALB\noc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-namespace.yaml oc adm policy add-scc-to-user anyuid system:serviceaccount:echoserver:default oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-deployment.yaml oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-service.yaml oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-ingress.yaml Curl the ALB ingress endpoint to verify the echoserver pod is accessible\nINGRESS=$(oc -n echoserver get ingress echoserver \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') curl -sH \"Host: echoserver.example.com\" \\ \"http://${INGRESS}\" | grep Hostname Hostname: echoserver-7757d5ff4d-ftvf2 Deploy Echo Server NLB Load Balancer\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Service metadata: name: echoserver-nlb namespace: echoserver annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: LoadBalancer selector: app: echoserver EOF Test the NLB endpoint\nNLB=$(oc -n echoserver get service echoserver-nlb \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') curl -s \"http://${NLB}\" | grep Hostname Hostname: echoserver-7757d5ff4d-ftvf2 Clean Up Delete the Operator and the AWS Roles\noc delete subscription aws-load-balancer-operator -n aws-load-balancer-operator aws iam detach-role-policy \\ --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --policy-arn $POLICY_ARN aws iam delete-role \\ --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" If you wish to delete the policy you can run\naws iam delete-policy --policy-arn $POLICY_ARN ","description":"","tags":["AWS","ROSA"],"title":"AWS Load Balancer Operator On ROSA","uri":"/docs/rosa/aws-load-balancer-operator/"},{"content":"Please refer to the The Managed OpenShift Black Belt team maintained Helm chart at here .\n","description":"","tags":["ARO","Azure"],"title":"Helm Chart to set up extra MachineSets on ARO clusters","uri":"/docs/aro/aro-machinesets/readme/"},{"content":"This document explain how to integrate ARO cluster with Azure Arc-enabled Kubernetes. When you connect a Kubernetes/OpenShift cluster with Azure Arc, it will:\nBe represented in Azure Resource Manager with a unique ID Be placed in an Azure subscription and resource group Receive tags just like any otherAzure resource Azure Arc-enabled Kubernetes supports the following scenarios for connected clusters:\nConnect Kubernetes running outside of Azure for inventory, grouping, and tagging. Deploy applications and apply configuration using GitOps-based configuration management. View and monitor your clusters using Azure Monitor for containers. Enforce threat protection using Microsoft Defender for Kubernetes. Apply policy definitions using Azure Policy for Kubernetes. Use Azure Active Directory for authentication and authorization checks on your cluster Prerequisites a public ARO cluster azure cli oc cli An identity (user or service principal) which can be used to log in to Azure CLI and connect your cluster to Azure Arc. Enable Extensions and Plugins Install the connectedk8s Azure Cli extension of version \u003e= 1.2.0\naz extension add --name \"connectedk8s\" az extension add --name \"k8s-configuration\" az extension add --name \"k8s-extension\" Register providers for Azure Arc-enabled Kubernetes. Registration may take up to 5 minutes.\naz provider register --namespace Microsoft.Kubernetes az provider register --namespace Microsoft.KubernetesConfiguration az provider register --namespace Microsoft.ExtendedLocation Connect an existing ARO cluster Make sure you are logged into your ARO cluster\nkubeadmin_password=$(az aro list-credentials --name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group name\u003e\u003e --query kubeadminPassword --output tsv) apiServer=$(az aro show -g \u003c\u003cresource group name\u003e\u003e -n \u003c\u003ccluster name\u003e\u003e --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password Run the following command:\naz connectedk8s connect --resource-group $resourceGroupName --name $clusterName --distribution openshift --infrastructure auto After running the commnad. grant the following permissions and restart kube-aad-proxy pod\noc project azure-arc oc adm policy add-scc-to-user privileged system:serviceaccount:azure-arc:azure-arc-kube-aad-proxy-sa oc get pod | grep kube-aad-proxy-6d9b66b9cd-g27xr 0/2 ContainerCreating 0 26s oc delete pod kube-aad-proxy-6d9b66b9cd-g27xr Wait for a few mins and you will see all the pods in azure-arc namespace running\noc get pods NAME READY STATUS RESTARTS AGE cluster-metadata-operator-7dfd94949c-wtvjw 2/2 Running 0 4m47s clusterconnect-agent-7d78db9859-wzthd 3/3 Running 0 4m47s clusteridentityoperator-7b96bcb448-hzthh 2/2 Running 0 4m47s config-agent-dbf66bbc7-r27qs 2/2 Running 0 4m47s controller-manager-67547546f-cmlb9 2/2 Running 0 4m47s extension-manager-548c9d7d6b-jrrdn 2/2 Running 0 4m47s flux-logs-agent-bb994c74f-m5gdc 1/1 Running 0 4m47s kube-aad-proxy-6d9b66b9cd-g27xr 2/2 Running 0 3m16s metrics-agent-7d794679c6-k4b7g 2/2 Running 0 4m47s resource-sync-agent-bb79c44b8-5brjr 2/2 Running 0 4m47s This commands take about 5 mins to complete. Upon the completion of the command you should see the following output and your cluster under Kubernetes - Azure Arc service in Azure Portal\n{ \"agentPublicKeyCertificate\": \"MIICCgKCAgEArNXWSoWVg7q/W5t7vwY24Y8c+dRxy3we/EIRryXx1Orl8GEX94BsHJqvP0iW6ANZ0qoWE675+NR6V3nDMSkis5/aSYMQ8/yWMcUzieKwFfFmTSfCpkzwxy6PSbdRjMwK5H3DDOOXyRQcJV557F5FjHVYfC/0DkPYdhfepcVade+HgOwOOJH28hSNw58pWo/GNNmcwtzFPVdx/TM574CbNVz4OdrtsMy7FKKC63lYW+W3wkzFOqB+qPaITwqwzkruIoSi5HIatONoCPijdTLm3+RoK/CbTYqzHEEId8gFFJd+J4qfSeCYu6jeDNOpwt8DKDLFLvv04oHyxm+Nr34xPBm3+sjggvkLQ5UWpGZ9h7jWTEP2pWEcXF0KqAqAEFPBOOqDKEaYfLtJSJ/yExS1otydDCJEZ1sRPvsjdH5f0DKVXPHgiDa4SoLXomqkarF3g9i6CEK/XE9JTVa8WBJT6wXdXBa0xh8EnzZ9uyVuY1k/2L7d4BR5+sIjqtcDfRSVtxN+LNxgqpo20ltXM1hWkd8WacK7VY+t2lxbYf01zhXWOpaBGgeAMqxqqcHeQor2vzA9PENYYr5zo8eP1LcySmC4LIFiDfN1NxAiZ5SCnrorNFbmrgEDFnWvZzdu2w4r55fsV9qnozUjn6iRqByhyMoeLn5EZLLK5zsW8sA/CeUCAwEAAQ==\", \"agentVersion\": null, \"connectivityStatus\": \"Connecting\", \"distribution\": \"OpenShift\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": \"xxxx-xxxx-xxxx-xxxx\", \"type\": \"SystemAssigned\" }, \"infrastructure\": \"azure\", \"kubernetesVersion\": null, \"lastConnectivityTime\": null, \"location\": \"eastus\", \"managedIdentityCertificateExpirationTime\": null, \"name\": \"sazed-aro-cluster\", \"offering\": null, \"provisioningState\": \"Succeeded\", \"resourceGroup\": \"sazed-aro-cluster\", \"systemData\": { \"createdAt\": \"2022-09-15T19:23:40.540376+00:00\", \"createdBy\": \"sazed@redhat.com\", \"createdByType\": \"User\", \"lastModifiedAt\": \"2022-09-15T19:23:40.540376+00:00\", \"lastModifiedBy\": \"sazed@redhat.com\", \"lastModifiedByType\": \"User\" }, \"tags\": {}, \"totalCoreCount\": null, \"totalNodeCount\": null, \"type\": \"microsoft.kubernetes/connectedclusters\" } To check the status of clusters connected to Azure ARC, run the following command\naz connectedk8s list --resource-group \u003c\u003cresource group\u003e\u003e --output table Name Location ResourceGroup ------------------- ---------- ------------------- \u003c\u003c cluster name \u003e\u003e\u003e eastus \u003c\u003c resource group \u003e\u003e Enable observability In order to see ARO resource inside Azure Arc, you need to create a service account and provide it to Azure Arc.\noc project azure-arc oc create serviceaccount azure-arc-observability oc create clusterrolebinding azure-arc-observability-rb --clusterrole cluster-admin --serviceaccount azure-arc:azure-arc-observability apiVersion: v1 kind: Secret metadata: name: azure-arc-observability-secret namespace: azure-arc annotations: kubernetes.io/service-account.name: azure-arc-observability type: kubernetes.io/service-account-token oc apply -f azure-arc-secret.yaml TOKEN=$(oc get secret azure-arc-observability-secret -o jsonpath='{$.data.token}' | base64 -d | sed 's/$/\\\\n/g') echo $TOKEN Copy the token, goto Azure portal and select your cluster under “Kubernetes - Azure Arc” Select Namespaces from the left side menu and paste the token in “Service account bearer token” input field.\nNow you can see all of your ARO rearouses inside ARC UI. you can see the following resources inside Azure ARC portal:\nNamespaces Workloads Services and Ingress Storage Configurations Access Secrets from Azure Key Vault The Azure Key Vault Provider for Secrets Store CSI Driver allows for the integration of Azure Key Vault as a secrets store with a Kubernetes cluster via a CSI volume. For Azure Arc-enabled Kubernetes clusters, you can install the Azure Key Vault Secrets Provider extension to fetch secrets.\nInstall extension az k8s-extension create --cluster-name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group\u003e\u003e --cluster-type connectedClusters --extension-type Microsoft.AzureKeyVaultSecretsProvider --name akvsecretsprovider { \"aksAssignedIdentity\": null, \"autoUpgradeMinorVersion\": true, \"configurationProtectedSettings\": {}, \"configurationSettings\": {}, \"customLocationSettings\": null, \"errorInfo\": null, \"extensionType\": \"microsoft.azurekeyvaultsecretsprovider\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster-1/providers/Microsoft.KubernetesConfiguration/extensions/akvsecretsprovider\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": null, \"type\": \"SystemAssigned\" }, \"installedVersion\": null, \"name\": \"akvsecretsprovider\", \"packageUri\": null, \"provisioningState\": \"Succeeded\", \"releaseTrain\": \"Stable\", \"resourceGroup\": \"sazed-aro-cluster\", \"scope\": { \"cluster\": { \"releaseNamespace\": \"kube-system\" }, \"namespace\": null }, \"statuses\": [], \"systemData\": { \"createdAt\": \"2022-09-15T20:45:47.152390+00:00\", \"createdBy\": null, \"createdByType\": null, \"lastModifiedAt\": \"2022-09-15T20:45:47.152390+00:00\", \"lastModifiedBy\": null, \"lastModifiedByType\": null }, \"type\": \"Microsoft.KubernetesConfiguration/extensions\", \"version\": \"1.3.0\" } Validate the extension installation\naz k8s-extension show --cluster-type connectedClusters --cluster-name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group\u003e\u003e --name akvsecretsprovider { \"aksAssignedIdentity\": null, \"autoUpgradeMinorVersion\": true, \"configurationProtectedSettings\": {}, \"configurationSettings\": {}, \"customLocationSettings\": null, \"errorInfo\": null, \"extensionType\": \"microsoft.azurekeyvaultsecretsprovider\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster-1/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster-1/providers/Microsoft.KubernetesConfiguration/extensions/akvsecretsprovider\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": null, \"type\": \"SystemAssigned\" }, \"installedVersion\": null, \"name\": \"akvsecretsprovider\", \"packageUri\": null, \"provisioningState\": \"Succeeded\", \"releaseTrain\": \"Stable\", \"resourceGroup\": \"sazed-aro-cluster\", \"scope\": { \"cluster\": { \"releaseNamespace\": \"kube-system\" }, \"namespace\": null }, \"statuses\": [], \"systemData\": { \"createdAt\": \"2022-09-15T20:45:47.152390+00:00\", \"createdBy\": null, \"createdByType\": null, \"lastModifiedAt\": \"2022-09-15T20:45:47.152390+00:00\", \"lastModifiedBy\": null, \"lastModifiedByType\": null }, \"type\": \"Microsoft.KubernetesConfiguration/extensions\", \"version\": \"1.3.0\" } Create or Select an Azure Key Vault az keyvault create -n \u003c\u003ccluster name\u003e\u003e -g \u003c\u003cresource group\u003e\u003e -l eastus az keyvault secret set --vault-name \u003c\u003ccluster name\u003e\u003e -n DemoSecret --value MyExampleSecret Provide identity to access Azure Key Vault Currently, the Secrets Store CSI Driver on Arc-enabled clusters can be accessed through a service principal. Follow the steps below to provide an identity that can access your Key Vault.\nUse the provided Service Principal credentials provided with the lab and create a secret in ARO cluster\noc create secret generic secrets-store-creds --from-literal clientid=\"\u003cclient-id\u003e\" --from-literal clientsecret=\"\u003cclient-secret\u003e\" oc label secret secrets-store-creds secrets-store.csi.k8s.io/used=true Create a SecretProviderClass with the following YAML, filling in your values for key vault name, tenant ID, and objects to retrieve from your AKV instance\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: akvprovider-demo spec: provider: azure parameters: usePodIdentity: \"false\" keyvaultName: \u003ckey-vault-name\u003e objects: | array: - | objectName: DemoSecret objectType: secret objectVersion: \"\" tenantId: \u003ctenant-Id\u003e oc apply -f azure-arc-secretproviderclass.yaml Create a pod with the following YAML, filling in the name of your identity\nkind: Pod apiVersion: v1 metadata: name: secret-store-pod spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"akvprovider-demo\" nodePublishSecretRef: name: secrets-store-creds oc apply -f azure-arc-pod.yaml Validate the secrets After the pod starts, the mounted content at the volume path specified in your deployment YAML is available.\n## show secrets held in secrets-store oc exec secret-store-pod -- ls /mnt/secrets-store/ DemoSecret ## print a test secret 'DemoSecret' held in secrets-store oc exec secret-store-pod -- cat /mnt/secrets-store/DemoSecret MyExampleSecret Enable log aggregation In order to collect logs from ARO cluster and store it in Azure ARC. configure azure monitor\nCreate Azure Log Analytics Workspace\naz monitor log-analytics workspace create --resource-group \u003c\u003csame as above\u003e\u003e --workspace-name loganalyticsworkspace Goto Azure ARC portal and click on logs\nClick on configure azure monitor button and select the workspace created in last step and click on configure.\nNow you can go see logs and metrics for your cluster.\nMonitor ARO cluster against Goverance Policies Azure Policy extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), to apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner. Azure Policy makes it possible to manage and report on the compliance state of your Kubernetes clusters from one place. The add-on enacts the following functions:\nChecks with Azure Policy service for policy assignments to the cluster. Deploys policy definitions into the cluster as constraint template and constraint custom resources. Reports auditing and compliance details back to Azure Policy service. Azure policy plugin is enabled when you connect your ARO cluster with Azure ARC. you can click on go to Azure Policies to look at the policies assigned to your cluster, check their status and attach more policies.\n","description":"","tags":["ARO","Azure"],"title":"Integrating Azure ARC with ARO","uri":"/docs/aro/azure-arc-integration/"},{"content":"This guide shows how to deploy the Cluster Log Forwarder operator and configure it to use the Vector logging agent to forward logs to CloudWatch.\nVector will replaced FluentD as the default logging agent used by the Openshift Logging Operator when version 5.6 is released in Q4 2022. Version 5.5.3 of the operator can enable Vector by configuring it in the ClusterLogging resource.\nVersion 5.5.3 of the operator does not support passing an STS role to Vector, but version 5.6 will. Until 5.6 is released, using Vector will require passing traditional IAM creds, but the conversion from IAM to STS will be relatively straightforward and will be documented here when it’s available.\nPrerequisites A ROSA cluster (configured with STS) The jq cli command The aws cli command Environment Setup Configure the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport ROSA_CLUSTER_NAME=\u003ccluster_name\u003e export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-vector\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy for OpenShift Log Forwarding\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM user for logging\naws iam create-user \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH/aws-user.json Fetch Access and Secret Keys for IAM User\naws iam create-access-key \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH/aws-access-key.json Attach Policy to AWS IAM User\naws iam attach-user-policy \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ --policy-arn ${POLICY_ARN} Create an OCP Secret to hold the AWS creds:\nAWS_ID=`cat $SCRATCH/aws-access-key.json | jq -r '.AccessKey.AccessKeyId'` AWS_KEY=`cat $SCRATCH/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` cat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: aws_access_key_id: $AWS_ID aws_secret_access_key: $AWS_KEY EOF Deploy Operators Deploy the Cluster Logging operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: cluster-logging.5.5.3 EOF Configure Cluster Logging Create a cluster log forwarding resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: logs: type: vector vector: {} forwarder: managementState: Managed EOF Check AWS CloudWatch for logs Use the AWS console or CLI to validate that there are log streams from the cluster\nNote: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet.\naws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] } Cleanup Delete the Cluster Log Forwarding resource\noc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource\noc delete -n openshift-logging clusterlogging instance Delete the IAM credential secret\noc -n openshift-logging delete secret cloudwatch-credentials Detach the IAM Policy to the IAM Role\naws iam detach-user-policy --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" \\ --policy-arn \"${POLICY_ARN}\" 1. Delete the IAM User access keys ```bash aws iam delete-access-key --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" \\ --access-key-id \"${AWS_ID}\" 1. Delete the IAM User ```bash aws iam delete-user --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" Delete the IAM Policy\nOnly run this command if there are no other resources using the Policy\naws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups\nIf there are any user workloads on the cluster they’ll have their own log groups that will also need to be deleted\naws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\" ","description":"","tags":["AWS","ROSA"],"title":"Configuring the Cluster Log Forwarder for CloudWatch Logs using Vector","uri":"/docs/rosa/clf-cloudwatch-vector/"},{"content":"There may be situations when you prefer not to use wild-card certificates. This ROSA guide talks about certificate management with cert-manager and letsencrypt, to dynamically issue certificates to routes created on a custom domain that’s hosted on AWS Route53.\nPrerequisites Set up environment Prepare AWS Account Set up cert-manager Create the Issuer and the Certficiate Configure Certificate Requestor Create the Certificate, which will later be used by the Custom Domain. Create the Custom Domain, which will be used to access your applications. Dynamic Certificates for Custom Domain Routes. Test an application. Debugging Prerequisites A Red Hat OpenShift on AWS (ROSA) cluster The oc CLI #logged in. The aws CLI #logged in. The rosa CLI #logged in. jq gettext A Public Route53 Hosted Zone, and the related Domain to use. Set up environment Export few environment variables\nexport CLUSTER_NAME=\"sts-pvtlnk-cluster\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" export LETSENCRYPT_EMAIL=youremail@work.com export HOSTED_ZONE_ID=ABCDEFGHEXAMPLE export HOSTED_ZONE_REGION=us-east-2 export DOMAIN=lab.domain.com #Custom Hosted Zone Domain for apps mkdir -p $SCRATCH_DIR Install jq \u0026 gettext\nInstalling or ensuring the gettext \u0026 jq package is installed, will allow us to use envsubst to simplify some of our configuration so we can use output of CLI commands as input into YAMLs to reduce the complexity of manual editing.\nbrew install gettext jq # or, for Linux / Windows WSL #sudo dnf install gettext jq Prepare AWS Account In order to make changes to the AWS Route53 Hosted Zone to add/remove DNS TXT challenge records by the cert-manager pod, we first need to create an IAM role with specific policy permissions \u0026 a trust relationship to allow access to the pod.\nMy Custom Domain Hosted Zone is in the same accout as the ROSA cluster. If these are in different accounts, few additional steps for Cross Account Access will be required.\nPrepare an IAM Policy file\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/cert-manager-r53-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } EOF Create the IAM Policy using the above created file.\nThis creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler.\nPOLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-cert-manager-r53-policy\" \\ --policy-document file://$SCRATCH_DIR/cert-manager-r53-policy.json \\ --query 'Policy.Arn' --output text) || \\ echo $POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:cert-manager:cert-manager\" ] } } } ] } EOF Create an IAM Role for the cert-manager Operator, with the above trust policy.\nROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-cert-manager-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the permissions policy to the Role\naws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-cert-manager-operator\" \\ --policy-arn $POLICY Set up cert-manager Create a project (namespace) in the ROSA cluster.\noc new-project cert-manager --display-name=\"Certificate Manager\" --description=\"Project contains Certificates and Custom Domain related components.\" Install the cert-manager Operator\ncat \u003c\u003cEOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: cert-manager- namespace: cert-manager --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cert-manager namespace: cert-manager spec: channel: stable installPlanApproval: Automatic name: cert-manager source: community-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete its set up.\nWait for cert-manager operator to finish installing.\nOur next steps depends on the successful installation of the operator. I recommend that you login to your cluster console to confirm the succeess status of cert-manager operator, in the Installed Operators section.\nAnnotate the ServiceAccount.\nThis is to enable the AWS SDK client code running within the cert-manager pod to interact with AWS STS service for temporary tokens, by assuming the IAM Role that was created in an earlier step. This is referred to as IRSA .\noc annotate serviceaccount cert-manager -n cert-manager eks.amazonaws.com/role-arn=$ROLE Normally, after ServiceAccount annotations, a restart of the pod is required. However, the next step will automatically cause a restart of the pod.\nUpdate the CA truststore of the cert-manager pod.\nThis step is usually not required. However, it was noticed that the cert-manager pod had difficulties in trusting the STS \u0026 LetsEncrypt endpoints. So the below commands essentially downloads the CA chain of these endpoints, puts them into a ConfigMap, which then gets attached to the pod as a Volume. Along with this step, I’ll also be setting the NameServers that the cert-manager will use for DNS01 self-check The Volume attachment to the pod and the setting of NameServers will be done together by patching the cert-manager CSV resource to persist these changes to the cert-manager deployment. This will cause the rollout of a new deployment and restart of the cert-manager pod.\nopenssl s_client -showcerts -verify 5 -connect sts.amazonaws.com:443 \u003c /dev/null 2\u003e /dev/null | awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; print}' \u003e $SCRATCH_DIR/extra-ca.pem openssl s_client -showcerts -verify 5 -connect acme-v02.api.letsencrypt.org:443 \u003c /dev/null 2\u003e /dev/null | awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; print}' \u003e\u003e $SCRATCH_DIR/extra-ca.pem oc create configmap extra-ca --from-file=$SCRATCH_DIR/extra-ca.pem -n cert-manager CERT_MANAGER_CSV_NAME=$(oc get csv | grep 'cert-manager' | awk '{print $1}') CLUSTER_DNS_SERVICE_IP=$(oc get svc -n openshift-dns | grep 'dns-default' | awk '{print $3}') echo $CERT_MANAGER_CSV_NAME echo $CLUSTER_DNS_SERVICE_IP oc patch csv $CERT_MANAGER_CSV_NAME --type='json' -p '[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/args/-\", \"value\":'--dns01-recursive-nameservers-only'}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/args/-\", \"value\":'--dns01-recursive-nameservers=$CLUSTER_DNS_SERVICE_IP:53'}]' oc patch csv $CERT_MANAGER_CSV_NAME --type='json' -p '[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/volumes\", \"value\": [{\"name\": \"extra-ca\"}]}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/volumes/0/configMap\", \"value\": {\"name\": \"extra-ca\", \"defaultMode\": 420}}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/volumeMounts\", \"value\": [{\"name\": \"extra-ca\", \"mountPath\": \"/etc/ssl/certs/extra-ca-bundle.pem\", \"readOnly\": true, \"subPath\": \"extra-ca-bundle.pem\"}]}]' During an Operator upgrade, the above changes might be lost. There seems to be improvement plans to facilitate these changes directly through the Operator config, but until then, it’d be a good idea to maintain some automation around this to persist these changes if it ever gets overridden to defaults.\nCreate the Issuer and the Certficiate Configure Certificate Requestor Create Cluster Issuer to use Let’s Encrypt\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencryptissuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPT_EMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: route53: hostedZoneID: $HOSTED_ZONE_ID region: $HOSTED_ZONE_REGION secretAccessKeySecretRef: name: '' EOF Describe the ClusterIssuer to confirm it’s ready.\noc describe clusterissuer letsencryptissuer You should see an output that mentions that the issuer is Registered/Ready. Note this can take a few minutes.\nConditions: Last Transition Time: 2022-11-17T10:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u003cnone\u003e Once the above command is complete, the status of the ClusterIssues in the cluster console will look similar to the below screenshot.\nCreate the Certificate, which will later be used by the Custom Domain. *I’ve used a SAN certificate here to show how SAN certificates could be created, which will be useful for clusters intended to run only a fixed set of applications. However, this is optional; a single subject/domain certificate works too *\nConfigure the Certificate\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: customdomain-cert namespace: cert-manager spec: secretName: custom-domain-certificate-tls issuerRef: name: letsencryptissuer kind: ClusterIssuer commonName: \"x.apps.$DOMAIN\" dnsNames: - \"x.apps.$DOMAIN\" - \"y.apps.$DOMAIN\" - \"z.apps.$DOMAIN\" EOF View the Certificate status\nIt’ll take up to 5 minutes for the Certificate to show as Ready status. If it takes too long, the oc describe command will mention issues if any.\noc get certificate customdomain-cert -n cert-manager oc describe certificate customdomain-cert -n cert-manager Create the Custom Domain, which will be used to access your applications. Create the Custom Domain\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: appdomain spec: domain: x.apps.$DOMAIN certificate: name: custom-domain-certificate-tls namespace: cert-manager scope: Internal EOF View the status of the Custom Domain\noc get customdomain appdomain -n cert-manager It will take 2-3 minutes for the custom domain to change from NotReady to Ready status. When ready, an endpoint also will be visible in the output of the above command, as shown below\nNext, we need to add a DNS record in my Custom Domain Route53 Hosted Zone to CNAME the the wildcard applications domain to the above obtained endpoint, as shown below. CUSTOM_DOMAIN_ENDPOINT=$(oc get customdomain appdomain -n cert-manager | grep 'appdomain' | awk '{print $2}') echo $CUSTOM_DOMAIN_ENDPOINT cat \u003c\u003cEOF \u003e $SCRATCH_DIR/add_cname_record.json { \"Comment\":\"Add apps CNAME to Custom Domain Endpoint\", \"Changes\":[{ \"Action\":\"CREATE\", \"ResourceRecordSet\":{ \"Name\": \"*.apps.$DOMAIN\", \"Type\":\"CNAME\", \"TTL\":30, \"ResourceRecords\":[{ \"Value\": \"$CUSTOM_DOMAIN_ENDPOINT\" }] } }] } EOF aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://$SCRATCH_DIR/add_cname_record.json The wild card CNAME’ing avoids the need to create a new record for every new application. The certificate that each of these applications use will NOT be a wildcard certificate\nAt this stage, you will be able to expose cluster applications on any of the listed DNS names that were specified in the previously created Certificate. But what if you have many more applications that will need to be securely exposed too. Well, one approach is to keep updating the Certificate resource with additional SAN names as more applications prepare to get onboarded, and this Certificate update which will trigger an update to the Custom Domain to honor the newly added SAN names. Another approach is to dynamically issue a Certificate to every new Route; Read on to find the details about this latter approach.\nDynamic Certificates for Custom Domain Routes. Create OpenShift resources required for issuing Dynamic Certificates to Routes. This step will create a new deployment (and hence a pod) that’ll watch out for specifically annotated routes in the cluster, and if the issuer-kind and issuer-name annotations are found in a new route, it will request the Issuer (ClusterIssuer in my case) for a new Certificate that’s unique to this route and which will honor the hostname that was specified while creating the route.\noc apply -f https://github.com/cert-manager/openshift-routes/releases/latest/download/cert-manager-openshift-routes.yaml -n cert-manager Additonal OpenShift resources such as a ClusterRole (with permissions to watch and update the routes across the cluster), a ServiceAccount (with these permissions, that will be used to run this newly created pod) and a ClusterRoleBinding to bind these two resources, will be created in this step too. If the cluster does not have access to github, you may as well save the raw contents locally, and run oc apply -f localfilename.yaml -n cert-manager\nView the status of the new pod. Check if all the pods are running successfully and that the events do not mention any errors.\noc get po -n cert-manager Test an application. Create a test applciation in a new namespace.\noc new-project testapp oc new-app --docker-image=docker.io/openshift/hello-openshift -n testapp Expose the test application Service.\nLet’s create a Route to expose the application from outside the cluster, and annotate the Route to give it a new Certificate.\noc create route edge --service=hello-openshift testroute --hostname hello.apps.$DOMAIN -n testapp oc annotate route testroute -n testapp cert-manager.io/issuer-kind=ClusterIssuer cert-manager.io/issuer-name=letsencryptissuer It will take a 2-3 minutes for the Certificate to be created. The renewal of the certitificate will automatically be managed by the cert-manager compoenents as it approaches expiry.\nAccess the application Route.\nDo a curl test (or any http client of your preference) to confirm there are no certificate related errors.\nOutput should print “Hello OpenShfit!”, and you should also notice a line that says “subjectAltName: host hello.apps.$DOMIAN” matched cert’s “hello.apps.$DOMIAN”\ncurl -vv https://hello.apps.$DOMAIN Debugging Please note that while creating certificates, the validation process usually take upto 2-3 minutes to complete.\nIf you face issues during certificate create step, run ‘oc describe’ against each of - ‘certificate,certificaterequest,order \u0026 challenge’ resources to view the events/reasons that’ll mostly help with identifying the cause of the issue.\noc get certificate,certificaterequest,order,challenge This is a very helpful guide in debugging certificates as well.\nYou may also use the cmctl CLI tool for various certificate management activities such as to check the status of certificates, testing renewals etc.\n","description":"","tags":["AWS","ROSA"],"title":"Dynamic Certificates for ROSA Custom Domain","uri":"/docs/rosa/dynamic-certificates/"},{"content":"","description":"","tags":null,"title":"ACS","uri":"/tags/acs/"},{"content":"This document is based in the RHACS workshop and in the RHACS official documentation .\nPrerequisites An ARO cluster or a ROSA cluster . Set up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat Unzip the downloaded file on your local machine\nPlace the extracted oc executable in your OS path or local directory\nLogin to ARO / ROSA Login to your ARO / ROSA clusters with user with cluster-admin privileges. Installing Red Hat Advanced Cluster Security in ARO/ROSA For install RHACS in ARO/ROSA you have two options:\nOption 1 - Manual Installation Option 2 - Automated Installation using Ansible Option 1 - Manual Installation For install RHACS using the Option 1 - Manual installation:\nFollow the steps within the RHACS Operator Installation Workshop to install the RHACS Operator.\nFollow the steps within the RHACS Central Cluster Installation Workshop to install the RHACS Central Cluster.\nFollow the steps within the RHACS Secured Cluster Configuration , to import the ARO/ROSA cluster into RHACS.\nOption 2 - Automated Installation using Ansible For install the RHACS in ROSA/ARO you can use the rhacs-demo repository that will install RH-ACS using Ansible playbooks:\nClone the rhacm-demo repo and install the galaxy collection: ansible-galaxy collection install kubernetes.core pip3 install kubernetes jmespath git clone https://github.com/rh-mobb/rhacs-demo cd rhacs-demo Deploy the RHACS with the ansible-playbook command: ansible-playbook rhacs-install.yaml This will install RHACS and also a couple of example Apps to demo. If you want just the plain RHACS installation, use the rhacs-only-install.yaml playbook.\nDeploying Example Apps for demo RHACS Deploy some example apps for demo RHACS policies and violations: oc new-project test oc run shell --labels=app=shellshock,team=test-team \\ --image=vulnerables/cve-2014-6271 -n test oc run samba --labels=app=rce \\ --image=vulnerables/cve-2017-7494 -n test ","description":"","tags":["ACS","ARO","ROSA"],"title":"Deploying Red Hat Advanced Cluster Security in ARO/ROSA","uri":"/docs/redhat/rhacs/"},{"content":"This guide demonstrates how to create and assign a static public IP address to an OpenShift service in Azure Red Hat OpenShift (ARO). By default, the public IP address assigned to an OpenShift service with a type of LoadBalancer created by an ARO cluster is only valid for the lifespan of that resource. If you delete the OpenShift service, the associated load balancer and IP address are also deleted. If you want to assign a specific IP address or retain an IP address for redeployed OpenShift services, you can create and use a static public IP address.\nThis guide will walk through the following steps:\nCreate a new static public IP address. Grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the parent resource group. Create the load balancer service and assign the static public IP address. Prerequisites An existing ARO cluster. If you need an ARO cluster, see the quickstart here . The Azure CLI. If you need to install the Azure CLI, see the Microsoft documentation here . Before you begin Before we begin, we need to set a few environment variables that will help us run the commands included in the guide.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster PUBLIC_IP_NAME=example-pip # Replace this with the name you want your static public IP to have Create a new static public IP address Create a static public IP address by using the az network public ip create command. The following command creates a static IP resource using the name you specified above in the parent resource group of the cluster object. To create the IP, run the following command:\naz network public-ip create \\ --resource-group ${RESOURCE_GROUP} \\ --name ${PUBLIC_IP_NAME} \\ --sku Standard \\ --allocation-method static The static public IP address provisioned is displayed as a part of the output of the command. It will look something like this:\n{ \"publicIp\": { ... \"ipAddress\": \"40.121.183.52\", ... } } Grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the parent resource group Next, we must grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the network resources of the parent resource group where we’ve created the static public IP. This must be done because the public IP lives outside of the cluster’s managed resource group (which starts with aro-). To grant the necessary access, run the following command:\nCLIENT_ID=$(az aro show --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --query \"servicePrincipalProfile.clientId\" --output tsv) SUB_ID=$(az account show --query \"id\" --output tsv) az role assignment create \\ --assignee ${CLIENT_ID} \\ --role \"Network Contributor\" \\ --scope /subscriptions/${SUB_ID}/resourceGroups/${RESOURCE_GROUP} Create the load balancer service and assign the static public IP address. Finally, we need to create a LoadBalancer service inside of OpenShift that specifies the static public IP address, as well as the parent resource group. Next, generate the necessary YAML for the LoadBalancer service with the loadBalancerIP property and resource group annotation set. To do so, run the following command, making sure to replace the variables specified:\nPUBLIC_IP=$(az network public-ip show --resource-group ${RESOURCE_GROUP} --name ${PUBLIC_IP_NAME} --query ipAddress --output tsv) cat \u003c\u003c EOF \u003e pip-service.yaml apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/azure-load-balancer-resource-group: ${RESOURCE_GROUP} name: static-ip-lb spec: loadBalancerIP: ${PUBLIC_IP} type: LoadBalancer ports: - port: 443 selector: app: static-ip-lb EOF Feel free to further modify this output (which is saved in your current directory as pip-service.yaml).\nFinally, apply the service configuration to the cluster by running the following command (note this will deploy the service directly into the current namespace):\noc apply -f ./pip-service.yaml The cluster should provision the load balancer within a minute or two. You can verify this by running the following command:\noc describe service static-ip-lb The output will look similar to this:\nName: static-ip-lb Namespace: example Labels: \u003cnone\u003e Annotations: service.beta.kubernetes.io/azure-load-balancer-resource-group: example-rg Selector: app=static-ip-lb Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 172.30.74.108 IPs: 172.30.74.108 IP: 20.168.220.211 LoadBalancer Ingress: 20.168.220.211 Port: port-8080 443/TCP TargetPort: 8080/TCP NodePort: port-8080 31616/TCP Endpoints: 10.129.2.10:8080 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 31m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 31m service-controller Ensured load balancer You can now access your load balancer using the IP address provided!\n","description":"","tags":["AWS","ROSA"],"title":"Configure a load balancer service to use a static public IP","uri":"/docs/aro/static-ip-load-balancer/"},{"content":"","description":"","tags":null,"title":"STS","uri":"/tags/sts/"},{"content":"To proceed with the deployment of a ROSA cluster, an account must support the required roles and permissions. AWS Service Control Policies (SCPs) cannot block the API calls made by the installer or operator roles.\nDetails about the IAM resources required for an STS-enabled installation of ROSA can be found here: https://docs.openshift.com/rosa/rosa_architecture/rosa-sts-about-iam-resources.html This guide is validated for ROSA v4.11.X.\nPrerequisites AWS CLI ROSA CLI v1.2.6 jq CLI AWS role with required permissions Verify ROSA Permissions To verify the permissions required for ROSA we can run the script below without ever creating any AWS resources.\nThe script uses the rosa, aws, and jq CLI commands to create files in the working directory that will be used to verify permissions in the account connected to the current AWS configuration.\nThe AWS Policy Simulator is used to verify the permissions of each role policy against the API calls extracted by jq; results are then stored in a text file appended with .results.\nThis script will verify the permissions for the current account and region.\n#!/bin/bash while getopts 'p:' OPTION; do case \"$OPTION\" in p) PREFIX=\"$OPTARG\" ;; ?) echo \"script usage: $(basename \\$0) [-p PREFIX]\" \u003e\u00262 exit 1 ;; esac done shift \"$(($OPTIND -1))\" rosa create account-roles --mode manual --prefix $PREFIX INSTALLER_POLICY=$(cat sts_installer_permission_policy.json | jq ) CONTROL_PLANE_POLICY=$(cat sts_instance_controlplane_permission_policy.json | jq) WORKER_POLICY=$(cat sts_instance_worker_permission_policy.json | jq) SUPPORT_POLICY=$(cat sts_support_permission_policy.json | jq) simulatePolicy () { outputFile=\"${2}.results\" echo $2 aws iam simulate-custom-policy --policy-input-list \"$1\" --action-names $(jq '.Statement | map(select(.Effect == \"Allow\"))[].Action | if type == \"string\" then . else .[] end' \"$2\" -r) --output text \u003e $outputFile } simulatePolicy \"$INSTALLER_POLICY\" \"sts_installer_permission_policy.json\" simulatePolicy \"$CONTROL_PLANE_POLICY\" \"sts_instance_controlplane_permission_policy.json\" simulatePolicy \"$WORKER_POLICY\" \"sts_instance_worker_permission_policy.json\" simulatePolicy \"$SUPPORT_POLICY\" \"sts_support_permission_policy.json\" Usage Instructions To use the script, run the following commands in a bash terminal (the -p option defines a prefix for the roles):\nmkdir scratch cd scratch curl https://raw.githubusercontent.com/rh-mobb/documentation/main/content/docs/rosa/verify-permissions/verify-permissions.sh --output verify-permissions.sh chmod +x verify-permissions.sh ./verify-permissions.sh -p SimPolTest After the script completes, review each results file to ensure that none of the required API calls are blocked:\n$ cat sts_support_permission_policy.json.results EVALUATIONRESULTS\tcloudtrail:DescribeTrails\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 EVALUATIONRESULTS\tcloudtrail:LookupEvents\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 EVALUATIONRESULTS\tcloudwatch:GetMetricData\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 ... If any actions are blocked, review the error provided by AWS and consult with your Administrator to determine if SCPs are blocking the required API calls.\n","description":"","tags":["AWS","ROSA","STS"],"title":"Verify Permissions for ROSA STS Deployment","uri":"/docs/rosa/verify-permissions/"},{"content":"Azure Red Hat Openshift clusters have built in metrics and logs that can be viewed by both Administrators and Developers via the OpenShift Console. But there are many reasons you might want to store and view these metrics and logs from outside of the cluster.\nThe OpenShift developers have anticipated this needs and have provided ways to ship both metrics and logs outside of the cluster. In Azure we have the Azure Blob storage service which is perfect for storing the data.\nIn this guide we’ll be setting up Thanos and Grafana Agent to forward cluster and user workload metrics to Azure Blob as well the Cluster Logging Operator to forward logs to Loki which stores the logs in Azure Blob.\nPrerequisites Azure CLI Terraform OC CLI Helm Git Preparation Note: This guide was written on Fedora Linux (using the zsh shell) running inside Windows 11 WSL2. You may need to modify these instructions slightly to suit your Operating System / Shell of choice.\nCreate some environment variables to be reused through this guide\nModify these values to suit your environment, especially the storage account name which must be globally unique.\nexport CLUSTER=\"aro-${USERNAME}\" export WORKDIR=\"/tmp/${CLUSTER}\" export NAMESPACE=mobb-aro-obs export AZR_STORAGE_ACCOUNT_NAME=\"aro${USERNAME}obs\" mkdir -p ${WORKDIR} cd \"${WORKDIR}\" Log into Azure CLI\naz login Create ARO Cluster You can skip this step if you already have a cluster, or if you want to create it another way.\nThis will create a default ARO cluster named aro-${USERNAME}, you can modify the TF variables/Makefile to change settings, just update the environment variables loaded above to suit.\nclone down the Black Belt ARO Terraform repo\ngit clone https://github.com/rh-mobb/terraform-aro.git cd terraform-aro Initialize, Create a plan, and apply\nmake create This should take about 35 minutes and the final lines of the output should look like\nazureopenshift_redhatopenshift_cluster.cluster: Still creating... [35m30s elapsed] azureopenshift_redhatopenshift_cluster.cluster: Still creating... [35m40s elapsed] azureopenshift_redhatopenshift_cluster.cluster: Creation complete after 35m48s [id=/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/my-tf-cluster-rg/providers/Microsoft.RedHatOpenShift/openShiftClusters/my-tf-cluster] Save, display the ARO credentials, and login\naz aro list --query \\ \"[?name=='${CLUSTER}'].{Name:name,Console:consoleProfile.url,API:apiserverProfile.url, ResourceGroup:resourceGroup,Location:location}\" \\ -o tsv | read -r NAME CONSOLE API RESOURCEGROUP LOCATION az aro list-credentials -n $NAME -g $RESOURCEGROUP \\ -o tsv | read -r OCP_PASS OCP_USER oc login ${API} --username ${OCP_USER} --password ${OCP_PASS} echo \"$ oc login ${API} --username ${OCP_USER} --password ${OCP_PASS}\" echo \"Login to ${CONSOLE} as ${OCP_USER} with password ${OCP_PASS}\" Now would be a good time to use the output of this command to log into the OCP Console, you can always run echo \"Login to ${CONSOLE} as ${OCP_USER} with password ${OCP_PASS}\" at any time to remind yourself of the URL and credentials.\nConfigure additional Azure resources These steps create the Storage Account (and two storage containers) in the same Resource Group as the ARO cluster to make cleanup easier. You may want to change it, especially if you plan to host metrics and logs for multiple clusters in the one Storage Account.\nCreate Azure Storage Account and Storage Containers\n# Create Storage Account az storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $RESOURCEGROUP \\ --location $LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 # Fetch the Azure storage key AZR_STORAGE_KEY=$(az storage account keys list -g \"${RESOURCEGROUP}\" \\ -n \"${AZR_STORAGE_ACCOUNT_NAME}\" --query \"[0].value\" -o tsv) # Create Azure Storage Containers az storage container create --name \"${CLUSTER}-metrics\" \\ --account-name \"${AZR_STORAGE_ACCOUNT_NAME}\" \\ --account-key \"${AZR_STORAGE_KEY}\" az storage container create --name \"${CLUSTER}-logs\" \\ --account-name \"${AZR_STORAGE_ACCOUNT_NAME}\" \\ --account-key \"${AZR_STORAGE_KEY}\" Configure MOBB Helm Repository Helm charts do a lot of the heavy lifting for us, and reduce the need for us to copy/paste a pile of YAML. The Managed OpenShift Black Belt team maintain these charts here .\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories\nhelm repo update Create a namespace to use\noc new-project \"${NAMESPACE}\" Update the Pull Secret and enable OperatorHub This is required to provide credentials to the cluster to pull various Red Hat images in order to enable and configure the Operator Hub.\nDownload a Pull secret from Red Hat Cloud Console and save it in ${SCRATCHDIR}/pullsecret.txt\nUpdate the cluster’s pull secret using the mobb/aro-pull-secret Helm Chart\n# Annotate resources for Helm oc -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret oc -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config oc -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm # Update pull secret (change path needed) cat \u003c\u003c EOF \u003e \"${WORKDIR}/pullsecret.yaml\" pullSecret: | $(\u003c \"${WORKDIR}/pull-secret.txt\") EOF helm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --values \"${WORKDIR}/pullsecret.yaml\" # Enable Operator Hub oc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Wait for OperatorHub pods to be ready\nwatch oc -n openshift-marketplace get pods NAME READY STATUS RESTARTS AGE certified-operators-xm674 1/1 Running 0 117s community-operators-c5pcq 1/1 Running 0 117s marketplace-operator-7696c9454c-wgtzp 1/1 Running 1 (30m ago) 47m redhat-marketplace-sgnsg 1/1 Running 0 117s redhat-operators-pdbg8 1/1 Running 0 117s Configure Metrics Federation to Azure Blob Storage Next we can configure Metrics Federation to Azure Blob Storage. This is done by deploying the Grafana Operator (to install Grafana to view the metrics later) and the Resource Locker Operator (to configure the User Workload Metrics) and then the mobb/aro-thanos-af Helm Chart to Deploy and Configure Thanos and Grafana Agent to store and retrieve the metrics in Azure Blob.\nGrafana Operator Deploy the Grafana Operator\n# Create a file containing the Grafana operator mkdir -p $WORKDIR/metrics cat \u003c\u003cEOF \u003e $WORKDIR/metrics/grafana-operator.yaml subscriptions: - name: grafana-operator channel: v4 installPlanApproval: Automatic source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v4.7.0 operatorGroups: - name: ${NAMESPACE} targetNamespace: ~ EOF # Deploy the Grafana Operator using Helm helm upgrade -n \"${NAMESPACE}\" clf-operators \\ mobb/operatorhub --install \\ --values \"${WORKDIR}/metrics/grafana-operator.yaml\" # Wait for the Grafana Operator to be installed while ! oc get grafana; do sleep 5; echo -n .; done After a few minutes you should see the following\nerror: the server doesn't have a resource type \"grafana\" error: the server doesn't have a resource type \"grafana\" No resources found in mobb-aro-obs namespace. Resource Locker Operator Deploy the Resource Locker Operator\n# Create the namespace `resource-locker-operator` oc create namespace resource-locker-operator # Create a file containing the Grafana operator cat \u003c\u003cEOF \u003e $WORKDIR/resource-locker-operator.yaml subscriptions: - name: resource-locker-operator channel: alpha installPlanApproval: Automatic source: community-operators sourceNamespace: openshift-marketplace namespace: resource-locker-operator operatorGroups: - name: resource-locker namespace: resource-locker-operator targetNamespace: all EOF # Deploy the Resource Locker Operator using Helm helm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --install \\ --values \"${WORKDIR}\"/resource-locker-operator.yaml # Wait for the Operators to be installed while ! oc get resourcelocker; do sleep 5; echo -n .; done After a few minutes you should see the following\nerror: the server doesn't have a resource type \"resourcelocker\" error: the server doesn't have a resource type \"resourcelocker\" No resources found in mobb-aro-obs namespace. Configure Metrics Federation Deploy mobb/aro-thanos-af Helm Chart to configure metrics federation\nhelm upgrade -n \"${NAMESPACE}\" aro-thanos-af \\ --install mobb/aro-thanos-af --version 0.4.1 \\ --set \"aro.storageAccount=${AZR_STORAGE_ACCOUNT_NAME}\" \\ --set \"aro.storageAccountKey=${AZR_STORAGE_KEY}\" \\ --set \"aro.storageContainer=${CLUSTER}-metrics\" \\ --set \"enableUserWorkloadMetrics=true\" ## Configure Logs Federation to Azure Blob Storage Next we need to deploy the Cluster Logging and Loki Operators so that we can use the `mobb/aro-clf-blob` Helm Chart to deploy and configure Cluster Log Forwarding and the Loki Stack to store metrics in Azure Blob. ### Deploy Operators 1. Deploy the cluster logging and loki operators ```bash # Create nanespaces oc create ns openshift-logging oc create ns openshift-operators-redhat # Configure and deploy operators mkdir -p \"${WORKDIR}/logs\" cat \u003c\u003c EOF \u003e \"${WORKDIR}/logs/log-operators.yaml\" subscriptions: - name: cluster-logging channel: stable installPlanApproval: Automatic source: redhat-operators sourceNamespace: openshift-marketplace namespace: openshift-logging startingCSV: cluster-logging.5.5.2 - name: loki-operator channel: stable installPlanApproval: Automatic source: redhat-operators sourceNamespace: openshift-marketplace namespace: openshift-operators-redhat startingCSV: loki-operator.5.5.2 operatorGroups: - name: openshift-logging namespace: openshift-logging targetNamespace: openshift-logging - name: openshift-operators-redhat namespace: openshift-operators-redhat targetNamespace: all EOF # Deploy the OpenShift Loki Operator and the Red Hat OpenShift Logging Operator helm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --install \\ --values \"${WORKDIR}/logs/log-operators.yaml\" # Wait for the Operators to be installed while ! oc get clusterlogging; do sleep 5; echo -n .; done while ! oc get lokistack; do sleep 5; echo -n .; done Deploy and Configure Cluster Logging and Loki Configure the loki stack to log to Azure Blob\nNote: Only Infrastructure and Application logs are configured to forward by default to reduce storage and traffic. You can add the argument --set clf.audit=true to also forward debug logs.\nhelm upgrade -n \"${NAMESPACE}\" aro-clf-blob \\ --install mobb/aro-clf-blob --version 0.1.1 \\ --set \"azure.storageAccount=${AZR_STORAGE_ACCOUNT_NAME}\" \\ --set \"azure.storageAccountKey=${AZR_STORAGE_KEY}\" \\ --set \"azure.storageContainer=${CLUSTER}-logs\" Wait for the logging stack to come online\nwatch oc -n openshift-logging get pods NAME READY STATUS RESTARTS AGE cluster-logging-operator-8469d5479f-kzh4j 1/1 Running 0 2m10s collector-gbqpr 2/2 Running 0 61s collector-j7f4j 2/2 Running 0 40s collector-ldj2k 2/2 Running 0 58s collector-pc82l 2/2 Running 0 56s collector-qrzlb 2/2 Running 0 58s collector-vsj7z 2/2 Running 0 56s logging-loki-compactor-0 1/1 Running 0 89s logging-loki-distributor-565c84c54f-4f24j 1/1 Running 0 89s logging-loki-gateway-69d68bc47f-rfp8t 2/2 Running 0 88s logging-loki-index-gateway-0 1/1 Running 0 88s logging-loki-ingester-0 0/1 Running 0 89s logging-loki-querier-96c699b7d-fjdrr 1/1 Running 0 89s logging-loki-query-frontend-796d85bf5b-cb4dh 1/1 Running 0 88s logging-view-plugin-98cf668b-dbdkd 1/1 Running 0 107s Sometimes the log collector needs to be restarted for logs to flow correctly into Loki. Wait a few minutes then run the following\noc -n openshift-logging rollout restart daemonset collector Validate Metrics and Logs Now that the Metrics and Log forwarding is set up we can view them in Grafana.\nFetch the Route for Grafana\noc -n \"${NAMESPACE}\" get route grafana-route Browse to the provided route address and login using your OpenShift credentials (username kubeadmin, password echo $OCP_PASS).\nView an existing dashboard such as mobb-aro-obs -\u003e Node Exporter -\u003e USE Method -\u003e Cluster.\nClick the Explore (compass) Icon in the left hand menu, select “Loki (Application)” in the dropdown and search for {kubernetes_namespace_name=\"mobb-aro-obs\"}\nDebugging loki If you don’t see logs in Grafana you can validate that Loki is correctly storing them by querying it directly like so.\nPort forward to the Loki Service\noc port-forward -n openshift-logging svc/logging-loki-gateway-http 8080:8080 Make sure you can curl the Loki service and get a list of labels\nYou can get the bearer token from the login command screen in the OCP Dashboard\ncurl -k -H \"Authorization: Bearer \u003cBEARER TOKEN\u003e\" \\ 'https://localhost:8080/api/logs/v1/infrastructure/loki/api/v1/labels' You can also use the Loki CLI logcli --bearer-token=\"\u003cBEARER TOKEN\u003e\" --tls-skip-verify --addr https://localhost: 8080/api/logs/v1/infrastructure/ labels Cleanup Assuming you didn’t deviate from the guide then you created everything in the Resource Group of the ARO cluster and you can simply destroy our Terraform stack and everything will be cleaned up.\nDelete the ARO cluster\ncd \"${WORKDIR}/terraform-aro\" make delete ","description":"","tags":["ARO","Azure"],"title":"Shipping logs and metrics to Azure Blob storage","uri":"/docs/aro/shipping-logs-and-metrics-to-azure-blob/"},{"content":"If you prefer a more visual medium, you can watch this video on YouTube .\nThis short video talks about how the STS OIDC flow work in ROSA (Red Hat OpenShift Service on AWS).\n","description":"","tags":["AWS","ROSA","STS"],"title":"STS OIDC in ROSA : How it works!","uri":"/docs/rosa/sts-oidc-flow/"},{"content":"The Security Reference Architecture for ROSA is a set of guidelines for deploying Red Hat OpenShift on AWS (ROSA) clusters to support high-security production workloads that align with Red Hat and AWS best practices.\nThis overall architectural guidance compliments detailed, specific recommendations for AWS services and Red Hat OpenShift Container Platform.\nThe Security Reference Architecture (SRA) for ROSA is a living document and is updated periodically based on new feature releases, customer feedback and evolving security best practices.\nThis document is divided into the following sections:\nROSA Day 1 Configuration ROSA Day 2 Security and Operations ROSA Day 1 Configuration ROSA Day 1 configurations are applied to the cluster at the time it is created; they cannot be modified after the cluster has been deployed.\nAWS PrivateLink Networking ROSA provides 3 network deployment patterns: public, private and PrivateLink. Choosing the PrivateLink option provides the most secure configuration and is recommended for customers with sensitive workloads or strict compliance requirements. The PrivateLink option uses AWS PrivateLink to allow Red Hat Site Reliability Engineering (SRE) teams to manage the cluster using a private subnet connected to the cluster’s PrivateLink endpoint in an existing VPC.\nWhen using the PrivateLink model, a VPC with Private Subnets must exist in the AWS account where ROSA will be deployed. The subnets are provided to the installer via CLI flags.\nDetails on the PrivateLink Architecture can be found in the Red Hat and AWS documentation:\nROSA PrivateLink Architecture ROSA PrivateLink Prerequisites Firewall Egress Requirements Deploy a VPC and PrivateLink Cluster AWS Security Token Service (STS) Mode There are two supported methods for providing AWS permissions to ROSA:\nUsing static IAM user credentials with AdministratorAccess policy - “ROSA with IAM Users” (not recommended) Using AWS Security Token Service (STS) with short-lived, dynamic tokens (preferred) - “ROSA with STS” The STS method uses least-privilege predefined roles and policies to grant ROSA minimal permissions in the AWS account for the service to operate and is the recommended option.\nAs stated in the AWS documentation AWS STS “enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users)”. In this case, AWS STS can be used to grant the ROSA service, limited, short-term access, to resources in your AWS account. After these credentials expire (typically an hour after being requested), they are no longer recognized by AWS and they no longer have any kind of account access from API requests made with them.\nDetails on ROSA with STS can be found in Red Hat documentation and blogs:\nROSA with STS Explained AWS prerequisites for ROSA with STS IAM Resources for Clusters that Use STS Customer-Supplied KMS Key By default, ROSA encrypts all Elastic Block Store (EBS) volumes used for node storage and Persistent Volumes (PVs) with an AWS-managed Key Management Service (KMS) key.\nUsing a Customer Managed KMS key allows you to have full control over the KMS key including key policies, key rotation and deletion.\nTo configure a cluster with a custom KMS Key, consider the following references:\nROSA STS Customizations Deploy ROSA with a Custom KMS Key Multi-Availability Zone ROSA clusters that will be used for production workloads should be deployed across multiple availability zones. In this configuration, control plane nodes are distributed across availability zones and at least one worker node is required in each availability zone.\nThis provides the highest level of fault tolerance and protects against the loss of a single availability zone in an AWS region.\nDeploy the Day 1 ROSA SRA via ROSA CLI The Day 1 ROSA SRA can be deployed quickly using the AWS CLI and the ROSA CLI. To deploy the cluster, the following prerequisites must be met:\nAWS Account: Access to an AWS account with sufficient permissions to deploy a ROSA cluster. If using AWS Organizations and Service Control Policies (SCPs), the SCPs must not be more restrictive than the minimum permissions required to operate the service. Sufficient quota to support the cluster deployment. Networking: An AWS VPC, with 3 private subnets across 3 availability zones and outbound internet access. Make note of the AWS VPC Subnet IDs as they will be needed for the installer. Tooling: AWS CLI ROSA CLI v1.2.6 Prepare the ROSA Workload Account Log in to the AWS account with a user that has been assigned AdministratorAccess and run the following command using the aws CLI:\nexport AWS_REGION=\"ca-central-1\" aws iam create-service-linked-role --aws-service-name \"elasticloadbalancing.amazonaws.com\" Create the required ROSA Account Roles Creation of the account roles is a one-time activity, create them with the following command:\nrosa create account-roles --mode auto -y Create the Required KMS Key and Initial Policy The custom KMS key is used to encrypt EC2 EBS node volumes and the EBS volumes that are created by the default StorageClass on OpenShift.\nCreate a new Symmetric KMS Key for EBS Encryption:\nKMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'rosa-ebs-key' --query KeyMetadata.Arn --output text) Generate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own.\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Deploy a multi-AZ, single subnet, PrivateLink, STS ROSA cluster To deploy the cluster, you must gather the following info:\n--subnet-ids: AWS subnet IDs that the cluster will be deployed in --machine-cidr: The VPC CIDR Deploy the cluster with the following command:\nROSA_CLUSTER_NAME=rosa-ct1 rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts --private-link \\ --region ca-central-1 --version 4.11.4 \\ --machine-cidr 10.0.0.0/20 \\ --subnet-ids subnet-058aa558a63da3d51,subnet-058aa558a63da3d52,subnet-058aa558a63da3d53 \\ --enable-customer-managed-key --kms-key-arn $KMS_ARN -y --mode auto To complete the KMS key policy, you must retrieve the Cluster CSI and Machine API operator role names:\nrosa describe cluster -c $ROSA_CLUSTER_NAME The operator role names will be similar to:\narn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credenti arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials Replace the role names in the following script with your EXACT Operator Role names:\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly updated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default After creating the operator roles, create the required OIDC provider:\nrosa create oidc-provider --mode auto --cluster $ROSA_CLUSTER_NAME Wait for the cluster deployment to finish.\nROSA Day 2 Security and Operations This section of the SRA describes tasks that are completed once the cluster has been deployed. These configurations enhance the security of the cluster and are often requirements for customers operating in regulated environments.\nConfigure an Identity Provider ROSA provides an easy way to access clusters immediately after deployment through the creation of a cluster-admin user through the ROSA CLI. This method creates an HTPASSWORD identity provider on the cluster. This is good if you need quick access to the cluster, but should not be used for clusters that will host any workloads.\nThe recommended approach is to use a formal identity provider (IDP) to access the cluster (and then grant that user admin privileges, if desired).\nROSA supports several commercially available IDPs and common protocols. The full listing can be found in the ROSA documentation:\nhttps://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-sts-config-identity-providers.html#understanding-idp-supported_rosa-sts-config-identity-providers Some examples of how to configure an IDP can be found on the mobb.ninja website:\nConfigure Azure AD as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ROSA with group claims Configure CloudWatch Log Forwarding ROSA does not provide persistent logging by default, but it can be enabled through the cluster-logging operator from the OpenShift Marketplace. This add-on service offers an optional application log-forwarding solution based on AWS CloudWatch. This logging solution can be installed after the ROSA cluster is provisioned.\nTo capture all logging events in AWS CloudWatch, all three log types should be enabled:\nApplications logs: Permits the Operator to collect application logs, which includes everything that is not deployed in the openshift-, kube-, and default namespaces. Infrastructure logs: Permits the Operator to collect logs from OpenShift Container Platform, Kubernetes, and some nodes. Audit logs: Permits the Operator to collect node logs related to security audits. By default, Red Hat stores audit logs outside the cluster through a separate mechanism that does not rely on the Cluster Logging Operator. For more information about default audit logging, see the ROSA Service Definition. After the operator has been enabled the logs can be viewed in the AWS Console, and persistently stored based on the CloudWatch configuration of the AWS Account.\nThe cluster-logging operator has the following limits when configured for CloudWatch log forwarding:\nMessage Size (bytes) Maximum logging rate (messages/second/node) 512 1,000 1,024 650 2,048 450 Details on this configuration can be found at the following links:\nConfiguring the Cluster Log Forwarder for CloudWatch Logs and STS Viewing cluster logs in the AWS Console Configure Custom Ingress TLS Profile By default, ROSA supports multiple versions of TLS on the Ingress COntrollers used for applications to support the broadest set of clients and libraries. To support specific versions of TLS, the tlsSecurityProfile value on cluster ingress controllers can be modified.\nReview the OpenShift Documentation that explains the options for the tlsSecurityProfile to determine which profile meets your organization’s needs. By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile:\nOpenShift documentation on tlsSecurityProfile Intermediate Mozilla Profile The tlsSecurityProfile can be modified by following these instructions:\nConfigure ROSA/OSD to use custom TLS ciphers on the ingress controllers Compliance Operator The Compliance Operator lets ROSA administrators describe the required compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of ROSA, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content.\nThere are several profiles available as part of the Compliance Operator installation. These profiles represent different compliance benchmarks. Each profile has the product name that it applies to added as a prefix to the profile’s name. ocp4-e8 applies the Essential 8 benchmark to the OpenShift Container Platform product, while rhcos4-e8 applies the Essential 8 benchmark to the Red Hat Enterprise Linux CoreOS (RHCOS) product.\nImportant note: The compliance benchmarks are continuously updated and maintained by Red Hat based on each control profile. ROSA-specific benchmarks are under development to account for the managed service components.\nTo understand and install the compliance operator, read the Red Hat documentation:\nInstalling the Compliance Operator Understanding the Compliance Operator Supported compliance profiles OpenShift Service Mesh Red Hat OpenShift Service Mesh addresses a variety of problems in a microservice architecture by creating a centralized point of control in an application. It adds a transparent layer on existing distributed applications without requiring any changes to the application code.\nRed Hat OpenShift Service Mesh provides a number of key capabilities uniformly across a network of services:\nTraffic Management - Control the flow of traffic and API calls between services, make calls more reliable, and make the network more robust in the face of adverse conditions. Service Identity and Security - Provide services in the mesh with a verifiable identity and provide the ability to protect service traffic as it flows over networks of varying degrees of trustworthiness. Policy Enforcement - Apply an organizational policy to the interaction between services, ensure access policies are enforced and resources are fairly distributed among consumers. Policy changes are made by configuring the mesh, not by changing application code. Telemetry - Gain an understanding of the dependencies between services and the nature and flow of traffic between them, providing the ability to quickly identify issues. To learn more about OpenSHift Service Mesh and to install the Service Mesh, read the OpenShift documentation:\nUnderstanding Service Mesh Installing the Service Mesh Operator Adding workloads to the Service Mesh Service Mesh Security Backup and Restore / Disaster Recovery An important part of any platform used to host business and user workloads is data protection. Data protection may include operations including on-demand backup, scheduled backup and restore. These operations allow the objects within a cluster to be backed up to a storage provider, either locally or on a public cloud and restore that cluster from the backup in the event of a failure or scheduled maintenance.\nAs part of the Shared Responsibility Model for ROSA, consumers of the service are responsible for backing up cluster and application data when the STS option is used. To implement a backup and disaster recovery solution, administrators can use OpenShift APIs for Data Protection (OADP). OADP is an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP provides the following APIs:\nBackup Restore Schedule BackupStorageLocation VolumeSnapshotLocation You can learn how to install and use OADP from the following resources:\nOADP features and plug-ins Deploying OpenShift Advanced Data Protection on a ROSA cluster Configure AWS WAF and CloudFront for Application Ingress ROSA does not provide advanced firewall or DDoS protection by default, however, this can easily be achieved by combining three AWS services to protect the cluster and applications:\nAWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting. Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost effective way to distribute content with low latency and high data transfer speeds. AWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS. To learn more about these services and how to configure them for ROSA, read the documentation below:\nAWS WAF FAQ Amazon CloudFront FAQ AWS Shield FAQ Using CloudFront + WAF on ROSA Using ALB + WAF on ROSA Use and Store Secrets Securely in AWS Kubernetes Secrets are insecure by default, this is described in the Kubernetes documentation:\nKubernetes Secrets are, by default, stored unencrypted in the API server’s underlying data store (etcd). This design is not unique to ROSA and affects all Kubernetes distributions. Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.\nCustomers looking for secure ways to manage application secrets often chose to use a third-party tool to manage secrets due to this behavior.\nThe AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in AWS Secrets Manager and then retrieve them through your workloads running on ROSA.\nThis is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity.\nTo use the AWS Secrets Manager CSI with ROSA and STS, follow this guide:\nUsing AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS Provide External Persistent Storage to Applications on ROSA ROSA supports both Amazon Elastic Block Storage (EBS) and Elastic File Storage (EFS) for persistent application data.\nWhen applications require ReadWriteMany capabilities, or when multiple applications must read the same data, EFS should be used.\nWith the release of OpenShift 4.10 the EFS CSI Driver is now GA and available.\nTo learn more, or to install the EFS CSI driver, review the following documentation:\nPersistent Storage using EFS Persistent Storage using EBS Enabling the AWS EFS CSI Driver Operator on ROSA ","description":"","tags":["AWS","ROSA"],"title":"Security Reference Architecture for ROSA","uri":"/docs/rosa/security-ra/"},{"content":"This guide demonstrates how to configure Azure AD as the cluster identity provider in Azure Red Hat OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Azure Red Hat OpenShift (ARO) to authenticate using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual users. Before you Begin If you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because \u003ccode\u003ezsh\u003c/code\u003e disables comments in interactive shells from being used .\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username” when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation .\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider.\nTo do so, ensure you are logged in to the OpenShift command line interface (oc) by running the following command, making sure to replace the variables specified:\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:\nCLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster’s OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:\nIDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat \u003c\u003c EOF \u003e cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email name: - name preferredUsername: - preferred_username clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: - profile - openid - email issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml).\nFinally, apply the new configuration to the cluster’s OAuth provider by running the following command:\noc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored.\nOnce the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD.\nIf you have a private cluster behind a firewall, you may get an error message like the image below when you try login into the web console using the AAD option. In this case you should open a firewall rule allowing access from the cluster to graph.microsoft.com.\nIf you are using Azure Firewall, you can run those commands to allow this access:\naz network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Microsoft_Graph' --action allow --priority 100 \\ -n 'Microsoft_Graph' --source-address '*' --protocols 'any' \\ --source-addresses '*' --destination-fqdns 'graph.microsoft.com' \\ --destination-ports '*' Now you should be able to login choosing the AAD option:\nThen inform the user you would like to use:\n4. Grant additional permissions to individual users Once you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant your user access to the cluster-admin role, you must create a ClusterRoleBinding to your user account.\nUSERNAME=example@redhat.com # Replace with your Azure AD username oc create clusterrolebinding cluster-admin-user \\ --clusterrole=cluster-admin \\ --user=$USERNAME For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .\n","description":"","tags":["Azure","ARO"],"title":"Configure ARO to use Azure AD","uri":"/docs/idp/azuread-aro/"},{"content":"This guide demonstrates how to configure Azure AD as the cluster identity provider in Red Hat OpenShift Service on AWS (ROSA). This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation .\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because \u003ccode\u003ezsh\u003c/code\u003e disables comments in interactive shells from being used .\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\" Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username” when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation .\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we’ll configure the cluster’s OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified:\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims preferred_username \\ --extra-scopes email,profile 4. Grant additional permissions to individual users Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD.\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant your user access to the cluster-admin role, run the following command:\nUSERNAME=example@redhat.com # Replace with your Azure AD username rosa grant user cluster-admin \\ --user=${USERNAME} \\ --cluster=${CLUSTER_NAME} For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .\n","description":"","tags":["Azure","ROSA","OSD"],"title":"Configure Azure AD as an OIDC identity provider for ROSA/OSD","uri":"/docs/idp/azuread/"},{"content":"","description":"","tags":null,"title":"Azure Service Operator","uri":"/docs/aro/azure-service-operator/"},{"content":"The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.\nThis example uses ASO V1, which has now been replaced by ASO V2. ASO V2 does not (as of 5/19/2022) yet have an entry in the OCP OperatorHub, but is functional and should be preferred for use, especially if V1 isn’t already installed on a cluster. MOBB has documented the [install of ASO V2 on ROSA]. MOBB has not tested running the two in parallel.\nPrerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster Prepare your Azure Account and ARO Cluster Set the following environment variables:\nNote: modify the cluster name, region and resource group to match your cluster\nAZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"openshift\" AZURE_RESOURCE_GROUP=\"openshift\" AZURE_REGION=\"eastus\" Create a Service Principal with Contributor permissions to your subscription:\nNote: You may want to lock this down to a specific resource group.\nread -r ASO_USER ASO_PASS \u003c \u003c(az ad sp create-for-rbac -n \"$CLUSTER_NAME-ASO\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID -o tsv \\ --query \"[name,password]\" | xargs) Create a secret containing your Service Principal credentials:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: azureoperatorsettings namespace: openshift-operators stringData: AZURE_TENANT_ID: $AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID: $ASO_USER AZURE_CLIENT_SECRET: $ASO_PASS AZURE_CLOUD_ENV: AzurePublicCloud EOF Deploy the ASO Operator:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/azure-service-operator.openshift-operators: \"\" name: azure-service-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: azure-service-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: azure-service-operator.v1.0.28631 EOF Deploy an Azure Redis Cache Create a Project:\noc new-project redis-demo Allow the redis app to run as any user:\noc adm policy add-scc-to-user anyuid -z default Create a random string to use as the unique redis hostname:\nREDIS_HOSTNAME=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 8 | head -n 1) Deploy a Redis service using the ASO Operator and an example application\ncat \u003c\u003cEOF | oc apply -f - apiVersion: azure.microsoft.com/v1alpha1 kind: RedisCache metadata: name: $REDIS_HOSTNAME spec: location: $AZURE_REGION resourceGroup: $AZURE_RESOURCE_GROUP properties: sku: name: Basic family: C capacity: 1 enableNonSslPort: true --- apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS value: $REDIS_HOSTNAME.redis.cache.windows.net - name: REDIS_PWD valueFrom: secretKeyRef: name: rediscache-$REDIS_HOSTNAME key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Wait for Redis to be ready\nThis may take 10 to 15 minutes.\nwatch oc get rediscache $REDIS_HOSTNAME the output should eventually show the following:\nNAME PROVISIONED MESSAGE l67for49 true successfully provisioned Get the URL of the example app\noc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working\nCleanup Delete the project containing the demo app\noc delete project redis-demo ","description":"","tags":["ARO","Azure"],"title":"Azure Service Operator V1 in ARO","uri":"/docs/aro/azure-service-operator/v1/"},{"content":"The Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.\nThis example uses ASO V2, which is a replacement for ASO V1. Equivalent documentation for ASO V1 can be found here . For new installs, V2 is recommended. MOBB has not tested running them in parallel.\nPrerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster The helm CLI tool Prepare your Azure Account and ARO Cluster Install cert-manager:\nASO relies on having the CRDs provided by cert-manager so it can request self-signed certificates. By default, cert-manager creates an Issuer of type SelfSigned, so it will work for ASO out-of-the-box. On an OpenShift cluster, the easiest way to do this is by using the OCP console, navigating to ‘Operators | OperatorHub’ and installing it from there; both the Red Hat certified and community versions will work. It’s also possible to install by applying manifests directly as covered here .\nSet the following environment variables:\nNote: modify the cluster name, region and resource group to match your cluster\nAZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"test-cluster\" AZURE_RESOURCE_GROUP=\"test-rg\" AZURE_REGION=\"westus2\" Create a Service Principal with Contributor permissions to your subscription:\nNote: You may want to lock this down to a specific resource group.\naz ad sp create-for-rbac -n \"$CLUSTER_NAME-aso\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID The result should look something like this:\n{ \"appId\": \"12f48391-31ac-4565-936a-8249232aeb18\", \"displayName\": \"test-cluster-aso\", \"password\": \"xsr5Pz3IsPnnYxhsc7LhnNkY00cYxe.IPk\", \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } You’ll need two of these values for the Helm deploy of ASO:\nAZURE_CLIENT_ID=\u003cthe_appId_from_above\u003e AZURE_CLIENT_SECRET=\u003cthe_password_from_above\u003e Deploy the ASO Operator using Helm:\nFirst, add the ASO repo (this may already be present, Helm will thow a status message if so):\nhelm repo add aso2 \\ https://raw.githubusercontent.com/Azure/azure-service-operator/main/v2/charts Then install the operator itself:\nhelm upgrade --install --devel aso2 aso2/azure-service-operator \\ --create-namespace \\ --namespace=azureserviceoperator-system \\ --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientID=$AZURE_CLIENT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET It will typically take 2-3 minutes for resources to converge and for the controller to be read to provision Azure resources. There will be one Pod created in the azureserviceoperator-system namespace with two containers, an oc -n azureserviceoperator-system logs \u003cpod_name\u003e manager will likely show a string of ‘TLS handshake error’ messages as the operator waits for a Certificate to be issued, but when they stop, the operator will be ready.\nDeploy an Azure Redis Cache Create a Project:\noc new-project redis-demo Allow the redis app to run as any user:\noc adm policy add-scc-to-user anyuid -z redis-demo Create an Azure Resource Group to hold project resources. Make sure the namespace matches the project name, and that the location is in the same region the cluster is:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: resources.azure.com/v1beta20200601 kind: ResourceGroup metadata: name: redis-demo namespace: redis-demo spec: location: westus EOF Deploy a Redis service using the ASO Operator. This also shows creating a random string as part of the hostname because the Azure DNS namespace is global, and a name like sampleredis is likely to be taken. Also make sure the location spec matches.\nREDIS_HOSTNAME=redis-$(head -c24 \u003c /dev/random | base64 | LC_CTYPE=C tr -dc 'a-z0-9' | cut -c -8) cat \u003c\u003cEOF | oc apply -f - apiVersion: cache.azure.com/v1beta20201201 kind: Redis metadata: name: $REDIS_HOSTNAME namespace: redis-demo spec: location: westus owner: name: redis-demo sku: family: C name: Basic capacity: 0 enableNonSslPort: true redisConfiguration: maxmemory-delta: \"10\" maxmemory-policy: allkeys-lru redisVersion: \"6\" operatorSpec: secrets: primaryKey: name: redis-secret key: primaryKey secondaryKey: name: redis-secret key: secondaryKey hostName: name: redis-secret key: hostName port: name: redis-secret key: port EOF This will take a couple of minutes to complete as well. Also note that there is typically a bit of lag between a resource being created and showing up in the Azure Portal.\nDeploy the sample application This uses a published sample application from Microsoft:\ncat \u003c\u003cEOF | oc -n redis-demo apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS valueFrom: secretKeyRef: name: redis-secret key: hostName - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS_PWD valueFrom: secretKeyRef: name: redis-secret key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Get the URL of the example app\noc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working\nCleanup Delete the project containing the demo app\noc delete project redis-demo Further Resources There is a library of examples for creating various Azure resource types here: https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples ","description":"","tags":["ARO","Azure"],"title":"Azure Service Operator V2 in ARO","uri":"/docs/aro/azure-service-operator/v2/"},{"content":"This guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user’s group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate and manage authorization using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation .\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because \u003ccode\u003ezsh\u003c/code\u003e disables comments in interactive shells from being used .\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\" Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade , then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username”, as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation .\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\nNext, select the “Add groups claim” button.\nSelect the “Security groups” option and click the “Add” button to configure group claims for your Azure AD application.\nNote: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend _scoping the groups provided by the group claim to only those groups which are applicable to OpenShift.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we’ll configure the cluster’s OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified:\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims preferred_username \\ --groups-claims groups 4. Grant additional permissions to individual groups Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID).\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID.\nGROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access.\nFor more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation .\n","description":"","tags":null,"title":"Configure ROSA to use Azure AD Group Claims","uri":"/docs/idp/group-claims/rosa/"},{"content":"","description":"","tags":null,"title":"Quay on ARO","uri":"/docs/aro/setup-quay/"},{"content":"\nRed Hat Quay setup on ARO (Azure Openshift) A guide to deploying an Azure Red Hat OpenShift Cluster with Red Hat Quay.\nAuthor: [Kristopher White x Connor Wooley]\nVideo Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through Quay Registry Storage Setup on YouTube .\nRed Hat Quay Setup Backend Storage Setup Login to Azure Search/Click Create Resource Groups\nName Resource Group \u003e Click Review + Create \u003e Click Create\nSearch/Click Create Storage Accounts\nChoose Resource Group \u003e Name Storage Account \u003e Choose Region \u003e Choose Performance \u003e Choose Redundancy \u003e Click Review + Create \u003e Click Create Click Go To Resource\nGo to Data Storage \u003e Click Container \u003e Click New Container \u003e Name Container \u003e Set Privacy to Public Access Blob \u003e Click Create\nGo to Storage Account \u003e Click Access Keys \u003e Go to key 1 \u003e Click Show Key\nStorage Account Name, Container Name, and Access Keys will be used to configure quay registry storage.\nRed Hat Quay Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials.\nMake sure you have selected the Administrator view.\nClick Operators \u003e OperatorHub \u003e Red Hat Quay.\nSearch for and click the tile for the Red Hat Quay operator.\nClick Install.\nIn the Install Operator pane:\nSelect the latest update channel.\nSelect the option to install Red Hat Quay in one namespace or for all namespaces on your cluster. If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace.\nSelect the Automatic approval strategy.\nClick Install.\nSuccessful Install Redhat Quay Registry Deployment Make sure you have selected the Administrator view.\nClick Operators \u003e Installed Operators \u003e Red Hat Quay \u003e Quay Registry \u003e Create QuayRegistry.\nForm View YAML View Click Create \u003e Click Registry\nSuccessful Registry Deployment Click Config Editor Credentials Secret\nGo to Data \u003e Reveal Values (These values are used to login to Config Editor Endpoint)\nGo to Registry Console \u003e Click Config Editor Endpoint \u003e\nScroll down to Registry Storage \u003e Click Edit Fields \u003e Go to Storage Engine click the drop down and select Azure Blob Storage \u003e Fill in Azure Storage Container with Storage Container Name \u003e Fill in Azure Account Name with Azure Storage Account Name \u003e Fill in Azure Account Key with Azure Storage Account Access Key\nClick Validate Configuration Changes\nClick Reconfigure Quay Go to Registry Console \u003e Click Registry Endpoint\nClick Create Account\nLogin to Quay.\nClick Create Repository\n","description":"","tags":["ARO","Azure"],"title":"Setting up Quay on an ARO cluster via Console","uri":"/docs/aro/setup-quay/quay-console/"},{"content":"Observability ROSA Submariner ROSA Submariner ARO Submariner ","description":"MOBB Docs and Guides for acm","tags":null,"title":"Advanced Cluster Management","uri":"/docs/redhat/acm/"},{"content":" ACM Observability on ROSA ","description":"","tags":null,"title":"Advanced Cluster Management - Observability","uri":"/docs/redhat/acm/observability/"},{"content":"Misc Topics: Common Managed OpenShift References/Tasks Cost Management for Cloud Services Azure DevOps with Managed OpenShift Custom TLS Ciphers Jupyter Notebook K8s Secret Store CSI OADP Sharing Common Images Stop Default Router from Serving Custom Domains ","description":"MOBB Docs and Guides for misc","tags":null,"title":"Miscellaneous","uri":"/docs/misc/"},{"content":"MOBB Docs and Guides for group-claims\n","description":"MOBB Docs and Guides for group-claims","tags":null,"title":"MOBB Docs and Guides - group-claims","uri":"/docs/idp/group-claims/"},{"content":"MOBB Docs and Guides for OADP Deploying OADP on ROSA ","description":"MOBB Docs and Guides for oadp","tags":null,"title":"MOBB Docs and Guides - oadp","uri":"/docs/misc/oadp/"},{"content":" Deploy Grafana on OpenShift 4 OpenShift Logging Azure Log Analytics ","description":"MOBB Docs and Guides for Observability","tags":null,"title":"Observability","uri":"/docs/o11y/"},{"content":" 3Scale API Management on ROSA and OSD RHACS on ARO/ROSA GitOps w/ ArgoCD Migrate K8s Apps w/ Konveyer Crane ","description":"","tags":null,"title":"Other Red Hat Products","uri":"/docs/redhat/"},{"content":"","description":"","tags":null,"title":"Backup","uri":"/tags/backup/"},{"content":"Prerequisites An STS enabled ROSA cluster Getting Started Create the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o jsonpath='{.spec.serviceAccountIssuer}' | sed 's|^https://||') export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export CLUSTER_VERSION=`rosa describe cluster -c ${CLUSTER_NAME} -o json | jq -r .version.raw_id | cut -f -2 -d '.'` export ROLE_NAME=\"${CLUSTER_NAME}-openshift-oadp-aws-cloud-credentials\" export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${CLUSTER_NAME}/oadp\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy to allow for S3 Access\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaOadpVer1'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateBucket\", \"s3:DeleteBucket\", \"s3:PutBucketTagging\", \"s3:GetBucketTagging\", \"s3:PutEncryptionConfiguration\", \"s3:GetEncryptionConfiguration\", \"s3:PutLifecycleConfiguration\", \"s3:GetLifecycleConfiguration\", \"s3:GetBucketLocation\", \"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\", \"s3:ListBucketMultipartUploads\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\", \"ec2:DescribeSnapshots\", \"ec2:DescribeVolumes\", \"ec2:DescribeVolumeAttribute\", \"ec2:DescribeVolumesModifications\", \"ec2:DescribeVolumeStatus\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\" ], \"Resource\": \"*\" } ]} EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaOadpVer1\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn \\ --tags Key=rosa_openshift_version,Value=${CLUSTER_VERSION} Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-oadp Key=operator_name,Value=openshift-oadp \\ --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster\ncat \u003c\u003cEOF \u003e ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ENDPOINT}:sub\": [ \"system:serviceaccount:openshift-adp:openshift-adp-controller-manager\", \"system:serviceaccount:openshift-adp:velero\"] } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \\ \"${ROLE_NAME}\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --tags Key=rosa_cluster_id,Value=${ROSA_CLUSTER_ID} Key=rosa_openshift_version,Value=${CLUSTER_VERSION} Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-adp Key=operator_name,Value=openshift-oadp \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role\naws iam attach-role-policy --role-name \"${ROLE_NAME}\" \\ --policy-arn ${POLICY_ARN} Deploy OADP on cluster Create a namespace for OADP\noc create namespace openshift-adp Create a credentials secret\ncat \u003c\u003cEOF \u003e ${SCRATCH}/credentials [default] role_arn = ${ROLE_ARN} web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc -n openshift-adp create secret generic cloud-credentials \\ --from-file=${SCRATCH}/credentials Deploy OADP Operator\nNOTE: there is currently an issue with 1.1 of the operator with backups that have a PartiallyFailed status. This does not seem to affect the backup and restore process, but it should be noted as there are issues with it.\ncat \u003c\u003c EOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-adp- namespace: openshift-adp name: oadp spec: targetNamespaces: - openshift-adp --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: redhat-oadp-operator namespace: openshift-adp spec: channel: stable-1.2 installPlanApproval: Automatic name: redhat-oadp-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait for the operator to be ready\nwatch oc -n openshift-adp get pods NAME READY STATUS RESTARTS AGE openshift-adp-controller-manager-546684844f-qqjhn 1/1 Running 0 22s Create Cloud Storage\ncat \u003c\u003c EOF | oc create -f - apiVersion: oadp.openshift.io/v1alpha1 kind: CloudStorage metadata: name: ${CLUSTER_NAME}-oadp namespace: openshift-adp spec: creationSecret: key: credentials name: cloud-credentials enableSharedConfig: true name: ${CLUSTER_NAME}-oadp provider: aws region: $REGION EOF Check your application’s storage default storage class\noc get pvc -n \u003cnamespace\u003e NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE applog Bound pvc-351791ae-b6ab-4e8b-88a4-30f73caf5ef8 1Gi RWO gp3-csi 4d19h mysql Bound pvc-16b8e009-a20a-4379-accc-bc81fedd0621 1Gi RWO gp3-csi 4d19h oc get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE gp2 kubernetes.io/aws-ebs Delete WaitForFirstConsumer true 4d21h gp2-csi ebs.csi.aws.com Delete WaitForFirstConsumer true 4d21h gp3 ebs.csi.aws.com Delete WaitForFirstConsumer true 4d21h gp3-csi (default) ebs.csi.aws.com Delete WaitForFirstConsumer true 4d21h Using either gp3-csi, gp2-csi, gp3 or gp2 will work. If the application(s) that are being backed up are all using PV’s with CSI, we recommend including the CSI plugin in the OADP DPA configuration.\nDeploy a Data Protection Application - CSI only\ncat \u003c\u003c EOF | oc create -f - apiVersion: oadp.openshift.io/v1alpha1 kind: DataProtectionApplication metadata: name: ${CLUSTER_NAME}-dpa namespace: openshift-adp spec: backupImages: false features: dataMover: enable: false backupLocations: - bucket: cloudStorageRef: name: ${CLUSTER_NAME}-oadp credential: key: credentials name: cloud-credentials default: true configuration: velero: defaultPlugins: - openshift - aws - csi restic: enable: false EOF Deploy a Data Protection Application - CSI or non-CSI volumes\ncat \u003c\u003c EOF | oc create -f - apiVersion: oadp.openshift.io/v1alpha1 kind: DataProtectionApplication metadata: name: ${CLUSTER_NAME}-dpa namespace: openshift-adp spec: backupImages: false features: dataMover: enable: false backupLocations: - bucket: cloudStorageRef: name: ${CLUSTER_NAME}-oadp credential: key: credentials name: cloud-credentials default: true configuration: velero: defaultPlugins: - openshift - aws restic: enable: false snapshotLocations: - velero: config: credentialsFile: /tmp/credentials/openshift-adp/cloud-credentials-credentials enableSharedConfig: 'true' profile: default region: ${REGION} provider: aws EOF Note\nContainer image backup and restore ( spec.backupImages=false ) is disabled and not supported in OADP 1.1.x or OADP 1.2.0 Rosa STS environments. The Restic feature ( restic.enable=false ) is disabled and not supported in Rosa STS environments. The DataMover feature ( dataMover.enable=false ) is disabled and not supported in Rosa STS environments. Perform a backup Note the following sample hello-world application has no attached PV’s. Either DPA configuration will work.\nCreate a workload to backup\noc create namespace hello-world oc new-app -n hello-world --image=docker.io/openshift/hello-openshift Expose the route\noc expose service/hello-openshift -n hello-world Check the application is working.\ncurl `oc get route/hello-openshift -n hello-world -o jsonpath='{.spec.host}'` Hello OpenShift! Backup workload\ncat \u003c\u003c EOF | oc create -f - apiVersion: velero.io/v1 kind: Backup metadata: name: hello-world namespace: openshift-adp spec: includedNamespaces: - hello-world storageLocation: ${CLUSTER_NAME}-dpa-1 ttl: 720h0m0s EOF Wait until backup is done\nwatch \"oc -n openshift-adp get backup hello-world -o json | jq .status\" { \"completionTimestamp\": \"2022-09-07T22:20:44Z\", \"expiration\": \"2022-10-07T22:20:22Z\", \"formatVersion\": \"1.1.0\", \"phase\": \"Completed\", \"progress\": { \"itemsBackedUp\": 58, \"totalItems\": 58 }, \"startTimestamp\": \"2022-09-07T22:20:22Z\", \"version\": 1 } Delete the demo workload\noc delete ns hello-world Restore from the backup\ncat \u003c\u003c EOF | oc create -f - apiVersion: velero.io/v1 kind: Restore metadata: name: hello-world namespace: openshift-adp spec: backupName: hello-world EOF Wait for the Restore to finish\nwatch \"oc -n openshift-adp get restore hello-world -o json | jq .status\" { \"completionTimestamp\": \"2022-09-07T22:25:47Z\", \"phase\": \"Completed\", \"progress\": { \"itemsRestored\": 38, \"totalItems\": 38 }, \"startTimestamp\": \"2022-09-07T22:25:28Z\", \"warnings\": 9 } Check the workload is restored\noc -n hello-world get pods NAME READY STATUS RESTARTS AGE hello-openshift-9f885f7c6-kdjpj 1/1 Running 0 90s curl `oc get route/hello-openshift -n hello-world -o jsonpath='{.spec.host}'` Hello OpenShift! For troubleshooting tips please refer to the OADP team’s troubleshooting documentation Additional sample applications can be found in the OADP team’s sample applications directory Cleanup Delete the workload\noc delete ns hello-world Delete the Data Protection Application\noc -n openshift-adp delete dpa ${CLUSTER_NAME}-dpa Delete the Cloud Storage\noc -n openshift-adp delete cloudstorage ${CLUSTER_NAME}-oadp WARNING: if this command hangs, you may need to delete the finalizer:\noc -n openshift-adp patch cloudstorage ${CLUSTER_NAME}-oadp -p '{\"metadata\":{\"finalizers\":null}}' --type=merge Remove the operator if it is no longer required: oc -n openshift-adp delete subscription oadp-operator Remove the namespace for the operator: oc delete ns openshift-adp Remove the backup and restore resources from the cluster if they are no longer required: oc delete backup hello-world oc delete restore hello-world To delete the backup/restore and remote objects in s3\nvelero backup delete hello-world velero restore delete hello-world Remove the Custom Resource Definitions from the cluster if you no longer wish to have them: for CRD in `oc get crds | grep velero | awk '{print $1}'`; do oc delete crd $CRD; done for CRD in `oc get crds | grep -i oadp | awk '{print $1}'`; do oc delete crd $CRD; done Delete the AWS S3 Bucket\naws s3 rm s3://${CLUSTER_NAME}-oadp --recursive aws s3api delete-bucket --bucket ${CLUSTER_NAME}-oadp Detach the Policy from the role\naws iam detach-role-policy --role-name \"${ROLE_NAME}\" \\ --policy-arn \"${POLICY_ARN}\" Delete the role\naws iam delete-role --role-name \"${ROLE_NAME}\" ","description":"","tags":["ROSA","AWS","STS","OADP","Velero","Backup","Restore","Storage"],"title":"Deploying OpenShift API for Data Protection on a ROSA cluster","uri":"/docs/misc/oadp/rosa-sts/"},{"content":"Quickstarts / Getting Started Red Hat OpenShift on AWS (ROSA) Azure Red Hat OpenShift (ARO) Advanced Managed OpenShift ROSA Deploying ROSA in Private Link mode Add Public Ingress to Private Link Cluster Deploying ROSA PrivateLink Cluster with Ansible Deploying ROSA in STS mode Deploying ROSA in STS mode with Private Link Deploying ROSA in STS mode with custom KMS Key Deploying ROSA via CRD and GitOps Installing the AWS Load Balancer Operator on ROSA Assign Egress IP for External Traffic Adding AWS WAF in front of ROSA / OSD Use AWS Secrets CSI with ROSA in STS mode Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Configuring Alerts for User Workloads in ROSA AWS EFS on ROSA Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR) Configuring a ROSA cluster to use ECR secret operator Deploy and use the AWS Kubernetes Controller S3 controller Verify Required Permissions for a ROSA STS deployment STS OIDC flow in ROSA Operators Dynamic Certificates for ROSA Custom Domain External DNS for ROSA Custom Domain Security Reference Architecture for ROSA Configure ROSA for Nvidia GPU Workloads ARO Deploying private ARO Cluster with Jump Host access Using the Egressip Ipam Operator with a Private ARO Cluster Considerations for Disaster Recovery with ARO Getting Started with the Azure Key Vault CSI Driver Deploy and use the Azure Service Operator V1(ASO) Deploy and use the Azure Service Operator V2(ASO) Create an additional Ingress Controller for ARO Configure the Managed Upgrade Operator Configure ARO with Azure NetApp Trident Operator IBM Cloud Paks for Data Operator Setup Install ARO with Custom Domain using LetsEncrypt with cert manager Configure ARO for Nvidia GPU Workloads Configure ARO with Azure Front Door Create a point to site VPN connection for an ARO Cluster Configure access to ARO Image Registry Configure ARO with OpenShift Data Foundation Setting Up Quay on an ARO Cluster using Azure Container Storage via CLI via GUI Configure ARO with Azure Policy Create infrastructure nodes on an ARO Cluster Configure a load balancer service to use a static public IP Upgrade a disconnected ARO cluster Using Azure Container Registry in Private ARO clusters Configure a Private ARO cluster with Azure File via a Private Endpoint GCP Deploy OSD in GCP using Pre-Existent VPC and Subnets Using Filestore with OpenShift Dedicated in GCP Advanced Cluster Manager (ACM) Deploy ACM Observability to a ROSA cluster Deploy ACM Submariner for connecting overlay networks of ROSA clusters Deploy ACM Submariner for connect overlay networks ARO - ROSA clusters Observability Deploy Grafana on OpenShift 4 Configuring Alerts for User Workloads Federating ROSA metrics to S3 Federating ROSA metrics to AWS Prometheus Configure ROSA STS Cluster Logging to CloudWatch Federating ARO metrics to Azure Files Sending ARO cluster logs to Azure Log Analytics Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Security Kubernetes Secret Store CSI Driver Just the CSI itself + HashiCorp CSI + AWS Secrets CSI with ROSA in STS mode + Azure Key Vault CSI Driver Configuring Specific Identity Providers Configure GitLab as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ARO Configure Azure AD as an identity provider for ARO Configure Azure AD as an identitiy provider for ARO with group claims Configure Azure AD as an identitiy provider for ROSA with group claims Configure Azure AD as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ARO via the CLI Configure Red Hat SSO with Azure AD as a Federated Identity Provider for ARO Considerations when using AAD as IDP Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD Deploying Advanced Security for Kubernetes in ROSA/ARO Deploying ACS in ROSA/ARO Applications Deploying Astronomer to OpenShift Deploying 3scale API Management to ROSA/OSD Ingress Configure a custom ingress TLS profile for ROSA/OSD Data Science on Jupyter Notebook on OpenShift Prerequistes and Concepts Build minimal notebook JupyterHub notebook with GPU Miscellaneous Demonstrating GitOps - ArgoCD Migrate Kubernetes Applications with Konveyor Crane Red Hat Cost Management for Cloud Services Deploy OpenShift Advanced Data Protection on a ROSA STS cluster Azure DevOps with Managed OpenShift Fixes / Workarounds Here be dragons - use at your own risk\nFix Cluster Logging Operator Addon for ROSA STS Clusters Stop default router from serving custom domain routes ","description":"MOBB Docs and Guides","tags":null,"title":"Documentation from the MOBB","uri":"/"},{"content":"Quickstarts / Getting Started Red Hat OpenShift on AWS (ROSA) Azure Red Hat OpenShift (ARO) Advanced Managed OpenShift ROSA Deploying ROSA in Private Link mode Add Public Ingress to Private Link Cluster Deploying ROSA PrivateLink Cluster with Ansible Deploying ROSA in STS mode Deploying ROSA in STS mode with Private Link Deploying ROSA in STS mode with custom KMS Key Deploying ROSA via CRD and GitOps Installing the AWS Load Balancer Operator on ROSA Assign Egress IP for External Traffic Adding AWS WAF in front of ROSA / OSD Use AWS Secrets CSI with ROSA in STS mode Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Configuring Alerts for User Workloads in ROSA AWS EFS on ROSA Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR) Configuring a ROSA cluster to use ECR secret operator Deploy and use the AWS Kubernetes Controller S3 controller Verify Required Permissions for a ROSA STS deployment STS OIDC flow in ROSA Operators Dynamic Certificates for ROSA Custom Domain External DNS for ROSA Custom Domain Security Reference Architecture for ROSA Configure ROSA for Nvidia GPU Workloads ARO Deploying private ARO Cluster with Jump Host access Using the Egressip Ipam Operator with a Private ARO Cluster Considerations for Disaster Recovery with ARO Getting Started with the Azure Key Vault CSI Driver Deploy and use the Azure Service Operator V1(ASO) Deploy and use the Azure Service Operator V2(ASO) Create an additional Ingress Controller for ARO Configure the Managed Upgrade Operator Configure ARO with Azure NetApp Trident Operator IBM Cloud Paks for Data Operator Setup Install ARO with Custom Domain using LetsEncrypt with cert manager Configure ARO for Nvidia GPU Workloads Configure ARO with Azure Front Door Create a point to site VPN connection for an ARO Cluster Configure access to ARO Image Registry Configure ARO with OpenShift Data Foundation Setting Up Quay on an ARO Cluster using Azure Container Storage via CLI via GUI Configure ARO with Azure Policy Create infrastructure nodes on an ARO Cluster Configure a load balancer service to use a static public IP Upgrade a disconnected ARO cluster Using Azure Container Registry in Private ARO clusters Configure a Private ARO cluster with Azure File via a Private Endpoint GCP Deploy OSD in GCP using Pre-Existent VPC and Subnets Using Filestore with OpenShift Dedicated in GCP Advanced Cluster Manager (ACM) Deploy ACM Observability to a ROSA cluster Deploy ACM Submariner for connecting overlay networks of ROSA clusters Deploy ACM Submariner for connect overlay networks ARO - ROSA clusters Observability Deploy Grafana on OpenShift 4 Configuring Alerts for User Workloads Federating ROSA metrics to S3 Federating ROSA metrics to AWS Prometheus Configure ROSA STS Cluster Logging to CloudWatch Federating ARO metrics to Azure Files Sending ARO cluster logs to Azure Log Analytics Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Security Kubernetes Secret Store CSI Driver Just the CSI itself + HashiCorp CSI + AWS Secrets CSI with ROSA in STS mode + Azure Key Vault CSI Driver Configuring Specific Identity Providers Configure GitLab as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ARO Configure Azure AD as an identity provider for ARO Configure Azure AD as an identitiy provider for ARO with group claims Configure Azure AD as an identitiy provider for ROSA with group claims Configure Azure AD as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ARO via the CLI Configure Red Hat SSO with Azure AD as a Federated Identity Provider for ARO Considerations when using AAD as IDP Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD Deploying Advanced Security for Kubernetes in ROSA/ARO Deploying ACS in ROSA/ARO Applications Deploying Astronomer to OpenShift Deploying 3scale API Management to ROSA/OSD Ingress Configure a custom ingress TLS profile for ROSA/OSD Data Science on Jupyter Notebook on OpenShift Prerequistes and Concepts Build minimal notebook JupyterHub notebook with GPU Miscellaneous Demonstrating GitOps - ArgoCD Migrate Kubernetes Applications with Konveyor Crane Red Hat Cost Management for Cloud Services Deploy OpenShift Advanced Data Protection on a ROSA STS cluster Azure DevOps with Managed OpenShift Fixes / Workarounds Here be dragons - use at your own risk\nFix Cluster Logging Operator Addon for ROSA STS Clusters Stop default router from serving custom domain routes ","description":"MOBB Docs and Guides","tags":null,"title":"Documentation from the MOBB","uri":"/docs/"},{"content":"","description":"","tags":null,"title":"GCP","uri":"/tags/gcp/"},{"content":"","description":"","tags":null,"title":"OADP","uri":"/tags/oadp/"},{"content":" Filestore Storage for OSD on GCP Creating OSD on GCP w/ Existing VPC ","description":"MOBB Docs and Guides for GCP","tags":["GCP","OSD"],"title":"OSD on Google Cloud","uri":"/docs/gcp/"},{"content":"","description":"","tags":null,"title":"Restore","uri":"/tags/restore/"},{"content":"","description":"","tags":null,"title":"Storage","uri":"/tags/storage/"},{"content":"","description":"","tags":null,"title":"Velero","uri":"/tags/velero/"},{"content":"This page is deprecated. In order to get the best experience for custom alerting in ROSA, please upgrade your cluster to to 4.12 and follow the newer documentation.\nROSA 4.9.x introduces a new way to provide custom AlertManager configuration to receive alerts from User Workload Management.\nThe OpenShift Administrator can use the Prometheus Operator to create a custom AlertManager resource and then use the AlertManagerConfig resource to configure User Workload Monitoring to use the custom AlertManager.\nPrerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.9.0 or higher Create Environment Variables Before we get started we need to set some environment variables to be used throughout the guide.\nexport PROM_NAMESPACE=custom-alert-manager Install Prometheus Operator If you prefer you can do this from the Operator Hub in the cluster console itself.\nCreate a OperatorGroup and Subscription for the Prometheus Operator\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: ${PROM_NAMESPACE} --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: federated-metrics namespace: ${PROM_NAMESPACE} spec: targetNamespaces: - ${PROM_NAMESPACE} --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: prometheus namespace: ${PROM_NAMESPACE} spec: channel: beta installPlanApproval: Automatic name: prometheus source: community-operators sourceNamespace: openshift-marketplace EOF Deploy AlertManager Create an Alert Manager Configuration file\nThis will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration.\nSLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat \u003c\u003c EOF | kubectl apply -n ${PROM_NAMESPACE} -f - apiVersion: v1 kind: Secret metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: slack-notifications group_by: [alertname] receivers: - name: slack-notifications slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true --- apiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: securityContext: {} replicas: 3 configSecret: custom-alertmanager --- apiVersion: v1 kind: Service metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: type: ClusterIP ports: - name: web port: 9093 protocol: TCP targetPort: web selector: alertmanager: custom-alertmanager EOF Configure User Workload Monitoring to use the custom AlertManager Create an AlertManagerConfig for User Workload Monitoring\nNote: This next command assumes the existing config.yaml in the user-workload-monitoring-config config map is empty. You should verify it with kubectl get -n openshift-user-workload-monitoring cm user-workload-monitoring-config -o yaml and simply edit in the differences if its not.\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | thanosRuler: additionalAlertmanagerConfigs: - scheme: http pathPrefix: / timeout: \"30s\" apiVersion: v1 staticConfigs: [\"custom-alertmanager.$PROM_NAMESPACE.svc.cluster.local:9093\"] EOF Create an Example Alert Verify it works by creating a Prometheus Rule that will fire off an alert\ncat \u003c\u003c EOF | kubectl apply -n $PROM_NAMESPACE -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules namespace: ${PROM_NAMESPACE} spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service\nkubectl port-forward -n ${PROM_NAMESPACE} svc/custom-alertmanager 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert “ExampleAlert”\nCheck the Alert was sent to Slack\n","description":"","tags":["AWS","ROSA"],"title":"Custom AlertManager in ROSA 4.9.x","uri":"/docs/rosa/custom-alertmanager-4.9/"},{"content":"This guide demonstrates how to properly patch the cluster ingress controllers, as well as ingress controllers created by the Custom Domain Operator. This functionality allows customers to modify the tlsSecurityProfile value on cluster ingress controllers. This guide will demonstrate how to apply a custom tlsSecurityProfile, a scoped service account (with the associated role and role binding), and a CronJob that the cipher changes are reapplied with 60 minutes (in the event that an ingress controller is recreated or modified).\nBefore you Begin Review the OpenShift Documentation that explains the options for the \u003ccode\u003etlsSecurityProfile\u003c/code\u003e . By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile .\n1. Create a service account for the CronJob to use A service account allows our CronJob to directly access the cluster API, without using a regular user’s credentials. To create a service account, run the following command:\noc create sa cron-ingress-patch-sa -n openshift-ingress-operator 2. Create a role and role binding that allows limited access to patch the ingress controllers Role-based access control (RBAC) is critical to ensuring security inside your cluster. Creating a role allows us to provide scoped access to only the API resources we need within the cluster. To create the role, run the following command:\noc create role cron-ingress-patch-role --verb=get,patch,update --resource=ingresscontroller.operator.openshift.io -n openshift-ingress-operator Once the role has been created, you need to bind the role to the service account using a role binding. To create the role binding, run the following command:\noc create rolebinding cron-ingress-patch-rolebinding --role=cron-ingress-patch-role --serviceaccount=openshift-ingress-operator:cron-ingress-patch-sa -n openshift-ingress-operator 3. Patch the ingress controller Important note: The examples provided below add an additional cipher to the ingress controller’s tlsSecurityProfile to allow IE 11 access from Windows Server 2008 R2. You should modify this command to meet your specific business requirements.\nBefore we create the CronJob, we first want to apply the tlsSecurityProfile configuration to validate our changes. This process depends on if you are using the Custom Domain Operator .\nClusters not using the Custom Domain Operator If you are only using the default ingress controller, and not using the Custom Domain Operator , you will run the following command to patch the ingress controller:\noc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' This patch will add the TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA cipher which allows access from IE 11 on Windows Server 2008 R2 when using RSA certificates.\nOnce you’ve run the command, you’ll receive a response that looks like this:\ningresscontroller.operator.openshift.io/default patched Clusters using the Custom Domain Operator Customers who are using the Custom Domain Operator will need to loop through each of their ingress controllers to patch each one. To patch all of your cluster’s ingress controllers, run the following command:\nfor ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done Once you’ve run the command, you’ll receive a response that looks like this:\ningresscontroller.operator.openshift.io/default patched ingresscontroller.operator.openshift.io/custom1 patched ingresscontroller.operator.openshift.io/custom2 patched 4. Create the CronJob to ensure the TLS configuration is not overwritten Occasionally, the cluster’s ingress controller can get recreated. In these cases, the ingress controller will likely not retain the tlsSecurityProfile changes that we’ve made. To ensure this doesn’t happen, we’ll create a CronJob that goes through and updates the cluster’s ingress controller(s). This process depends on if you are using the Custom Domain Operator .\nClusters not using the Custom Domain Operator If you are not using the Custom Domain Operator , creating the CronJob is as simple as running the following command:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything:\ningresscontroller.operator.openshift.io/default patched (no change) Clusters using the Custom Domain Operator If you are using the Custom Domain Operator the CronJob will need to loop through and patch each ingress controller. To create this CronJob, run the following command:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything:\ningresscontroller.operator.openshift.io/default patched (no change) ingresscontroller.operator.openshift.io/custom1 patched (no change) ingresscontroller.operator.openshift.io/custom2 patched (no change) ","description":"","tags":["ROSA","AWS","OSD"],"title":"Configure ROSA/OSD to use custom TLS ciphers on the ingress controllers","uri":"/docs/misc/tls-cipher-customization/"},{"content":"This guide shows how to deploy the Cluster Log Forwarder operator and configure it to use STS authentication to forward logs to CloudWatch.\nPrerequisites A ROSA cluster (configured with STS) The jq cli command The aws cli command Environment Setup Configure the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport ROSA_CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\" | sed 's/-[a-z0-9]\\{5\\}$//') export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer | sed 's|^https://||') export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-sts\" mkdir -p ${SCRATCH} echo \"Cluster: ${ROSA_CLUSTER_NAME}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy for OpenShift Log Forwarding\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster\ncat \u003c\u003cEOF \u003e ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ENDPOINT}:sub\": \"system:serviceaccount:openshift-logging:logcollector\" } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role\naws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn ${POLICY_ARN} Deploy Operators Deploy the Cluster Logging operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Create a secret\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: role_arn: $ROLE_ARN EOF Configure Cluster Logging Create a cluster log forwarding resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: logs: type: fluentd forwarder: fluentd: {} managementState: Managed EOF Check AWS CloudWatch for logs Use the AWS console or CLI to validate that there are log streams from the cluster\nNote: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet.\naws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] } Cleanup Delete the Cluster Log Forwarding resource\noc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource\noc delete -n openshift-logging clusterlogging instance Detach the IAM Policy to the IAM Role\naws iam detach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn \"${POLICY_ARN}\" Delete the IAM Role\naws iam delete-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" Delete the IAM Policy\nOnly run this command if there are no other resources using the Policy\naws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups\naws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\" ","description":"","tags":["AWS","ROSA"],"title":"Configuring the Cluster Log Forwarder for CloudWatch Logs and STS","uri":"/docs/rosa/clf-cloudwatch-sts/"},{"content":"This document shows how to set up infrastructure nodes in an ARO cluster and move infrastructure related workloads to them. This can help with larger clusters that have resource contention between user workloads and infrastructure workloads such as Prometheus.\nImportant note: Infrastructure nodes are billed at the same rates as your existing ARO worker nodes.\nYou can find the original (and more detailed) document describing the process for a self-managed OpenShift Container Platform cluster here Prerequisites Azure Red Hat OpenShift cluster Helm CLI Create Infra Nodes We’ll use the MOBB Helm Chart for adding ARO machinesets which defaults to creating infra nodes, it looks up an existing machineset to collect cluster specific settings and then creates a new machineset specific for infra nodes with the same settings.\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Install the mobb/aro-machinesets Chart to create infra nodes\nhelm upgrade --install -n openshift-machine-api \\ infra mobb/aro-machinesets Wait for the new nodes to be available\nwatch oc get machines Moving Infra workloads Ingress You may choose this for any additional Ingress controllers you may have in the cluster, however if you application has very high Ingress resource requirements it may make sense to allow them to spread across the worker nodes, or even a dedicated MachineSet.\nSet the nodePlacement on the ingresscontroller to node-role.kubernetes.io/infra and increase the replicas to match the number of infra nodes\noc patch -n openshift-ingress-operator ingresscontroller default --type=merge \\ -p='{\"spec\":{\"replicas\":3,\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}}' Check the Ingress Controller Operator is starting pods on the new infra nodes\noc -n openshift-ingress get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-69f58645b7-6xkvh 1/1 Running 0 66s 10.129.6.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw \u003cnone\u003e \u003cnone\u003e router-default-69f58645b7-vttqz 1/1 Running 0 66s 10.131.4.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e router-default-6cb5ccf9f5-xjgcp 1/1 Terminating 0 23h 10.131.0.11 cz-cluster-hsmtw-worker-eastus2-xj9qx \u003cnone\u003e \u003cnone\u003e Registry Set the nodePlacement on the registry to node-role.kubernetes.io/infra\noc patch configs.imageregistry.operator.openshift.io/cluster --type=merge \\ -p='{\"spec\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"namespaces\":[\"openshift-image-registry\"],\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"logLevel\":\"Normal\",\"managementState\":\"Managed\",\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}' Check the Registry Operator is starting pods on the new infra nodes\noc -n openshift-image-registry get pods -l \"docker-registry\" -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES image-registry-84cbd76d5d-cfsw7 1/1 Running 0 3h46m 10.128.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml \u003cnone\u003e \u003cnone\u003e image-registry-84cbd76d5d-p2jf9 1/1 Running 0 3h46m 10.129.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw \u003cnone\u003e \u003cnone\u003e Cluster Monitoring Configure the cluster monitoring stack to use the infra nodes\nNote: This will override any other customizations to the cluster monitoring stack, so you may want to merge your existing customizations into this before running the command.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusOperator: {} grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" EOF Check the OpenShift Monitoring Operator is starting pods on the new infra nodes\nsome Pods like prometheus-operator will remain on master nodes.\noc -n openshift-monitoring get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES alertmanager-main-0 6/6 Running 0 2m14s 10.128.6.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml \u003cnone\u003e \u003cnone\u003e alertmanager-main-1 6/6 Running 0 2m46s 10.131.4.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e cluster-monitoring-operator-5bbfd998c6-m9w62 2/2 Running 0 28h 10.128.0.23 cz-cluster-hsmtw-master-1 \u003cnone\u003e \u003cnone\u003e grafana-599d4b948c-btlp2 3/3 Running 0 2m48s 10.131.4.10 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e kube-state-metrics-574c5bfdd7-f7fjk 3/3 Running 0 2m49s 10.131.4.8 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e ... ... ","description":"","tags":["ARO","Azure"],"title":"Adding infrastructure nodes to an ARO cluster","uri":"/docs/aro/add-infra-nodes/"},{"content":"Introduction Occasionally when you’re moving between major version of Kubernetes or Red Hat OpenShift, you’ll want to migrate your applications between clusters. Or if you’re moving between two clouds, you’ll want an easy way to migrate your workloads from one platform to another.\nThe Crane operator from the open source Konveyer project automates this migration process for you. The Konveyer site offers a selection of helpful projects to administer your cluster. Crane is designed to automate migration from one cluster to another and is surprisingly easy to get working.\nThis article shows you how we moved a default sample application from a Red Hat OpenShift on AWS (ROSA) to a Red Hat OpenShift on IBM Cloud (ROIC) cluster. To see how it’s done, watch the video or read the steps below.\nInstall the Crane operator First, log into the cluster console where your original application is hosted and also log into the console of the destination where you want to migrate your application. In our example, we logged into the OpenShift Service on AWS as our origin console and, in another tab, logged into to the Red Hat OpenShift on IBM Cloud console as our destination console.\nFrom the Operator Hub in both consoles, search for “Crane Operator” and follow the default prompts to install the operator.\nSet up your sample application From your origin cluster, choose the Developer profile and then click +Add to add a project where you will deploy your application into.\nChoose a sample app to play with. In our example, we chose the Python application. Name it and then click “Create”. It will pull the source information from GitHub and build an image, deploy the image, and expose it as a PHP endpoint.\nYou can change back from a Developer profile to the Admin profile in order to see if the operator has been installed correctly.\nCreate a migration (MIG) controller Now it’s time to create your migration controller.\nGo to your m migration cluster (in this example, our IBM console), select the Crane operator, and select Create migration controller. Do the same on the origin cluster (in our example, AWS).\nSwitch to the openshift-migration namespace.\nUpdate the host MIG controller.\napiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: host namespace: openshift-migration spec: isHostCluster: true Then you can apply your migration cluster.\nkubectl apply -f origin-migcluster.yaml NOTE: Run only this following command on the remote cluster Save your service account secret for the destination cluster.\noc sa get-token migration-controller -n openshift-migration | base64 -w 0 Write this into sa-secret-remote.yaml on your origin cluster:\napiVersion: v1 kind: Secret metadata: name: sa-token-remote namespace: openshift-config type: Opaque data: # [!] Change saToken to contain a base64 encoded SA token with cluster-admin # privileges on the remote cluster. # `oc sa get-token migration-controller -n openshift-migration | base64 -w 0` saToken: \u003cyour-base64-encoded-aws-sa-token-here\u003e kubectl apply -f sa-secret-remote.yaml Add your destination cluster:\napiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: src-ocp-3-cluster namespace: openshift-migration spec: insecure: true isHostCluster: false serviceAccountSecretRef: name: sa-token-remote namespace: openshift-config url: 'https://master.ocp3.mycluster.com/' kubectl apply -f dest-migcluster.yaml Configure s3 credentials to host migration storage. Included here is the correct access key for my files. You’ll need to have that handy.\napiVersion: v1 kind: Secret metadata: namespace: openshift-config name: migstorage-creds type: Opaque data: aws-access-key-id: aGVsbG8K aws-secret-access-key: aGVsbG8K kubectl apply -f mig-storage-creds.yaml Configure MIG storage to use s3\napiVersion: migration.openshift.io/v1alpha1 kind: MigStorage metadata: name: aws-s3 namespace: openshift-migration spec: backupStorageConfig: awsBucketName: konveyer-jj-migration # You need to change this for your s3 bucket credsSecretRef: name: migstorage-creds namespace: openshift-config backupStorageProvider: aws volumeSnapshotConfig: credsSecretRef: name: migstorage-creds namespace: openshift-config volumeSnapshotProvider: aws kubectl apply -f migstorage.yaml Create the migration plan. THe plan is essentially saying: “This is what I want”. The plan says what namespace you want to move. In this example, it’s project-a as referenced below.\napiVersion: migration.openshift.io/v1alpha1 kind: MigPlan metadata: name: migrate-project-a namespace: openshift-migration spec: destMigClusterRef: name: destination namespace: openshift-migration indirectImageMigration: true indirectVolumeMigration: true srcMigClusterRef: name: host namespace: openshift-migration migStorageRef: name: aws-s3 namespace: openshift-migration namespaces: - project-a # notice this is where you'd add other projects persistentVolumes: [] kubectl apply -f migplan.yaml Execute your migration plan.\napiVersion: migration.openshift.io/v1alpha1 kind: MigMigration metadata: name: migrate-project-a-execute namespace: openshift-migration spec: migPlanRef: name: migrate-project-a namespace: openshift-migration quiescePods: true stage: false kubectl apply -f migplanexecute.yaml Watch the magic Back in your original console, you can find the correct namespace and see that things have moved.\nGo back to the origin OpenShift console (AWS in our example). Bring up the migration GUI from the openshift-migration namespace. Check through the migration plans which show you the migration or watch the logs to see the migration happening.\nOr, watch the progress via the CLI if you prefer.\nkubectl logs -f migration-log-reader-\u003chash\u003e color You can also open your destination console (Red Hat OpenShift on IBM Cloud in our example) and see if the new cluster has migrated.\nConclusion Hopefully walking through these steps has helped you understand the power that Crane can offer you when migrating workloads between clusters. This was only a sample application. With a little work and testing, you should be able to leverage Crane for your applications. If you have any questions or thoughts, come around to #konveyer on the Kubernetes public Slack channel, and the team would me more then willing to help advise you.\nHappy migrating your apps!\n","description":"","tags":[],"title":"Migrate Kubernetes Applications with Konveyer Crane","uri":"/docs/redhat/crane/"},{"content":"Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Azure Policy supports arc enabled kubernetes cluster with both build-in and custom policies to ensure kubernetes resources are compliant. This article demonstrates how to make Azure Redhat Openshift cluster compliant with azure policy.\nPrerequisites Azure CLI Openshift CLI Azure Openshift Cluster (ARO Cluster) Deploy Azure Policy Deploy Azure Arc and Enable Azure Policy Add-on az connectedk8s connect -n [Cluster_Name] -g [Resource_Group_Name] az k8s-extension create --cluster-type connectedClusters --cluster-name [Cluster_Name] --resource-group [Resource_Group_Name] --extension-type Microsoft.PolicyInsights --name azurepolicy Verify Azure Arc and Azure Policy Add-on oc get pod -n azure-arc NAME READY STATUS RESTARTS AGE cluster-metadata-operator-6d4b957d65-5ts9b 2/2 Running 0 3h31m clusterconnect-agent-d5d6c6848-kbmfc 3/3 Running 0 3h31m clusteridentityoperator-6f5bf5c94-6qxlm 2/2 Running 0 3h31m config-agent-54b48fb5d9-wll42 2/2 Running 0 3h31m controller-manager-69fd59cf7-lf2mf 2/2 Running 0 3h31m extension-manager-695f99c94d-q6zmw 2/2 Running 0 3h31m flux-logs-agent-88588c88-j9xf8 1/1 Running 0 3h31m kube-aad-proxy-74d5747967-jhxq2 2/2 Running 0 3h31m metrics-agent-854dfbdc74-948bn 2/2 Running 0 3h31m resource-sync-agent-77f8bb95d4-94dpm 2/2 Running 0 3h31m oc get pod -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-bbdd45779-lbsmf 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-25pt2 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-sztck 1/1 Running 0 3h25m Demo a simple policy This policy will allow only images from a specific registry.\nOpen Azure Portal Policy Services Click on Assign Policy Select the subscription and ARO cluster resource group as the scope Select “Kubernetes cluster containers should only use allowed images” in the “policy definition” field Click Next -\u003e fill out namespace inclusion as [“test-policy”] -\u003e Allowed Registry Regex as “index.docker.io.+$” Save the result. The policy will take effect after around 30 minutes. oc get K8sAzureContainerAllowedImages #Get the latest policy oc get K8sAzureContainerAllowedImages azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 -o yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAzureContainerAllowedImages metadata: annotations: azure-policy-assignment-id: /subscriptions/${subscription_id}/resourceGroups/shaozhen-tf-rg/providers/Microsoft.Authorization/policyAssignments/9f9d73056d5f422bb3bbbc5f azure-policy-definition-id: /providers/Microsoft.Authorization/policyDefinitions/febd0533-8e55-448f-b837-bd0e06f16469 azure-policy-definition-reference-id: \"\" azure-policy-setdefinition-id: \"\" constraint-installed-by: azure-policy-addon creationTimestamp: \"2022-07-25T16:19:12Z\" generation: 2 labels: managed-by: azure-policy-addon name: azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 resourceVersion: \"169521\" uid: 0e25efc6-0099-4e3c-86a9-a223dd01e13d spec: enforcementAction: deny match: excludedNamespaces: - kube-system - gatekeeper-system - azure-arc kinds: - apiGroups: - \"\" kinds: - Pod namespaces: - test-policy parameters: excludedContainers: [] imageRegex: index.docker.io.+$ Policy Engine denies images from quay.io oc run -ti --image quay.io/alpine test -- /bin/sh Error from server ([azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed.): admission webhook \"validation.gatekeeper.sh\" denied the request: [azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed. References Azure Policy Overview Azure Arc-enabled kubernetes Understand Azure Policy for Kubernetes Azure Arc-enabled kubernetes built-in policy ","description":"","tags":["ARO","Azure"],"title":"Apply Azure Policy to Azure Policy","uri":"/docs/aro/azure-policy/"},{"content":"Pre Requisites An ARO cluster oc cli azure cli Steps Create Azure Resources Create Storage Account\naz login az group create --name \u003cresource-group\u003e --location \u003clocation\u003e az storage account create --name \u003cstorage-account\u003e --resource-group \u003cresource-group\u003e \\ --location eastus --sku Standard_LRS --kind StorageV2 Create Storage Container\naz storage account keys list --account-name \u003cstorage_account_name\u003e --resource-group \u003cresource_group\u003e --output yaml Note: this command returns a json by default with your keyName and Values, command above specifies yaml\naz storage container create --name \u003ccontainer_name\u003e --public-access blob \\ --account-name \u003cAZURE_STORAGE_ACCOUNT\u003e --account-key \u003cAZURE_STORAGE_ACCOUNT_KEY\u003e Note: Will need the storage container creds for later use\nInstall Quay-Operator and Create Quay Registry Login to your cluster’s OCM\nCreate a sub.yaml file with this template to install the quay operator\napiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: quay-operator namespace: \u003cnamespace\u003e spec: channel: \u003crelease_channel\u003e name: quay-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: quay-operator.\u003cversion\u003e oc apply -f sub.yaml Create the Quay Registry\nCreate the Azure Storage Secret Bundle\nCreate a config.yaml file that injects the azure resource info from the storage container created in step 2 of Create Azure Resources DISTRIBUTED_STORAGE_CONFIG: local_us: - AzureStorage - azure_account_key: \u003cAZURE_STORAGE_ACCOUNT_KEY\u003e azure_account_name: \u003cAZURE_STORAGE_ACCOUNT\u003e azure_container: \u003cAZURE_CONTAINER_NAME\u003e storage_path: /datastorage/registry DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: - local_us DISTRIBUTED_STORAGE_PREFERENCE: - local_us oc create secret generic --from-file config.yaml=./config.yaml -n \u003cnamespace\u003e \u003cconfig_bundle_secret_name\u003e Create the Quay Registry with the Secret\nCreate a quayregistry.yaml file with this format apiVersion: quay.redhat.com/v1 kind: QuayRegistry metadata: name: \u003cregistry_name\u003e namespace: \u003cnamespace\u003e finalizers: - quay-operator/finalizer generation: 3 spec: configBundleSecret: \u003cconfig_bundle_secret_name\u003e components: - kind: clair managed: true - kind: postgres managed: true - kind: objectstorage managed: false - kind: redis managed: true - kind: horizontalpodautoscaler managed: true - kind: route managed: true - kind: mirror managed: true - kind: monitoring managed: true - kind: tls managed: true - kind: quay managed: true - kind: clairpostgres managed: true``` oc create -n \u003cnamespace\u003e -f quayregistry.yaml Login to your Quay Registry and begin pushing images to it!\nNote: This configuration does not support in-cluster authentication integration with the quay deployment. User Management with the registry is handled by the registry.\n","description":"","tags":["ARO","Azure"],"title":"Setting up Quay on an ARO cluster via CLI","uri":"/docs/aro/setup-quay/quay-cli/"},{"content":"Thatcher Hubbard\n15 July 2022\nThis guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Okta as an OIDC identity provider for ROSA/OSD guide.\nTo set up group synchronization from Okta to ROSA/OSD you must:\nDefine groups and assign users in Okta Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Okta To synchronize groups and users with ROSA/OSD they must exist in Okta\nCreate groups to syncronize with ROSA/OSD if they do not already exist\nCreate user IDs to synchronize with ROSA/OSD if they do not already exist\nAssign newly created users to the appropriate group\nInstall the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator\nInstall the operator in the group-sync-operator namespace\n(Optional) Create an Okta Group Sync Administrator Tokens are created with whatever permissions the currently logged-in user has, which is typically ‘Super Admin’ for a developer account. This is obviously not good practice for anything other than the most basic testing.\nIdeally, a user would be created inside the Okta organization that was specifically for group synchronizations, which should only need to be able to read groups. Creating a user with ‘Read Administrator’ permissions on the account would be a good place to start for following the principle of “least privilege”. That user can then issue a token that includes only those permissions.\nCreate an Okta API Access Token Login as a user that has minimally has Group and User read permissions (see previous section) and generate an API token in Okta\nCreate and configure a new Group Sync instance Create a new secret named okta-group-sync in the group-sync-operator namespace. This will contain the Okta API key that was just created.\nUsing the OpenShift CLI, create the secret using the following format:\noc create secret generic okta-api-token --from-literal='okta-api-token=${API_TOKEN}' -n group-sync-operator Obtain values from Okta for the AppId and URL. The AppId is the client ID under the application created to support OpenID for the OCP Cluster(s). The URL is the same as the one used to admin Okta, without the -admin in the first term and should look something like this:\nhttps://dev-34278011.okta.com/ Create a new Group Sync instance in the group-sync-operator namespace\nUsing the example below, customize the YAML to match the group names and save the configuration\nSample YAML:\napiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: okta-sync spec: schedule: \"*/1 * * * *\" providers: - name: okta okta: credentialsSecret: name: okta-api-token namespace: group-sync-operator groups: - ocp-admins - ocp-restricted-users prune: true url: \"\u003cOkta URL here\u003e\" appId: \u003cOkta AppID here\u003e Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after.\nThe schedule setting of schedule: \"* * * * *\" would result in synchronization occuring every minute. It also supports the cron “slash” notation (e.g., “*/5 * * * *”, which would synchronize every five minutes).\nTesting the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message\nCheck to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list\nValidate that all users specified in Okta also show up as members of the associated group in ROSA/OSD\nAdd a new user in Okta and assign it to the admin group\nVerify that the user now appears in ROSA/OSD (after the specified synchronization time)\nNow deactivate a user from the Okta admin group\nVerify the user has been deleted from the ROSA/OSD admin group\nBinding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.\nAdditional Okta Config Options There are also other options that are provider-specific that are covered in the operator documentation that should be kept in mind: Pruning groups that cease to exist on Okta A numeric limit on the number of groups to sync A list of groups against which to match If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no “merge” functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., okta-ocp-admins or something like okta-contoso-ocp-admins in the case of multiple Okta providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed. ","description":"","tags":null,"title":"Using Group Sync Operator with Okta and ROSA/OSD","uri":"/docs/idp/okta-grp-sync/"},{"content":"Retrieve the login command If you are not logged in via the CLI, access your cluster via the web console, then click on the dropdown arrow next to your name in the top-right and select Copy Login Command.\nA new tab will open and select the authentication method you are using (in our case it’s github)\nClick Display Token\nCopy the command under where it says “Log in with this token”. Then go to your terminal and paste that command and press enter. You will see a similar confirmation message if you successfully logged in.\noc login --token=RYhFlXXXXXXXXXXXX --server=https://api.osd4-demo.abc1.p1.openshiftapps.com:6443 Logged into \"https://api.osd4-demo.abc1.p1.openshiftapps.com:6443\" as \"openshiftuser\" using the token provided. You don't have any projects. You can try to create a new project, by running oc new-project \u003cprojectname\u003e Create new project Create a new project called “notebook-demo” in your cluster by entering the following command:\noc new-project notebook-demo You should receive the following response\nNow using project \"notebook-demo\" on server \"https://api.aro.openshiftdemo.dev:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Equivalently you can also create this new project using the web console UI by clicking on “Projects” under “Home” on the left menu, and then click “Create Project” button on the right.\nImporting the Minimal Notebook A pre-built version of the minimal notebook which is based on CentOS, can be found at on quay.io at:\nhttps://quay.io/organization/jupyteronopenshift The name of the latest build version of this image is:\nquay.io/jupyteronopenshift/s2i-minimal-notebook-py36:latest Although this image could be imported into an OpenShift cluster using oc import-image, it is recommended instead that you load it using the supplied image stream definition, using:\noc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/image-streams/s2i-minimal-notebook.json This is preferred, as it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn’t change to a newer version of the image which you haven’t tested.\nOnce the image stream definition is loaded, the project it is loaded into should have the tagged image:\ns2i-minimal-notebook:3.6 Deploying the Minimal Notebook To deploy the minimal notebook image run the following commands:\noc new-app s2i-minimal-notebook:3.6 --name minimal-notebook \\ --env JUPYTER_NOTEBOOK_PASSWORD=mypassword The JUPYTER_NOTEBOOK_PASSWORD environment variable will allow you to access the notebook instance with a known password.\nDeployment should be quick if you build the minimal notebook from source code. If you used the image stream, the first deployment may be slow as the image will need to be pulled down from quay.io. You can monitor progress of the deployment if necessary by running:\noc rollout status dc/minimal-notebook Because the notebook instance is not exposed to the public network by default, you will need to expose it. To do this, and ensure that access is over a secure connection run:\noc create route edge minimal-notebook --service minimal-notebook \\ --insecure-policy Redirect To see the hostname which is assigned to the notebook instance, run:\noc get route/minimal-notebook Access the hostname shown using your browser and enter the password you used above.\nTo delete the notebook instance when done, run:\noc delete all --selector app=minimal-notebook Creating Custom Notebook Images To create custom notebooks images, you can use the s2i-minimal-notebook:3.6 image as an S2I builder. This repository contains two examples for extending the minimal notebook. These can be found in:\nscipy-notebook tensorflow-notebook These are intended to mimic the images of the same name available from the Jupyter project.\nIn the directories you will find a requirements.txt file listing the additional Python packages that need to be installed from PyPi. You will also find a .s2i/bin/assemble script which will be triggered by the S2I build process, and which installs further packages and extensions.\nTo use the S2I build process to create a custom image, you can then run the command:\noc new-build --name custom-notebook \\ --image-stream s2i-minimal-notebook:3.6 \\ --code https://github.com/jupyter-on-openshift/jupyter-notebooks \\ --context-dir scipy-notebook If any build of a custom image fails because the default memory limit on builds in your OpenShift cluster is too small, you can increase the limit by running:\noc patch bc/custom-notebook \\ --patch '{\"spec\":{\"resources\":{\"limits\":{\"memory\":\"1Gi\"}}}}' and start a new build by running:\noc start-build bc/custom-notebook If using the custom notebook image with JupyterHub running in OpenShift, you may also need to set the image lookup policy on the image stream created.\noc set image-lookup is/custom-notebook This is necessary so that the image stream reference in the pod definition created by JupyterHub will be able to resolve the name to that of the image stream.\nFor the scipy-notebook and tensorflow-notebook examples provided, if you wish to use the images, instead of running the above commands, after you have loaded the image stream for, or built the minimal notebook image, you can instead run the commands:\noc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-scipy-notebook.json oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-tensorflow-notebook.json When creating a custom notebook image, the directory in the Git repository the S2I build is run against can contain a requirements.txt file listing the Python package to be installed in the custom notebook image. Any other files in the directory will also be copied into the image. When the notebook instance is started from the image, those files will then be present in your workspace.\n","description":"","tags":["GPU","OCP"],"title":"How to deploy Jupyter Notebook","uri":"/docs/misc/jup/buildnotebook/"},{"content":"The Open Data Hub operator is available for deployment in the OpenShift OperatorHub as a Community Operators. You can install it from the OpenShift web console:\nFrom the OpenShift web console, log in as a user with cluster-admin privileges. For a developer installation from try.openshift.com including AWS and CRC, the kubeadmin user will work.\nCreate a new project named ‘jph-demo’ for your installation of Open Data Hub Find Open Data Hub in the OperatorHub catalog.\nSelect the new namespace if not already selected. Under Operators, select OperatorHub for a list of operators available for deployment. Filter for Open Data Hub or look under Big Data for the icon for Open Data Hub. Click the Install button and follow the installation instructions to install the Open Data Hub operator.(optional if operator not installed)\nThe subscription creation view will offer a few options including Update Channel, keep the rolling channel selected.\nTo view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under Operators -\u003e Installed Operators (inside the project you created earlier). Once the STATUS field displays InstallSucceeded, you can proceed to create a new Open Data Hub deployment.\nFind the Open Data Hub Operator under Installed Operators (inside the project you created earlier)\nClick on the Open Data Hub Operator to bring up the details for the version that is currently installed.\nClick Create Instance to create a new deployment.\nSelect the YAML View radio button to be presented with a YAML file to customize your deployment. Most of the components available in ODH have been removed, and only components for JupyterHub are required for this example. apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: creationTimestamp: '2022-06-24T18:55:12Z' finalizers: - kfdef-finalizer.kfdef.apps.kubeflow.org generation: 2 managedFields: - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:spec': .: {} 'f:applications': {} 'f:repos': {} manager: Mozilla operation: Update time: '2022-06-24T18:55:12Z' - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:finalizers': .: {} 'v:\"kfdef-finalizer.kfdef.apps.kubeflow.org\"': {} 'f:status': {} manager: opendatahub-operator operation: Update time: '2022-06-24T18:55:12Z' name: opendatahub namespace: jph-demo resourceVersion: '27393048' uid: f54399a6-faa7-4724-bf3d-be04a63d3120 spec: applications: - kustomizeConfig: repoRef: name: manifests path: odh-common name: odh-common - kustomizeConfig: parameters: - name: s3_endpoint_url value: s3.odh.com repoRef: name: manifests path: jupyterhub/jupyterhub name: jupyterhub - kustomizeConfig: overlays: - additional repoRef: name: manifests path: jupyterhub/notebook-images name: notebook-images repos: - name: kf-manifests uri: \u003e- https://github.com/opendatahub-io/manifests/tarball/v1.4.0-rc.2-openshift - name: manifests uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.2' status: {} Update the spec of the resource to match the above and click Create. If you accepted the default name, this will trigger the creation of an Open Data Hub deployment named opendatahub with JupyterHub.\nVerify the installation by viewing the project workload. JupyterHub and traefik-proxy should be running. Click Routes under Networking and url to launch Jupyterhub is created Open JupyterHub on web browser Configure GPU and start server Check for GPU in notebook Reference: Check the blog on Using the NVIDIA GPU Operator to Run Distributed TensorFlow 2.4 GPU Benchmarks in OpenShift 4 ","description":"","tags":["GPU","OCP"],"title":"Installing the Open Data Hub Operator","uri":"/docs/misc/jup/opendatahub-gpu/"},{"content":"You will need the following prerequistes in order to run a basic Jupyter notebook with GPU on OpenShift\n1. A OpenShift Cluster This will assume you have already provisioned a OpenShift cluster succesfully and are able to use it.\nYou will need to log in as cluster admin to deploy GPU Operator .\n2. OpenShift Command Line Interface Please see the OpenShift Command Line section for more information on installing.\nThe following guides through a step by step procedure in deploying Jupyter Notebook in OpenShift.\n3. Reference images You’ll be doing the majority of the labs using the OpenShift CLI, but you can also accomplish them using the OpenShift web console. You can create a minimal Jupyter notebook image using the Source-to-Image (S2I) build process. The image can be built in OpenShift, separately using the s2i tool, or using a docker build. One can deploy a custom notebook image. The Jupyter Project provides a number of images for notebooks on Docker Hub. These are:\nbase-notebook r-notebook minimal-notebook scipy-notebook tensorflow-notebook datascience-notebook pyspark-notebook all-spark-notebook The GitHub repository used to create these is:\nhttps://github.com/jupyter/docker-stacks Basic Concepts Source-To-Image (S2I) Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments.\nS2I Builds Creating Images How it works Start a container from the builder image with the application source injected into a known directory\nThe container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn’t change to a newer version of the image which you haven’t tested.\nGoals and benefits 1. Reproducibility Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes.\n2. Flexibility Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling.\n3. Speed Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image.\n4. Security Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time.\nRoutes An OpenShift Route exposes a service at a host name, like www.example.com , so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking “what’s the difference?”. Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below.\nNOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router.\nAlso of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details.\nImageStreams An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.\nWhat are the benefits? Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it’s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don’t really have to deal with the registry!\nYou can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference.\nSee below for more details:\nImage Streams Blog post OpenShift Docs - Understanding containers, images, and image streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process.\nOpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry.\nBuild objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time.\nSee Understanding image builds for more details.\n","description":"","tags":["GPU","OCP"],"title":"Jupyter Notebooks","uri":"/docs/misc/jup/"},{"content":"","description":"","tags":null,"title":"OCP","uri":"/tags/ocp/"},{"content":"Kevin Collins\n06/28/2022\nOne of the advantages of using OpenShift is the internal registry that comes with OpenShfit to build, deploy and manage container images locally. By default, access to the registry is limited to the cluster ( by design ) but can be extended to usage outside of the cluster. This guide will go through the steps required to access the OpenShift Registry on an ARO cluster outside of the cluster.\nPrerequisites an ARO Cluster oc cli podman or docker cli Expose the Registry Expose the registry service\noc create route reencrypt --service=image-registry -n openshift-image-registry Annotate the route\noc annotate route image-registry haproxy.router.openshift.io/balance=source -n openshift-image-registry Get the route host name\nHOST=$(oc get route image-registry -n openshift-image-registry --template='{{ .spec.host }}') Log into the image registry\npodman login -u $(oc whoami) -p $(oc whoami -t) $HOST Test it out podman pull openshift/hello-openshift podman images expected output\nopenshift/hello-openshift latest 7af3297a3fb4 4 years ago 6.09MB ","description":"","tags":["ARO","Azure"],"title":"Accessing the Internal Registry from ARO","uri":"/docs/aro/registry/"},{"content":"Kevin Collins\n06/28/2022\nNote: This guide demonstrates how to setup and configure self-managed OpenShift Data Foundation in Internal Mode on an ARO Cluster and test it out.\nPrerequisites An Azure Red Hat OpenShift cluster ( verion 4.10+ ) kubectl cli oc cli moreutils (sponge) jq Install compute nodes for ODF A best practice for optimal performance is to run ODF on dedicated nodes with a minimum of one per zone. In this guide, we will be provisioning 3 additional compute nodes, one per zone. Run the following script to create the additional nodes:\nLog into your ARO Cluster\nexport AZ_RG=\u003crg-name\u003e export AZ_ARO=\u003ccluster-name\u003e az aro list-credentials --name \"${AZ_ARO}\" --resource-group \"${AZ_RG}\" az aro show --name \"${AZ_ARO}\" --resource-group \"${AZ_RG}\" -o tsv --query consoleProfile API_SERVER=$(az aro show -g \"${AZ_RG}\" -n \"${AZ_ARO}\" --query apiserverProfile.url -o tsv) KUBE_ADM_USER=$(az aro list-credentials --name \"${AZ_ARO}\" --resource-group \"${AZ_RG}\" -o json | jq -r '.kubeadminUsername') KUBE_ADM_PASS=$(az aro list-credentials --name \"${AZ_ARO}\" --resource-group \"${AZ_RG}\" -o json | jq -r '.kubeadminPassword') oc login -u $KUBE_ADM_USER -p $KUBE_ADM_PASS $API_SERVER Create the new compute nodes\nfor ZONE in 1 2 3 do item=$((ZONE-1)) MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath=\"{.items[$item]}\" | jq -r '[.metadata.name] | @tsv') oc get machineset -n openshift-machine-api $MACHINESET -o json \u003e default_machineset$ZONE.json worker=odf-worker-$ZONE jq \".metadata.name = \\\"$worker\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq '.spec.replicas = 1' default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.selector.matchLabels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.template.metadata.labels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_D16s_v3\"' default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq \".spec.template.spec.providerSpec.value.zone = \\\"$ZONE\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq 'del(.status)' default_machineset$ZONE.json | sponge default_machineset$ZONE.json oc create -f default_machineset$ZONE.json done wait for compute node to be up and running It takes just a couple of minutes for new nodes to provision\nwhile [[ $(oc get machinesets.machine.openshift.io -n openshift-machine-api | grep odf-worker-1 | awk '{ print $5 }') -ne 1 ]] do echo \"Waiting for worker machines to be ready...\" sleep 5 done Label new compute nodes\nCheck if the nodes are ready:\noc get nodes | grep odf-worker expected output:\nodf-worker-1-jg7db Ready worker 10m v1.23.5+3afdacb odf-worker-2-ktvct Ready worker 10m v1.23.5+3afdacb odf-worker-3-rk22b Ready worker 10m v1.23.5+3afdacb Once you see the three nodes, the next step we need to do is label and taint the nodes. This will ensure the OpenShift Data Foundation is installed on these nodes, and no other workload will be placed on the nodes.\nfor worker in $(oc get nodes | grep odf-worker | awk '{print $1}') do oc label node $worker cluster.ocs.openshift.io/openshift-storage=`` oc adm taint nodes $worker node.ocs.openshift.io/storage=true:NoSchedule done Check nodes labels. The following command should list all three odf storage node\noc get node --show-labels | grep storage | awk '{print $1}' Deploy OpenShift Data Foundation Next, we will install OpenShift Data Foundation via an Operator.\nCreate the openshift-storage namespace cat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: \"true\" name: openshift-storage spec: {} EOF Create the Operator Group for openshift-storage cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-storage-operatorgroup namespace: openshift-storage spec: targetNamespaces: - openshift-storage EOF Subscribe to the ocs-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ocs-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # \u003c-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: ocs-operator source: redhat-operators # \u003c-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Subscribe to the odf-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: odf-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # \u003c-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: odf-operator source: redhat-operators # \u003c-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Create a Storage Cluster cat \u003c\u003cEOF | oc apply -f - apiVersion: ocs.openshift.io/v1 kind: StorageCluster metadata: annotations: uninstall.ocs.openshift.io/cleanup-policy: delete uninstall.ocs.openshift.io/mode: graceful generation: 2 name: ocs-storagecluster namespace: openshift-storage spec: storageDeviceSets: - config: {} count: 1 dataPVCTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Ti storageClassName: managed-premium volumeMode: Block name: ocs-deviceset-managed-premium portable: true replica: 3 version: 4.10.0 EOF Validate the install List the cluster service version for the ODF operators\noc get csv -n openshift-storage verify that the operators below have succeeded.\nNAME DISPLAY VERSION REPLACES PHASE mcg-operator.v4.10.4 NooBaa Operator 4.10.4 Succeeded ocs-operator.v4.10.4 OpenShift Container Storage 4.10.4 Succeeded odf-operator.v4.10.4 OpenShift Data Foundation 4.10.4 Succeeded Check that Storage cluster is ready\nwhile [[ $(oc get storageclusters.ocs.openshift.io -n openshift-storage | grep ocs-storagecluster | awk '{ print $3 }') != \"Ready\" ]] do echo \"storage cluster status is $(oc get storageclusters.ocs.openshift.io -n openshift-storage | grep ocs-storagecluster | awk '{ print $3 }')\" echo \"wait for storage cluster to be ready\" sleep 10 done Check that the ocs storage classes have been created\nnote: this can take around 5 minutes\noc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-csi disk.csi.azure.com Delete WaitForFirstConsumer true 118m managed-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 119m ocs-storagecluster-ceph-rbd openshift-storage.rbd.csi.ceph.com Delete Immediate true 7s ocs-storagecluster-cephfs openshift-storage.cephfs.csi.ceph.com Delete Immediate true 7s Test it out To test out ODF, we will create ‘writer’ pods on each node across all zones and then a reader pod to read the data that is written. This will prove both regional storage along with “read write many” mode is working correctly.\nCreate a new project\noc new-project odf-demo Create a RWX Persistent Volume Claim for ODF\ncat \u003c\u003cEOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 400Gi storageClassName: ocs-storagecluster-cephfs EOF Check PVC and PV status. It should be “Bound”\noc get pvc oc get pv Create writer pods via a DaemonSet Using a deamonset will ensure that we have a ‘writer pod’ on each worker node and will also prove that we correctly set a taint on the ‘ODF Workers’ where which we do not want workload to be added to.\nThe writer pods will write out which worker node the pod is running on, the data, and a hello message.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-odf labels: app: test-odf spec: selector: matchLabels: name: test-odf template: metadata: labels: name: test-odf spec: containers: - name: azodf image: centos:latest command: [\"sh\", \"-c\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do printenv --null NODE_NAME | tee -a /mnt/odf-data/verify-odf; echo ' says hello '$(date) | tee -a /mnt/odf-data/verify-odf; sleep 15; done;\", ] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Check the writer pods are running.\nnote: there should be 1 pod per non-ODF worker node\noc get pods expected output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-odf-47p2g 1/1 Running 0 107s 10.128.2.15 aro-kmobb-7zff2-worker-eastus1-xgksq \u003cnone\u003e \u003cnone\u003e test-odf-p5xk6 1/1 Running 0 107s 10.131.0.18 aro-kmobb-7zff2-worker-eastus3-h4gv7 \u003cnone\u003e \u003cnone\u003e test-odf-ss8b5 1/1 Running 0 107s 10.129.2.32 aro-kmobb-7zff2-worker-eastus2-sbfpm \u003cnone\u003e \u003cnone\u003e Create a reader pod The reader pod will simply log data written by the writer pods.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-odf-read spec: containers: - name: test-odf-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/odf-data/verify-odf\"] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Now let’s verify the POD is reading from shared volume.\noc logs test-odf-read Expected output\naro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 Notice that pods in different zones are writing to the PVC which is managed by ODF.\n","description":"","tags":["ARO","Azure"],"title":"Configure ARO with OpenShift Data Foundation","uri":"/docs/aro/odf/"},{"content":"ARO guide to running Nvidia GPU workloads.\nPrerequisites oc cli jq, moreutils, and gettext package ARO 4.10 If you need to install an ARO cluster, please read our ARO Quick start guide . Please be sure if you’re installing or using an existing ARO cluster that it is 4.10.x or higher.\nAs of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads.\nLinux:\nsudo dnf install jq moreutils gettext MacOS\nbrew install jq moreutils gettext Helm Prerequisites If you plan to use Helm to deploy the GPU operator, you will need do the following\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update GPU Quota All GPU quotas in Azure are 0 by default. You will need to login to the azure portal and request GPU quota. There is a lot of competition for GPU workers, so you may have to provision an ARO cluster in a region where you can actually reserve GPU. ARO supports the following GPU workers:\nNC4as T4 v3 NC6s v3 NC8as T4 v3 NC12s v3 NC16as T4 v3 NC24s v3 NC24rs v3 NC64as T4 v3 Please remember that when you request quota that Azure is per core. To request a single NC4as T4 v3 node, you will need to request quota in groups of 4. If you wish to request an NC16as T4 v3 you will need to request quota of 16.\nLogin to azure\nLogin to portal.azure.com , type “quotas” in search by, click on Compute and in the search box type “NCAsv3_T4”. Select the region your cluster is in (select checkbox) and then click Request quota increase and ask for quota (I chose 8 so i can build two demo clusters of NC4as T4s).\nConfigure quota\nLog in to your ARO cluster Login to OpenShift - we’ll use the kubeadmin account here but you can login with your user account as long as you have cluster-admin.\noc login \u003capiserver\u003e -u kubeadmin -p \u003ckubeadminpass\u003e Pull secret (Conditional) We’ll update our pull secret to make sure that we can install operators as well as connect to cloud.redhat.com.\nIf you have already re-created a full pull secret with cloud.redhat.com enabled you can skip this step\nUsing Helm Before Deploying the chart you need it to adopt the existing pull secret\nkubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config kubectl -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm Download your new pull secret from https://console.redhat.com/openshift/downloads -\u003e Tokens -\u003e Pull secret and use it to update create the pull secret in your cluster.\nUpdate the pull secret\nThis chart will merge the in-cluster pull secret with the new pull secret.\nhelm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --set-file pullSecret=$HOME/Downloads/pull-secret.txt Enable Operator Hub\noc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Skip to GPU Machine Set Manually Log into cloud.redhat.com Browse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and save it as pull-secret.txt\nThe following steps will need to be ran in the same working directory as your pull-secret.txt\nExport existing pull secret\noc get secret pull-secret -n openshift-config -o json | jq -r '.data.\".dockerconfigjson\"' | base64 --decode \u003e export-pull.json Merge downloaded pull secret with system pull secret to add cloud.redhat.com\njq -s '.[0] * .[1]' export-pull.json pull-secret.txt | tr -d \"\\n\\r\" \u003e new-pull-secret.json Upload new secret file\noc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=new-pull-secret.json You may need to wait for about ~1hr for everything to sync up with cloud.redhat.com.\nDelete secrets\nrm pull-secret.txt export-pull.json new-pull-secret.json GPU Machine Set ARO still uses Kubernetes Machinsets to create a machine set. I’m going to export the first machine set in my cluster (az 1) and use that as a template to build a single GPU machine in southcentralus region 1.\nHelm Create a new machine-set (replicas of 1), see the Chart’s values file for configuration options\nhelm upgrade --install -n openshift-machine-api \\ gpu mobb/aro-gpu Switch to the proper namespace (project):\noc project openshift-machine-api Wait for the new GPU nodes to be available\nwatch oc get machines Skip to Install Nvidia GPU Operator Manually View existing machine sets\nFor ease of set up, I’m going to grab the first machine set and use that as the one I will clone to create our GPU machine set.\nMACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath='{.items[0]}' | jq -r '[.metadata.name] | @tsv') Save a copy of example machine set\noc get machineset -n openshift-machine-api $MACHINESET -o json \u003e gpu_machineset.json Change the .metadata.name field to a new unique name\nI’m going to create a unique name for this single node machine set that shows nvidia-worker- that follows a similar pattern as all the other machine sets.\njq '.metadata.name = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Ensure spec.replicas matches the desired replica count for the MachineSet\njq '.spec.replicas = 1' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.selector.matchLabels.machine.openshift.io/cluster-api-machineset field to match the .metadata.name field\njq '.spec.selector.matchLabels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.template.metadata.labels.machine.openshift.io/cluster-api-machineset to match the .metadata.name field\njq '.spec.template.metadata.labels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.vmSize to match the desired GPU instance type from Azure.\nThe machine we’re using is Standard_NC4as_T4_v3.\njq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_NC4as_T4_v3\"' gpu_machineset.json | sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.zone to match the desired zone from Azure\njq '.spec.template.spec.providerSpec.value.zone = \"1\"' gpu_machineset.json | sponge gpu_machineset.json Delete the .status section of the yaml file\njq 'del(.status)' gpu_machineset.json | sponge gpu_machineset.json Verify the other data in the yaml file.\nCreate GPU machine set These steps will create the new GPU machine. It may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the azure portal and ensure you didn’t run across availability issues. You can go “Virtual Machines” and search for the worker name you created above to see the status of VMs.\nCreate GPU Machine set\noc create -f gpu_machineset.json This command will take a few minutes to complete.\nVerify GPU machine set\nMachines should be getting deployed. You can view the status of the machine set with the following commands\noc get machineset -n openshift-machine-api oc get machine -n openshift-machine-api Once the machines are provisioned, which could take 5-15 minutes, machines will show as nodes in the node list.\noc get nodes You should see a node with the “nvidia-worker-southcentralus1” name it we created earlier.\nInstall Nvidia GPU Operator This will create the nvidia-gpu-operator name space, set up the operator group and install the Nvidia GPU Operator.\nHelm Create namespaces\noc create namespace openshift-nfd oc create namespace nvidia-gpu-operator Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \\ mobb/operatorhub --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml Wait until the two operators are running\nwatch kubectl get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-7b66c67bd9-rk98w 2/2 Running 0 47s watch oc get pods -n nvidia-gpu-operator NAME READY STATUS RESTARTS AGE gpu-operator-5d8cb7dd5f-c4ljk 1/1 Running 0 87s Install the Nvidia GPU Operator chart\nhelm upgrade --install -n nvidia-gpu-operator nvidia-gpu \\ mobb/nvidia-gpu --disable-openapi-validation Skip to Validate GPU Manually Create Nvidia namespace\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Create Operator Group\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Get latest nvidia channel\nCHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}') If your cluster was created without providing the pull secret, the cluster won’t include samples or operators from Red Hat or from certified partners. This will result in the following error message:\nError from server (NotFound): packagemanifests.packages.operators.coreos.com “gpu-operator-certified” not found.\nTo add your Red Hat pull secret on an Azure Red Hat OpenShift cluster, follow this guidance .\nGet latest nvidia package\nPACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"'$CHANNEL'\") | .currentCSV') Create Subscription\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"$CHANNEL\" installPlanApproval: Automatic name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"$PACKAGE\" EOF Wait for Operator to finish installing\nDon’t proceed until you have verified that the operator has finished installing. It’s also a good point to ensure that your GPU worker is online.\nInstall Node Feature Discovery Operator The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads. We’ll install the NFD operator into the opneshift-ndf namespace and create the “subscription” which is the configuration for NFD.\nOfficial Documentation for Installing Node Feature Discovery Operator Set up Name Space\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-nfd EOF Create OperatorGroup\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd EOF Create Subscription\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait for Node Feature discovery to complete installation\nYou can login to your openshift console and view operators or simply wait a few minutes. The next step will error until the operator has finished installing.\nCreate NFD Instance\ncat \u003c\u003cEOF | oc apply -f - kind: NodeFeatureDiscovery apiVersion: nfd.openshift.io/v1 metadata: name: nfd-instance namespace: openshift-nfd spec: customConfig: configData: | # - name: \"more.kernel.features\" # matchOn: # - loadedKMod: [\"example_kmod3\"] # - name: \"more.features.by.nodename\" # value: customValue # matchOn: # - nodename: [\"special-.*-node-.*\"] operand: image: \u003e- registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b servicePort: 12000 workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time ## configurable and require a nfd-worker restart to take effect ## after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: # cpu: # cpuid: ## NOTE: whitelist has priority over blacklist # attributeBlacklist: # - \"BMI1\" # - \"BMI2\" # - \"CLMUL\" # - \"CMOV\" # - \"CX16\" # - \"ERMS\" # - \"F16C\" # - \"HTT\" # - \"LZCNT\" # - \"MMX\" # - \"MMXEXT\" # - \"NX\" # - \"POPCNT\" # - \"RDRAND\" # - \"RDSEED\" # - \"RDTSCP\" # - \"SGX\" # - \"SSE\" # - \"SSE2\" # - \"SSE3\" # - \"SSE4.1\" # - \"SSE4.2\" # - \"SSSE3\" # attributeWhitelist: # kernel: # kconfigFile: \"/path/to/kconfig\" # configOpts: # - \"NO_HZ\" # - \"X86\" # - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: # - \"class\" - \"vendor\" # - \"device\" # - \"subsystem_vendor\" # - \"subsystem_device\" # usb: # deviceClassWhitelist: # - \"0e\" # - \"ef\" # - \"fe\" # - \"ff\" # deviceLabelFields: # - \"class\" # - \"vendor\" # - \"device\" # custom: # - name: \"my.kernel.feature\" # matchOn: # - loadedKMod: [\"example_kmod1\", \"example_kmod2\"] # - name: \"my.pci.feature\" # matchOn: # - pciId: # class: [\"0200\"] # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # - pciId : # vendor: [\"8086\"] # device: [\"1000\", \"1100\"] # - name: \"my.usb.feature\" # matchOn: # - usbId: # class: [\"ff\"] # vendor: [\"03e7\"] # device: [\"2485\"] # - usbId: # class: [\"fe\"] # vendor: [\"1a6e\"] # device: [\"089a\"] # - name: \"my.combined.feature\" # matchOn: # - pciId: # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"] EOF Verify NFD is ready.\nThis operator should say Available in the status\nApply nVidia Cluster Config We’ll now apply the nvidia cluster config. Please read the nvidia documentation on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete.\nApply cluster config\ncat \u003c\u003cEOF | oc apply -f - apiVersion: nvidia.com/v1 kind: ClusterPolicy metadata: name: gpu-cluster-policy spec: migManager: enabled: true operator: defaultRuntime: crio initContainer: {} runtimeClass: nvidia deployGFD: true dcgm: enabled: true gfd: {} dcgmExporter: config: name: '' driver: licensingConfig: nlsEnabled: false configMapName: '' certConfig: name: '' kernelModuleConfig: name: '' repoConfig: configMapName: '' virtualTopology: config: '' enabled: true use_ocp_driver_toolkit: true devicePlugin: {} mig: strategy: single validator: plugin: env: - name: WITH_WORKLOAD value: 'true' nodeStatusExporter: enabled: true daemonsets: {} toolkit: enabled: true EOF Verify Cluster Policy\nLogin to OpenShift console and browse to operators and make sure you’re in nvidia-gpu-operator namespace. You should see it say State: Ready once everything is complete.\nValidate GPU It may take some time for the nVidia Operator and NFD to completely install and self-identify the machines. These commands can be ran to help validate that everything is running as expected.\nVerify NFD can see your GPU(s)\noc describe node | egrep 'Roles|pci-10de' | grep -v master You should see output like:\nRoles: worker feature.node.kubernetes.io/pci-10de.present=true Verify node labels\nYou can see the node labels by logging into the OpenShift console -\u003e Compute -\u003e Nodes -\u003e nvidia-worker-southcentralus1-. You should see a bunch of nvidia GPU labels and the pci-10de device from above.\nNvidia SMI tool verification\noc project nvidia-gpu-operator for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\\n' ; done You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type)\nCreate Pod to run a GPU workload\noc project nvidia-gpu-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"quay.io/giantswarm/nvidia-gpu-demo:latest\" resources: limits: nvidia.com/gpu: 1 nodeSelector: nvidia.com/gpu.present: true EOF View logs\noc logs cuda-vector-add --tail=-1 Please note, if you get an error “Error from server (BadRequest): container “cuda-vector-add” in pod “cuda-vector-add” is waiting to start: ContainerCreating”, try running “oc delete pod cuda-vector-add” and then re-run the create statement above. I’ve seen issues where if this step is ran before all of the operator consolidation is done it may just sit there.\nYou should see Output like the following (mary vary depending on GPU):\n[Vector addition of 5000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done If successful, the pod can be deleted\noc delete pod cuda-vector-add ","description":"","tags":["ARO","Azure","GPU"],"title":"ARO with Nvidia GPU Workloads","uri":"/docs/aro/gpu/"},{"content":"OSD and ROSA supports custom domain operator to serve application custom domain, which provisions openshift ingress controller and cloud load balancers. However, when a route with custom domain is created, both default router and custom domain router serve routes. This article describes how to use route labels to stop default router from serving custom domain routes.\nPrerequisites Rosa or OSD Cluster Custom Domain Deployed Problem Demo Deploy A Custom Domain oc create secret tls example-tls --cert=[cert_file] --key=[key_file] cat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: example spec: domain: example.com scope: External certificate: name: example-tls namespace: default EOF Create a sample application and Route oc new-app --image=openshift/hello-openshift cat \u003c\u003cEOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Both default router and custom router serve the routes oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-default.apps.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: default wildcardPolicy: None - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None End user can access the app from both ingress controllers’ cloud load balancer oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 Hello OpenShift! shading@shading-mac gcp_domain % curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift! Stop the default router from serving custom domain routes Delete the route oc delete route helloworld Custom Domain only serve routes with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/example \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchLabels\": {\"domain\": \"example.com\"}}}}' Exclude default router with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/default \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchExpressions\":[{\"key\":\"domain\",\"operator\":\"NotIn\",\"values\":[\"example.com\"]}]}}}' Create route with custom domain label cat \u003c\u003cEOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift domain: example.com app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Only Custom Domain router route the traffic oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 .... The application is currently not serving requests at this endpoint. It may not have been started or is still starting. ... curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift! ","description":"","tags":["OSD","ROSA"],"title":"Stop default router from serving custom domain routes","uri":"/docs/misc/default-router-custom-domain/"},{"content":"ARO guide to deploying an ARO cluster with custom domain and automating certificate management with cert-manager and letsencrypt certificates to manage the *.apps and api endpoints.\nPrerequisites az cli (already installed in Azure Cloud Shell) oc cli jq (already installed in Azure Cloud Shell) OpenShift 4.10+ domain name to use (we will create zones for this domain name during this guide) I’m going to be running this setup through Bash on the Azure Cloud Shell. Be sure to always use the same terminal/session for all commands since we’ll reference environment variables set or created through the steps.\nAzure Cloud Shell - Bash\nSee Azure Docs for alternative install options.\nInstall oc CLI\nFollow the instructions in Installing the OpenShift CLI on Linux to install oc CLI.\nPrepare Azure Account for Azure OpenShift Register resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click Copy pull secret\nBack in the Azure Cloud Shell, enter the following. Instead of \u003cpaste_your_secret\u003e, paste your actual secret on that line. To do so, right-click and choose paste.\ncat \u003e\u003e ./pull-secret.txt \u003c\u003c EOF \u003cpaste_your_secret\u003e EOF Deploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment\nPULL_SECRET=./pull-secret.txt # the path to pull-secret LOCATION=southcentralus # the location of your cluster RESOURCEGROUP=aro-rg # the name of the resource group where you want to create your cluster CLUSTER=aro-cluster # the name of your cluster DOMAIN=lab.domain.com # Domain or subdomain zone for cluster \u0026 cluster api Create an Azure resource group\naz group create \\ --name $RESOURCEGROUP \\ --location $LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 10.0.0.0/22 Create control plane subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @$PULL_SECRET \\ --domain $DOMAIN Wait until the ARO cluster is fully provisioned.\nCreate DNS Zones \u0026 Service Principal In order for cert-manager to work with AzureDNS, we need to create the zone and add a CAA record as well as create a Service Principal that we can use to manage records in this zone so CertManager can use DNS01 authentication for validating requests.\nThis zone should be a public zone since letsencrypt will need to be able to read records created here.\nIf you use a subdomain, please be sure to create the NS records in your primary domain to the subdomain.\nFor ease of management, we’re using the same resource group for domain as we have the cluster in.\nCreate Zone\naz network dns zone create -g $RESOURCEGROUP -n $DOMAIN You will need to configure your nameservers to point to Azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar.\nCreate API DNS record\nAPIREC=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.ip -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n api -a $APIREC Create Wildcard DNS record\nWILDCARDREC=$(az aro show -n $CLUSTER -g $RESOURCEGROUP --query '{ingress:ingressProfiles[0].ip}' -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n \"*.apps\" -a $WILDCARDREC Add CAA Record\nCAA is a type of DNS record that allows owners to specify which Certificate Authorities are allowed to issue certificates containing their domain names.\naz network dns record-set caa add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n @ --flags 0 --tag \"issuewild\" --value \"letsencrypt.org\" You should be able to view the records in your console\nNote - You may have to create NS records in your root zone for a subdomain if you use a subdomain zone to point to the subdomains name servers.\nSet environment variables to build new service principal and credentials to allow cert-manager to create records in this zone.\nAZURE_CERT_MANAGER_NEW_SP_NAME = the name of the service principal to create that will manage the DNS zone automation for cert-manager.\nAZURE_CERT_MANAGER_NEW_SP_NAME=aro-dns-sp LETSENCRYPTEMAIL=youremail@work.com DNS_SP=$(az ad sp create-for-rbac --name $AZURE_CERT_MANAGER_NEW_SP_NAME --output json) AZURE_CERT_MANAGER_SP_APP_ID=$(echo $DNS_SP | jq -r '.appId') AZURE_CERT_MANAGER_SP_PASSWORD=$(echo $DNS_SP | jq -r '.password') AZURE_TENANT_ID=$(echo $DNS_SP | jq -r '.tenant') AZURE_SUBSCRIPTION_ID=$(az account show --output json | jq -r '.id') Restrict service principal - remove contributor role if it exists.\nNote: If you get the error message No matched assignments were found to delete, that’s fine, and it is safe to proceed. We’re going to grant the DNS Management Role to it next.\naz role assignment delete --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role Contributor Grant DNS Zone Contributor to our Service Principal\nWe’ll grant DNS Zone Contributor to our DNS Service principal.\naz role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role befefa01-2a29-4197-83a8-272ff33ce314 --scope /subscriptions/$AZURE_SUBSCRIPTION_ID Assign service principal to DNS zone\nDNS_ID=$(az network dns zone show --name $DOMAIN --resource-group $RESOURCEGROUP --query \"id\" --output tsv) az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role \"DNS Zone Contributor\" --scope $DNS_ID Get OpenShift console URL\naz aro show -g $RESOURCEGROUP -n $CLUSTER --query \"consoleProfile.url\" -o tsv Get OpenShift API URL\naz aro show -g $RESOURCEGROUP -n $CLUSTER --query \"apiserverProfile.url\" -o tsv Get OpenShift credentials\nYou can use these to log in to the web console (will get a cert warning that you can bypass with typing “thisisunsafe” in a chrome browser or login with oc)\naz aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP Log In to Cluster Log in to your cluster through oc cli\nYou may need to flush your local dns resolver/cache before you can see the DNS/Hostnames. On Windows you can open up a command prompt as administrator and type “ipconfig /flushdns”\napiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) loginCred=$(az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP --query \"kubeadminPassword\" -o tsv) oc login $apiServer -u kubeadmin -p $loginCred You may get a warning that the certificate isn’t trusted. We can ignore that now since we’re in the process of adding a trusted certificate.\nSet up Cert-Manager We’ll install cert-manager from operatorhub. If you experience any issues installing here, it probably means that you didn’t provide a pull-secret when you installed your ARO cluster.\nCreate namespace\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: annotations: openshift.io/display-name: Red Hat Certificate Manager Operator labels: openshift.io/cluster-monitoring: 'true' name: openshift-cert-manager-operator EOF Switch openshift-cert-manager-operator project (namespace)\noc project openshift-cert-manager-operator Create OperatorGroup\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-cert-manager-operator spec: {} EOF Create subscription for cert-manager operator\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-cert-manager-operator namespace: openshift-cert-manager-operator spec: channel: tech-preview installPlanApproval: Automatic name: openshift-cert-manager-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete it’s set up. May be a good time to take a coffee break :)\nWait for cert-manager operator to finish installing.\nOur next steps can’t complete until the operator has finished installing. I recommend that you log in to your cluster console with the URL and credentials you captured after you ran the az aro create and view the installed operators to see that everything is complete and successful.\nConfigure cert-manager LetsEncrypt We’re going to set up cert-manager to use DNS verification for letsencrypt certificates. We’ll need to generate a service principal that can update the DNS zone and create short term records needed to validate certificate requests and associate this service principal with the cluster issuer.\nConfigure Certificate Requestor Switch openshift-cert-manager project (namespace)\noc project openshift-cert-manager Create azuredns-config secret for storing service principal credentials to manage zone.\noc create secret generic azuredns-config --from-literal=client-secret=$AZURE_CERT_MANAGER_SP_PASSWORD -n openshift-cert-manager Create Cluster Issuer\ncat \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPTEMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: azureDNS: clientID: $AZURE_CERT_MANAGER_SP_APP_ID clientSecretSecretRef: name: azuredns-config key: client-secret subscriptionID: $AZURE_SUBSCRIPTION_ID tenantID: $AZURE_TENANT_ID resourceGroupName: $RESOURCEGROUP hostedZoneName: $DOMAIN environment: AzurePublicCloud EOF Describe issuer\noc describe clusterissuer letsencrypt-prod You should see some output that the issuer is Registered/Ready\nConditions: Last Transition Time: 2022-06-17T17:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u003cnone\u003e Once the above command is complete, you should be able to log in to the OpenShift console, navigate to Installed Operators, click on the cert-manager Operator, make sure you’re in the “openshift-cert-manager-operator” project, and click the ClusterIssuer tab. You should see a screen like this. Again, if you have an ssl error and use a chrome browser - type “thisisunsafe” to get in if you get an error its an invalid cert.\nCreate \u0026 Install API Certificate Switch openshift-config project /\noc project openshift-config Configure API certificate\ncat \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-api namespace: openshift-config spec: secretName: openshift-api-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - api.$DOMAIN EOF View certificate status\noc describe certificate openshift-api -n openshift-config Create cluster api cert job\nThis job will install the certificate\ncat \u003c\u003cEOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-api-cert rules: - apiGroups: - \"\" resources: - secrets verbs: - get - list - apiGroups: - config.openshift.io resources: - apiservers verbs: - get - list - patch - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-api-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-api-cert subjects: - kind: ServiceAccount name: patch-cluster-api-cert namespace: openshift-config --- apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-api-cert --- apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-api-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest env: - name: API_HOST_NAME value: api.$DOMAIN command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-api-certificate -n openshift-config; then oc patch apiserver cluster --type=merge -p '{\"spec\":{\"servingCerts\": {\"namedCertificates\": [{\"names\": [\"'\\$API_HOST_NAME'\"], \"servingCertificate\": {\"name\": \"openshift-api-certificate\"}}]}}}' else echo \"Could not execute sync as secret 'openshift-api-certificate' in namespace 'openshift-config' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-api-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-api-cert serviceAccountName: patch-cluster-api-cert EOF Create \u0026 Install APPS Wildcard Certificate Switch openshift-ingress project (namespace)\noc project openshift-ingress Configure Wildcard Certificate\ncat \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-wildcard namespace: openshift-ingress spec: secretName: openshift-wildcard-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: \"*.apps.$DOMAIN\" dnsNames: - \"*.apps.$DOMAIN\" EOF This will generate our API and wildcard certificate requests. We’ll now create two jobs that will install these certificates.\nView certificate status\noc describe certificate openshift-wildcard -n openshift-ingress Install Wildcard Certificate Job\ncat \u003c\u003cEOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-wildcard-cert rules: - apiGroups: - operator.openshift.io resources: - ingresscontrollers verbs: - get - list - patch - apiGroups: - \"\" resources: - secrets verbs: - get - list --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-wildcard-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-wildcard-cert subjects: - kind: ServiceAccount name: patch-cluster-wildcard-cert namespace: openshift-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-wildcard-cert --- apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-wildcard-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-wildcard-certificate -n openshift-ingress; then oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{\"spec\": { \"defaultCertificate\": { \"name\": \"openshift-wildcard-certificate\" }}}' else echo \"Could not execute sync as secret 'openshift-wildcard-certificate' in namespace 'openshift-ingress' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-wildcard-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-wildcard-cert serviceAccountName: patch-cluster-wildcard-cert EOF Debugging One of the most helpful commands i’ve seen for debugging is in regards to challenges failing. The order says pending in perpetuity and you can run this to see why.\noc describe challenges This is a very helpful guide in debugging certificates as well.\nValidate Certificates It will take a few minutes for the jobs to successfully complete.\nOnce the certificate requests are complete, you should no longer see a browser security warning and you should have a valid SSL lock in your browser and no more warnings about SSL on cli.\nYou may want to open an InPrivate/Private browser tab to visit the console/api via the URLs you previously listed so you can see the new SSL cert without having to expire your cache.\nDelete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $RESOURCEGROUP ","description":"","tags":["ARO","Azure"],"title":"ARO Custom domain with cert-manager and LetsEncrypt","uri":"/docs/aro/cert-manager/"},{"content":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster with IBM Cloud Paks 4 Data.\nVideo Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through this quickstart on YouTube .\nPrerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended\nLog into https://console.redhat.com Browse to https://console.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-vm-size Standard_D16s_v3 \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL\naz aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials\naz aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.\nCloud Paks 4 Data Operator Setup Adding IBM Operator Catalog to Openshift Log into the OpenShift web console with your OpenShift cluster admin credentials.\nIn the top banner, click the plus (+) icon to open the Import YAML dialog box.\nPaste this resource definition into the dialog box:\napiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-operator-catalog namespace: openshift-marketplace spec: displayName: IBM Operator Catalog image: 'icr.io/cpopen/ibm-operator-catalog:latest' publisher: IBM sourceType: grpc updateStrategy: registryPoll: interval: 45m Click Create. IBM Cloud Paks 4 Data Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials.\nMake sure you have selected the Administrator view.\nClick Operators \u003e OperatorHub \u003e Integration \u0026 Delivery.\nSearch for and click the tile for the IBM Cloud Pak for Integration operator.\nClick Install.\nIn the Install Operator pane:\nSelect the latest update channel.\nSelect the option to install Cloud Pak for Integration in one namespace or for all namespaces on your cluster. If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace.\nSelect the Automatic approval strategy.\nClick Install.\nSuccessful Install Delete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP ","description":"","tags":["ARO","Azure"],"title":"ARO IBM Cloud Paks 4 Data","uri":"/docs/aro/ibm-cloud-paks-for-data/"},{"content":" Tip The official documentation for installing a OSD cluster in GCP can be found here .\nFor deploy an OSD cluster in GCP using existing Virtual Private Cloud (VPC) you need to implement some prerequisites that you must create before starting the OpenShift Dedicated installation though the OCM.\nPrerequisites gcloud CLI jq NOTE: Also the GCloud Shell can be used, and have the gcloud cli among other tools preinstalled.\nGenerate GCP VPC and Subnets This is a diagram showing the GCP infra prerequisites that are needed for the OSD installation:\nTo deploy the GCP VPC and subnets among other prerequisites for install the OSD in GCP using the preexisting VPCs you have two options:\nOption 1 - GCloud CLI Option 2 - Terraform Automation Please select one of these two options and proceed with the OSD install steps.\nOption 1 - Generate OSD VPC and Subnets using GCloud CLI As mentioned before, for deploy OSD in GCP using existing GCP VPC, you need to provide and create beforehand a GCP VPC and two subnets (one for the masters and another for the workers nodes).\nLogin and configure the proper GCP project where the OSD will be deployed:\nexport PROJECT_NAME=\u003cgoogle project name\u003e gcloud auth list gcloud config set project $PROJECT_NAME gcloud config list project Export the names of the vpc and subnets:\nexport REGION=\u003cregion name\u003e export OSD_VPC=\u003cvpc name\u003e export MASTER_SUBNET=\u003cmaster subnet name\u003e export WORKER_SUBNET=\u003cworker subnet name\u003e Create a custom mode VPC network:\ngcloud compute networks create $OSD_VPC --subnet-mode=custom gcloud compute networks describe $OSD_VPC NOTE: we need to create the mode custom for the VPC network, because the auto mode generates automatically the subnets with IPv4 ranges with predetermined set of ranges .\nThis example is using the standard configuration for these two subnets:\nmaster-subnet - CIDR 10.0.0.0/17 - Gateway 10.0.0.1 worker-subnet - CIDR 10.0.128.0/17 - Gateway 10.0.128.1 Create the GCP Subnets for the masters and workers within the previous GCP VPC network:\ngcloud compute networks subnets create $MASTER_SUBNET \\ --network=$OSD_VPC --range=10.0.0.0/17 --region=$REGION gcloud compute networks subnets create $WORKER_SUBNET \\ --network=$OSD_VPC --range=10.0.128.0/17 --region=$REGION Once the VPC and the two subnets are provided it is needed to create one GCP Cloud Router :\nexport OSD_ROUTER=\u003crouter name\u003e gcloud compute routers create $OSD_ROUTER \\ --project=$PROJECT_NAME --network=$OSD_VPC --region=$REGION {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nThen, we will deploy two GCP Cloud NATs and attach them within the GCP Router:\nGenerate the GCP Cloud Nat for the Master Subnets: export NAT_MASTER=\u003cmaster subnet name\u003e gcloud compute routers nats create $NAT_MASTER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$MASTER_SUBNET {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nGenerate the GCP Cloud NAT for the Worker Subnets: export NAT_WORKER=\u003cworker subnet name\u003e gcloud compute routers nats create $NAT_WORKER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$WORKER_SUBNET {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nAs you can check the Cloud NATs GW are attached now to the Cloud Router:\n{:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nOption 2 - Deploy OSD VPC and Subnets using Terraform You can use also automation code in Terraform to deploy all the GCP infrastructure required to deploy the OSD in preexistent VPCs.\nClone the tf-osd-gcp repository: git clone https://github.com/rh-mobb/tf-osd-gcp.git cd tf-osd-gcp Copy and modify the tfvars file in order to custom to your scenario: cp -pr terraform.tfvars.example terraform.tfvars Deploy the network infrastructure in GCP needed for deploy the OSD cluster: make all Install the OSD cluster using pre-existent VPCs These steps are based in the official OSD installation documentation .\nLog in to OpenShift Cluster Manager and click Create cluster.\nIn the Cloud tab, click Create cluster in the Red Hat OpenShift Dedicated row.\nUnder Billing model, configure the subscription type and infrastructure type {:style=“display:block; margin-left:auto; margin-right:auto”}\nSelect Run on Google Cloud Platform.\nClick Prerequisites to review the prerequisites for installing OpenShift Dedicated on GCP with CCS.\nProvide your GCP service account private key in JSON format. You can either click Browse to locate and attach a JSON file or add the details in the Service account JSON field. {:style=“display:block; margin-left:auto; margin-right:auto”}\nValidate your cloud provider account and then click Next. On the Cluster details page, provide a name for your cluster and specify the cluster details: {:style=“display:block; margin-left:auto; margin-right:auto”}\nNOTE: the Region used to be installed needs to be the same as the VPC and Subnets deployed in the early step.\nOn the Default machine pool page, select a Compute node instance type and a Compute node count: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nIn the Cluster privacy section, select Public endpoints and application routes for your cluster.\nSelect Install into an existing VPC to install the cluster in an existing GCP Virtual Private Cloud (VPC): {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nProvide your Virtual Private Cloud (VPC) subnet settings, that you deployed as prerequisites in the previous section: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nIn the CIDR ranges dialog, configure custom classless inter-domain routing (CIDR) ranges or use the defaults that are provided: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nOn the Cluster update strategy page, configure your update preferences.\nReview the summary of your selections and click Create cluster to start the cluster installation. Check that the Install into Existing VPC is enabled and the VPC and Subnets are properly selected and defined: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nCleanup Deleting a ROSA cluster consists of two parts:\nDeleting the OSD cluster can be done using the OCM console described in the official OSD docs .\nDeleting the GCP infrastructure resources (VPC, Subnets, Cloud NAT, Cloud Router). Depending of which option you selected you must perform:\nOption 1: Delete GCP resources using GCloud CLI:\ngcloud compute routers nats delete $NAT_WORKER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers nats delete $NAT_MASTER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers delete $OSD_ROUTER --region=$REGION --quiet gcloud compute networks subnets delete $MASTER_SUBNET --region=$REGION --quiet gcloud compute networks subnets delete $WORKER_SUBNET --region=$REGION --quiet gcloud compute networks delete $OSD_VPC --quiet Option 2: Delete GCP resources using Terraform:\nmake destroy ","description":"","tags":["GCP","OSD"],"title":"Creating a OSD in GCP with Existing VPCs","uri":"/docs/gcp/osd_preexisting_vpc/"},{"content":"AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS-managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster.\nROSA clusters have a set of the ACK controllers in Operator Hub which makes it relatively easy to get started and use it. Caution should be taken as it is a tech preview product from AWS.\nThis tutorial shows how to use the ACK S3 controller as an example, but can be adapted for any other ACK controller that has an operator in the OperatorHub of your cluster.\nPrerequisites A ROSA cluster AWS CLI OpenShift CLI oc Pre-install instructions Set some useful environment variables\nexport CLUSTER=ansible-rosa export NAMESPACE=ack-system export IAM_USER=${CLUSTER}-ack-controller export S3_POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess export SCRATCH_DIR=/tmp/ack export ACK_SERVICE=s3 export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create and bind an IAM service account for ACK to use\naws iam create-user --user-name $IAM_USER Create an access key for the user\nread -r ACCESS_KEY_ID ACCESS_KEY \u003c \u003c(aws iam create-access-key \\ --user-name $IAM_USER \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) Find the ARN of the recommended IAM policy\nNote: you can find the recommended policy in each projects github repo, example https://github.com/aws-controllers-k8s/s3-controller/blob/main/config/iam/recommended-policy-arn aws iam attach-user-policy \\ --user-name $IAM_USER \\ --policy-arn \"$S3_POLICY_ARN\" Install the ACK S3 Controller Log into your OpenShift console, click to OperatorHub and search for “ack” Select the S3 controller and install it.\nCreate a config map for ACK to use\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/config.txt ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=$CLUSTER_NAME EOF Apply the config map\noc create configmap --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/config.txt ack-s3-user-config Create a secret for ACK to use\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/secrets.txt AWS_ACCESS_KEY_ID=$ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$ACCESS_KEY EOF Apply the secret\noc create secret generic --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/secrets.txt ack-s3-user-secrets Check the ack-s3-controller is running\nkubectl -n ack-system get pods NAME READY STATUS RESTARTS AGE ack-s3-controller-6dc4b4c-zgs2m 1/1 Running 0 145m If its not, restart it so that it can read the new configmap/secret.\nkubectl rollout restart deployment ack-s3-controller Deploy an S3 Bucket Resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $CLUSTER-bucket spec: name: $CLUSTER-bucket EOF Verify the S3 Bucket Resource\naws s3 ls | grep $CLUSTER-bucket 2022-06-02 12:20:25 ansible-rosa-bucket ","description":"","tags":["AWS","ROSA"],"title":"Using AWS Controllers for Kubernetes (ACK) on ROSA","uri":"/docs/rosa/ack/"},{"content":"Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN\nCreate the policy cat \u003c\u003cEOF \u003e /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create a user and access key and attach the policy aws iam create-user --user-name ecr-bot aws create-access-key --user-name ecr-bot aws iam attach-user-policy --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy --user-name ecr-bot Notes: Save access key id and key for later usage\nSet up a specific ECR repository access cat \u003c\u003cEOF \u003e /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:user/ecr-bot\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create kubernetes Secret with iam user cat \u003c\u003cEOF \u003e /tmp/credentials [default] aws_access_key_id=\"\" aws_secret_access_key=\"\" EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials ","description":"","tags":["AWS","ROSA"],"title":"Create IAM user and Policy","uri":"/docs/rosa/ecr-secret-operator/iam_user/"},{"content":"About AWS STS and Assume Role Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN\nPrequisites\nAn STS Openshift Cluster Setup Environment Variables\nexport OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export REPOSITORY_NAME=test Create the policy\ncat \u003c\u003cEOF \u003e /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create the role and attach the policy cat \u003c\u003cEOF \u003e /tmp/trust_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": \"system:serviceaccount:ecr-secret-operator:ecr-secret-operator-controller-manager\" } } } ] } EOF aws iam create-role --role-name ECRLogin --assume-role-policy-document file:///tmp/trust_policy.json aws iam attach-role-policy --role-name ECRLogin --policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/ECRLoginPolicy Create the repository policy cat \u003c\u003cEOF \u003e /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT_ID}:role/ECRLogin\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name ${REPOSITORY_NAME} --policy-text file:///tmp/repo_policy.json Create STS kubernetes Secret cat \u003c\u003cEOF \u003e /tmp/credentials [default] role_arn = arn:aws:iam::${AWS_ACCOUNT_ID}:role/ECRLogin web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc new-project ecr-secret-operator oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials -n ecr-secret-operator ","description":"","tags":["AWS","ROSA"],"title":"Create STS Assume Role","uri":"/docs/rosa/ecr-secret-operator/iam_assume_role/"},{"content":"Amazon Elastic Container Registry Private Registry Authentication provides a temporary authorization token valid only for 12 hours. This operator refreshes automatically the Amazon ECR authorization token before it expires, reducing the overhead in managing the authentication flow.\nThis operator contains two Custom Resources which direct the operator to generate/refresh Amazon ECR authorization token in a timely manner:\nImage Pull Secret API Argo CD Repo Helm Chart Secret How to use this operator Prerequisites Create an ECR private repository Provide AWS Authentication to the operator. Two Options: IAM User STS Assume Role Install the operator Install the operator from operator hub community Create the ECR Secret CRD oc new-project test-ecr-secret-operator cat \u003c\u003c EOF | oc apply -f - apiVersion: ecr.mobb.redhat.com/v1alpha1 kind: Secret metadata: name: ecr-secret namespace: test-ecr-secret-operator spec: generated_secret_name: ecr-docker-secret ecr_registry: ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com frequency: 10h region: us-east-2 EOF A docker registry secret is created by the operator momentally and the token is patched every 10 hours\noc get secret ecr-docker-secret NAME TYPE DATA AGE ecr-docker-secret kubernetes.io/dockerconfigjson 1 16h A sample build process with generated secret Link the secret to builder\noc secrets link builder ecr-docker-secret Configure build config to point to your ECR Container repository\noc create imagestream ruby oc tag openshift/ruby:2.5-ubi8 ruby:2.5 cat \u003c\u003c EOF | oc apply -f - kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: ruby-sample-build namespace: test-ecr-secret-operator spec: runPolicy: Serial source: git: uri: \"https://github.com/openshift/ruby-hello-world\" strategy: sourceStrategy: from: kind: \"ImageStreamTag\" name: \"ruby:2.5\" incremental: true output: to: kind: \"DockerImage\" name: \"${AWS_ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com/test:latest\" postCommit: script: \"bundle exec rake test\" EOF oc start-build ruby-sample-build --wait Build should succeed and push the image to the the private ECR Container repository\nCreate the ECR Secret Argo CD Helm Repo CRD OpenShift GitOps is installed Helm chart stored in ecr Create the Helm Repo CRD cat \u003c\u003c EOF | oc apply -f - apiVersion: ecr.mobb.redhat.com/v1alpha1 kind: ArgoHelmRepoSecret metadata: name: helm-repo namespace: openshift-gitops spec: generated_secret_name: ecr-argo-helm-secret url: ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com frequency: 10h region: us-east-2 EOF Create a sample GitOps application cat \u003c\u003c EOF | oc apply -f - apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: test spec: destination: name: '' namespace: test-ecr-secret-operator server: 'https://kubernetes.default.svc' source: path: '' repoURL: ${AWS_ACCOUNT_ID}.dkr.ecr.us-east-2.amazonaws.com targetRevision: 0.1.0 chart: helm-test-chart project: default EOF The ArgoCD application should sync with ECR helm chart successfully ","description":"","tags":["AWS","ROSA"],"title":"ECR Secret Operator","uri":"/docs/rosa/ecr-secret-operator/"},{"content":" Tip The official documentation for installing a ROSA cluster in STS mode can be found here .\nQuick Introduction by Ryan Niksch (AWS) and Shaozen Ding (Red Hat) on YouTube STS allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies with Amazon STS (secure token service) to gain access to the AWS resources needed to install and operate the cluster.\nThis is a summary of the official docs that can be used as a line by line install guide and later used as a basis for automation in your favorite automation tool .\nNote that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $REGION instead or you will fail installation.\nPrerequisites AWS CLI Rosa CLI v1.2.2 OpenShift CLI (run rosa download openshift-client) jq Prepare local environment set some environment variables\nexport VERSION=4.11.31 \\ ROSA_CLUSTER_NAME=mycluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-2 \\ AWS_PAGER=\"\" Prepare AWS and Red Hat accounts If this is your first time deploying ROSA you need to do some preparation as described here . Stop just before running rosa init we don’t need to do that for STS mode.\nIf this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Associate your AWS account\nTo perform ROSA cluster provisioning tasks, you must create ocm-role and user-role IAM resources in your AWS account and link them to your Red Hat organization.\nCreate the OCM Role\nThe first role you will create is the ocm-role which the OpenShift Cluster Manager (OCM) will use to be able to administer and Create ROSA clusters. If this has already been done for your OCM Organization, you can skip to creating the user-role. If you haven’t already created the ocm-role, you can create and link the role with one command.\nrosa create ocm-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate.\nTip You can get your OCM role arn from AWS IAM:\naws iam list-roles | grep OCM Create the User Role\nThe second is the user-role that allows OCM to verify that users creating a cluster have access to the current AWS account. If you haven’t already created the user-role, you can create and link the role with one command.\nrosa create user-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate.\nIf you have already created the user-role, you can just link the user-role to your Red Hat organization. rosa link user-role --role-arn \u003carn\u003e Tip You can get your user-role arn by running the ROSA cli command: rosa whoami. Look for the AWS ARN: field. Deploy ROSA cluster Make you your ROSA CLI version is correct (v1.2.2 or higher)\nrosa version Run the rosa cli to create your cluster\nYou can run the command as provided in the ouput of the previous step to deploy in interactive mode.\nAdd any other arguments to this command to suit your cluster. for example --private-link and --subnet-ids=subnet-12345678,subnet-87654321.\nrosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} --mode auto -y Validate The cluster is now installing\nThe State should have moved beyond pending and show installing or ready.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Tip Newer versions of MacOS do not include a watch command. One can be installed using a package manager such as Homebrew .\nWatch the install logs\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate we can access it\nCreate an Admin user\nrosa create admin -c $ROSA_CLUSTER_NAME Wait a few moments and run the oc login command it provides.\nCleanup Delete the ROSA cluster\nrosa delete cluster -c $ROSA_CLUSTER_NAME Clean up the STS roles\nOnce the cluster has been deleted we can delete the STS roles.\nTip You can get the correct commands with the ID filled in from the output of the previous step.\nrosa delete operator-roles -c \u003cid\u003e --yes --mode auto rosa delete oidc-provider -c \u003cid\u003e --yes --mode auto ","description":"","tags":["AWS","ROSA","STS"],"title":"Deploying ROSA in STS mode","uri":"/docs/rosa/sts/"},{"content":"Note: This guide a simple “happy path” to show the path of least friction to showcasing how to use NetApp files with Azure Red Hat OpenShift. This may not be the best behavior for any system beyond demonstration purposes.\nPrerequisites An Azure Red Hat OpenShift cluster installed with Service Principal role/credentials. kubectl cli oc cli helm 3 cli Review official trident documentation In this guide, you will need service principal and region details. Please have these handy.\nAzure subscriptionID Azure tenantID Azure clientID (Service Principal) Azure clientSecret (Service Principal Secret) Azure Region If you don’t have your existing ARO service principal credentials, you can create your own service principal and grant it contributor to be able to manage the required resources. Please review the official Trident documentation regarding Azure NetApp files and required permissions.\nImportant Concepts Persistent Volume Claims are namespaced objects . Mounting RWX/ROX is only possible within the same namespace.\nNetApp files must be have a delegated subnet within your ARO Vnet’s and you must assign it to the Microsoft.Netapp/volumes service.\nConfigure Azure You must first register the Microsoft.NetApp provider and Create a NetApp account on Azure before you can use Azure NetApp Files.\nRegister NetApp files Azure Console or az cli\naz provider register --namespace Microsoft.NetApp --wait Create storage account Again, for brevity I am using the same RESOURCE_GROUP and Service Principal that the cluster was created with.\nAzure Console or az cli\nRESOURCE_GROUP=\"myresourcegroup\" LOCATION=\"southcentralus\" ANF_ACCOUNT_NAME=\"netappfiles\" az netappfiles account create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME Create capacity pool Creating one pool for now. The common pattern is to expose all three levels with unique pool names respective of each service level.\nAzure Console or az cli:\nPOOL_NAME=\"Standard\" POOL_SIZE_TiB=4 # Size in Azure CLI needs to be in TiB unit (minimum 4 TiB) SERVICE_LEVEL=\"Standard\" # Valid values are Standard, Premium and Ultra az netappfiles pool create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME \\ --pool-name $POOL_NAME \\ --size $POOL_SIZE_TiB \\ --service-level $SERVICE_LEVEL Delegate subnet to ARO Login to azure console, find the subnets for your ARO cluster and click add subnet. We need to call this subnet anf.subnet since that is the name we refer to in later configuration.\nInstall Trident Operator Login/Authenticate to ARO Login to your ARO cluster. You can create a token to login via cli straight from the web gui\noc login --token=sha256~abcdefghijklmnopqrstuvwxyz --server=https://api.randomseq.eastus.aroapp.io:6443 Helm Install Download latest Trident package\nwget https://github.com/NetApp/trident/releases/download/v22.04.0/trident-installer-22.04.0.tar.gz Extract tar.gz into working director\ntar -xzvf trident-installer-22.04.0.tar.gz cd into installer\ncd trident-installer/helm Helm install\nhelm install trident-operator trident-operator-22.04.0.tgz Example output from installation:\nW0523 17:45:22.189592 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 17:45:22.484071 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: trident-operator LAST DEPLOYED: Mon May 23 17:45:20 2022 NAMESPACE: openshift STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing trident-operator, which will deploy and manage NetApp's Trident CSI storage provisioner for Kubernetes. Your release is named 'trident-operator' and is installed into the 'openshift' namespace. Please note that there must be only one instance of Trident (and trident-operator) in a Kubernetes cluster. To configure Trident to manage storage resources, you will need a copy of tridentctl, which is available in pre-packaged Trident releases. You may find all Trident releases and source code online at https://github.com/NetApp/trident. To learn more about the release, try: $ helm status trident-operator $ helm get all trident-operator Validate\ncd .. ./tridentctl -n openshift version +----------------+----------------+ | SERVER VERSION | CLIENT VERSION | +----------------+----------------+ | 22.04.0 | 22.04.0 | +----------------+----------------+ Install tridentctl I put all my cli’s in /usr/local/bin\nsudo install tridentctl /usr/local/bin example output:\nwhich tridentctl /usr/local/bin/tridentctl Create trident backend FYI - Sample files for review are in sample-input/backends-samples/azure-netapp-files directory from the trident tgz we extracted earlier.\nReplace client ID with service principal ID Replace clientSecret with Service Principal Secret Replace tenantID with your account tenant ID Replace subscriptionID with your azure SubscriptionID Ensure location matches your Azure Region Note: I have used nfsv3 for basic compatibility. You can remove that line and use NetApp files defaults.\nvi backend.json Add the following snippet:\n{ \"version\": 1, \"nfsMountOptions\": \"nfsvers=3\", \"storageDriverName\": \"azure-netapp-files\", \"subscriptionID\": \"12abc678-4774-fake-a1b2-a7abcde39312\", \"tenantID\": \"a7abcde3-edc1-fake-b111-a7abcde356cf\", \"clientID\": \"abcde356-bf8e-fake-c111-abcde35613aa\", \"clientSecret\": \"rR0rUmWXfNioN1KhtHisiSAnoTherboGuskey6pU\", \"location\": \"southcentralus\", \"subnet\": \"anf.subnet\", \"labels\": { \"cloud\": \"azure\" }, \"storage\": [ { \"labels\": { \"performance\": \"Standard\" }, \"serviceLevel\": \"Standard\" } ] } run\ntridentctl -n openshift create backend -f backend.json example output:\n+------------------------+--------------------+--------------------------------------+--------+---------+ | NAME | STORAGE DRIVER | UUID | STATE | VOLUMES | +------------------------+--------------------+--------------------------------------+--------+---------+ | azurenetappfiles_eb177 | azure-netapp-files | f7f211afe-d7f5-41a5-a356-fa67f25ee96b | online | 0 | +------------------------+--------------------+--------------------------------------+--------+---------+ if you get a failure here, you can run to following command to review logs:\ntridentctl logs To view log output that may help steer you in the right direction.\nCreate storage class cat \u003c\u003cEOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: csi.trident.netapp.io parameters: backendType: \"azure-netapp-files\" fsType: \"nfs\" selector: \"performance=Standard\" # Matching labels in the backends... allowVolumeExpansion: true # To allow volume resizing. This parameter is optional mountOptions: - nconnect=16 EOF output:\nstorageclass.storage.k8s.io/standard created Provision volume Let’s create a new project and set up a persistent volume claim. Remember that PV Claims are namespaced objects and you must create the pvc in the namespace where it will be allocated. I’ll use the project “netappdemo”.\noc new-project netappdemo Now we’ll create a PV claim in the “netappdemo” project we just created.\ncat \u003c\u003cEOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 4000Gi storageClassName: standard EOF output:\npersistentvolumeclaim/standard created Verify Quick verification of storage, volumes and services.\nVerify Kubectl ➜ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 3h26m standard csi.trident.netapp.io Delete Immediate true 5m5s ➜ Verify OpenShift Login to your cluster as cluster-admin and verify your storage classes and persistent volumes.\nStorage Class\nPersisent Volumes Create Pods to test Azure NetApp We’ll create two pods here to exercise the Azure NetApp file mount. One to write data and another to read data to show that it is mounted as “read write many” and correctly working.\nWriter Pod This pod will write “hello netapp” to a shared NetApp mount.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp labels: app: test-aznetapp deploymethod: trident spec: containers: - name: aznetapp image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do echo 'hello netapp' | tee -a /mnt/netapp-data/verify-netapp \u0026\u0026 sleep 5; done;\", ] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF You can watch for this container to be ready:\nwatch oc get pod test-netapp Or view it in the OpenShift Pod console for the netappdemo project.\nReader Pod This pod will read back the data from the shared NetApp mount.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp-read spec: containers: - name: test-netapp-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/netapp-data/verify-netapp\"] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF Now let’s verify the POD is reading from shared volume.\noc logs test-netapp-read hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp You can also see the pod details in OpenShift for the reader:\n","description":"","tags":["ARO","Azure"],"title":"Trident NetApp operator setup for Azure NetApp files","uri":"/docs/aro/trident/"},{"content":"This is an example guide for creating a public ingress endpoint for a ROSA Private-Link cluster. Be aware of the security implications of creating a public subnet in your ROSA VPC this way.\nRefer to the blog \u0026ldquo;How to add public Ingress to a PrivateLink ROSA cluster\u0026rdquo; , to expose applications to the internet by deploying in a PrivateLink Red Hat OpenShift Service on AWS (ROSA) cluster within a truly private Virtual Private Cloud (VPC) that doesn’t have an internet gateway attached to it. Additionally, the blog details about creating CloudFront distribution for content delivery and WAF to protect web applications by filtering and monitoring HTTP traffic between a web application and the internet. Also,AWS network firewall will be used for fine-grained control over network traffic.\nPrerequisites AWS CLI Rosa CLI v1.0.8 jq A ROSA PL cluster Getting Started Set some environment variables Set the following environment variables, changing them to suit your cluster.\nexport ROSA_CLUSTER_NAME=private-link # this should be a free CIDR inside your VPC export PUBLIC_CIDR=10.0.2.0/24 export AWS_PAGER=\"\" export EMAIL=username.taken@gmail.com export DOMAIN=public.aws.mobb.ninja export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create a public subnet If you followed the above instructions to create the ROSA Private-Link cluster, you should already have a public subnet in your VPC and can skip to tagging the subnet.\nGet a Private Subnet ID from the cluster.\nPRIVATE_SUBNET_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME -o json \\ | jq -r '.aws.subnet_ids[0]') echo $PRIVATE_SUBNET_ID Get the VPC ID from the subnet ID.\nVPC_ID=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].VpcId' --output text) echo $VPC_ID Get the Cluster Tag from the subnet\nTAG=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].Tags[?Value == `shared`]' | jq -r '.[0].Key') echo $TAG Create a public subnet\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR \\ --query 'Subnet.SubnetId' --output text` echo $PUBLIC_SUBNET Tag the public subnet for the cluster\naws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public \\ Key=$TAG,Value=\"shared\" Key=kubernetes.io/role/elb,Value=\"true\" Create a Custom Domain Create TLS Key Pair for custom domain using certbot:\nSkip this if you already have a key pair.\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" \\ -d \"*.$DOMAIN\" Create TLS secret for custom domain:\nNote use your own keypair paths if not using certbot.\nCERTS=/tmp/scratch/config/live/$DOMAIN oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem Create Custom Domain resource:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait for the domain to be ready:\nwatch oc get customdomains Once its ready grab the CLB name:\nCDO_NAME=acme CLB_NAME=$(oc get svc -n openshift-ingress -o jsonpath='{range .items[?(@.metadata.labels.ingresscontroller\\.operator\\.openshift\\.io\\/owning-ingresscontroller==\"'$CDO_NAME'\")]}{.status.loadBalancer.ingress[].hostname}{\"\\n\"}{end}') echo $CLB_NAME Create a CNAME in your DNS provider for *.\u003c$DOMAIN\u003e that points at the CLB NAME from the above command.\nDeploy a public application Create a new project\noc new-project my-public-app Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application\noc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.$DOMAIN Check that you can access the application:\ncurl https://hello.$DOMAIN You should see the output\nHello OpenShift! ","description":"","tags":["AWS","ROSA","Private Link"],"title":"Adding a Public Ingress endpoint to a ROSA Private-Link Cluster","uri":"/docs/rosa/private-link/public-ingress/"},{"content":"","description":"","tags":null,"title":"Private Link","uri":"/tags/private-link/"},{"content":"By default, within OSD in GCP only the GCE-PD StorageClass is available in the cluster. With this StorageClass, only ReadWriteOnce mode is permitted, and the gcePersistentDisks can only be mounted by a single consumer in read-write mode .\nBecause of that, and for provide Storage with Shared Access (RWX) Access Mode to our OpenShift clusters a GCP Filestore could be used.\nGCP Filestore is not managed neither supported by Red Hat or Red Hat SRE team.\nPrerequisites gcloud CLI jq oc CLI The GCP Cloud Shell can be used as well and have all the prerequisites installed already.\nSteps From the CLI or GCP Cloud Shell, login within your account and your GCP project:\ngcloud auth login \u003cgoogle account user\u003e gcloud config set project \u003cgoogle project name\u003e Create a Filestore instance in GCP:\nexport ZONE_FS=\"us-west1-a\" export NAME_FS=\"nfs-server\" export TIER_FS=\"BASIC_HDD\" export VOL_NAME_FS=\"osd4\" export CAPACITY=\"1TB\" export VPC_NETWORK=\"projects/my-project/global/networks/demo-vpc\" gcloud filestore instances create $NAME_FS --zone=$ZONE_FS --tier=$TIER_FS --file-share=name=\"$VOL_NAME_FS\",capacity=$CAPACITY --network=name=\"$VPC_NETWORK\" Due to the Static Provisioning through the creation of the PV/PVC the Filestore for the RWX storage needs to be created upfront.\nAfter the creation, check the Filestore instance generated in the GCP project:\ngcloud filestore instances describe $NAME_FS --zone=$ZONE_FS Extract the ipAddresses from the NFS share for use them into the PV definition:\nNFS_IP=$(gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS --format=json | jq -r .networks[0].ipAddresses[0]) echo $NFS_IP Login your OSD in GCP cluster\nCreate a Persistent Volume using the NFS_IP of the Filestore as the nfs server into the PV definition, specifying the path of the shared Filestore:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 500Gi accessModes: - ReadWriteMany nfs: server: $NFS_IP path: \"/$VOL_NAME_FS\" EOF As you can check the PV is generated with the accessMode of ReadWriteMany (RWX)\nCheck that the PV is generated properly:\n$ oc get pv nfs NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 500Gi RWX Retain Available 12s Create a PersistentVolumeClaim for this PersistentVolume:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 500Gi EOF As we can check the storageClassName is empty because we’re using the Static Provisioning in this case.\nCheck that the PVC is generated properly and with the Bound status:\noc get pvc nfs NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 500Gi RWX 7s Generate an example app with more than replicas sharing the same Filestore NFS volume share:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nfs-web2 name: nfs-web spec: replicas: 2 selector: matchLabels: app: nfs-web strategy: {} template: metadata: creationTimestamp: null labels: app: nfs-web spec: containers: - image: nginxinc/nginx-unprivileged name: nginx-unprivileged ports: - name: web containerPort: 8080 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs EOF Check that the pods are up \u0026\u0026 running:\noc get pod NAME READY STATUS RESTARTS AGE nfs-web2-54f9fb5cd8-8dcgh 1/1 Running 0 118s nfs-web2-54f9fb5cd8-bhmkw 1/1 Running 0 118s Check that the pods mount the same volume provided by the Filestore NFS share:\nfor i in $(oc get pod --no-headers | awk '{ print $1 }'); do echo \"POD -\u003e $i\"; oc exec -ti $i -- df -h | grep nginx; echo \"\"; done POD -\u003e nfs-web2-54f9fb5cd8-8dcgh 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html POD -\u003e nfs-web2-54f9fb5cd8-bhmkw 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html ","description":"","tags":["GCP","OSD"],"title":"Create Filestore Storage for OSD in GCP","uri":"/docs/gcp/filestore/"},{"content":"Prerequisites AWS CLI Openshift CLI 4.11+ Podman Desktop Background Quick Introduction by Ryan Niksch \u0026 Charlotte Fung on YouTube .\nThere are two options to use to authenticate wth Amazon ECR to pull images.\nThe traditional method is to create a pull secret for ecr.\nExample:\noc create secret docker-registry ecr-pull-secret \\ --docker-server=\u003cregistry id\u003e.dkr.ecr.\u003cregion\u003e.amazonaws.com \\ --docker-username=AWS --docker-password=$(aws ecr get-login-password) \\ --namespace=hello-world However Amazon ECR tokens expire every 12 hours which will mean you will need to re-authenticate every 12 hours either through scripting or do so manually.\nA second, and preferred method, is to attach an ECR Policy to your cluster’s worker machine profiles which this guide will walk you through.\nAttach ECR Policy Role You can attach an ECR policy to your cluster giving the cluster permissions to pull images from your registries. ROSA worker machine instances comes with pre-defined IAM roles, named differently depending on whether its a STS cluster or a non-STS cluster.\nSTS Cluster Role ManagedOpenShift-Worker-Role is the IAM role attached to ROSA STS compute instances.\nnon-STS Cluster Role \u003ccluster name\u003e-\u003cidentifier\u003e-worker-role is the IAM role attached to ROSA non-STS compute instances.\nTip: To find the non-STS cluster role run the following command with your cluster name:\naws iam list-roles | grep \u003ccluster_name\u003e Configure ECR with ROSA ECR has several pre-defined policies that give permissions to interact with the service. In the case of ROSA, we will be pulling images from ECR and will only need to add the AmazonEC2ContainerRegistryReadOnly policy.\nAdd the AmazonEC2ContainerRegistryReadOnly policy to the ManagedOpenShift-Worker-Role for STS clusters (or the \u003ccluster name\u003e-\u003cidentifier\u003e-worker-role for non-STS clusters).\nSTS Example:\naws iam attach-role-policy \\ --role-name ManagedOpenShift-Worker-Role \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\" Set ENV variables\nSet our AWS Region and Registry name for creating a new ECR\nREGION=us-east-2 REGISTRY=hello-ecr Create a repository\naws ecr create-repository \\ --repository-name $REGISTRY \\ --image-scanning-configuration scanOnPush=true \\ --region $REGION Set Registry ID\nREGISTRYID=`aws ecr describe-repositories --repository-name hello-ecr | jq -r '.repositories[].registryId'` Log into ECR\npodman login -u AWS -p $(aws ecr get-login-password --region $REGION) $REGISTRYID.dkr.ecr.$REGION.amazonaws.com Pull an image\npodman pull openshift/hello-openshift Tag the image for ecr\npodman tag openshift/hello-openshift:latest $REGISTRYID.dkr.ecr.$REGION.amazonaws.com/hello-ecr:latest Push the image to ECR\npodman push $REGISTRYID.dkr.ecr.$REGION.amazonaws.com/hello-ecr:latest Create OC pull secret for new ECR registry\noc create secret docker-registry ecr-pull-secret --docker-server=$REGISTRYID.dkr.ecr.$REGION.amazonaws.com \\ --docker-username=AWS --docker-password=$(aws ecr get-login-password) --namespace=hello-ecr Create a new project\noc new-project hello-ecr Create a new app using the image on ECR oc new-app --name hello-ecr --image $REGISTRYID.dkr.ecr.$REGION.amazonaws.com/hello-ecr:latest View a list of pods in the namespace you created: oc get pods Expected output:\nIf you see the hello-ecr pod running … congratulations! You can now pull images from your ECR repository.\nClean up Simply delete the project you created to test pulling images:\noc delete project hello-ecr You may also want to remove the arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly policy from the worker nodes if you do no want them to continue to have access to the ECR.\n","description":"","tags":["AWS","ROSA"],"title":"Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR)","uri":"/docs/rosa/ecr/"},{"content":" Tip Official Documentation ROSA STS with custom KMS key This guide will walk you through installing ROSA (Red Hat OpenShift Service on AWS) with a customer-provided KMS key that will be used to encrypt both the root volumes of nodes as well as persistent volumes for mounted EBS claims.\nPrerequisites AWS CLI ROSA CLI v1.1.11 or higher OpenShift CLI - rosa download openshift-client Prepare AWS Account for ROSA Configure the AWS CLI by running the following command\naws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format\n% aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user\nValidate your credentials\naws sts get-caller-identity You should receive output similar to the following\n{ \"UserId\": \u003cyour ID\u003e, \"Account\": \u003cyour account\u003e, \"Arn\": \u003cyour arn\u003e } You will need to save the account ID for adding it to your KMS key to define installer role, so take note.\nIf this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Set the AWS region you plan to deploy your cluser into. For this example, we will deploy into us-east-2.\nexport AWS_REGION=\"us-east-2\" Create ROSA Cluster Make sure your ROSA CLI version is at minimum v1.1.11 or higher.\nrosa version Create the ROSA STS Account Roles\nIf you have already installed account-roles into your aws account, you can skip this step.\nrosa create account-roles --mode auto -y Set Environment Variables\nROSA_CLUSTER_NAME=poc-kmskey Using the ROSA CLI, create your cluster.\nWhile this is an example, feel free to customize this command to best suit your needs.\nrosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts \\ --region $AWS_REGION --compute-nodes 2 --machine-cidr 10.0.0.0/16 \\ --service-cidr 172.30.0.0/16 --pod-cidr 10.128.0.0/14 --host-prefix 23 \\ --kms-key-arn $KMS_ARN Take note of the role \u003ccluster-name-hash\u003e-openshift-cluster-csi-drivers-ebs-cloud-credential, which is provided at the end of the previous command.\nThe full name of the role gets truncated depending on the length of the cluster name. Do not copy and paste from the example above.\narn:aws:iam::\u003caws-account\u003e:role/poc-mskey-x6h0-openshift-cluster-csi-drivers-ebs-cloud-credenti Create KMS Key For this example, we will create a custom KMS key using the AWS CLI. If you would prefer, you could use an existing key instead.\nCreate a customer-managed KMS key\nKMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'Custom ROSA Encryption Key' --query KeyMetadata.Arn --output text) This command will save the ARN output of this custom key for further steps.\nGenerate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own.\nImportant note 1, if you specify a custom STS role prefix, you will need to update that in the command below.\nImportant note 2, adapt the name of the role openshift-cluster-csi-drivers-ebs-cloud-credentials as per captured in previous steps.\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"key-rosa-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/${ROSA_CLUSTER_NAME}-openshift-cluster-csi-drivers-ebs-cloud-credenti\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/${ROSA_CLUSTER_NAME}-openshift-cluster-csi-drivers-ebs-cloud-credenti\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Create the operator roles necessary for the cluster to function.\nrosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider necessary for the cluster to authenticate.\nrosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate that the cluster is now installing. Within 5 minutes, the cluster state should move beyond pending and show installing.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs as the cluster installs.\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate our access to the cluster.\nCreate an Admin user\nrosa create admin -c $ROSA_CLUSTER_NAME Run the resulting login statement from output. May take 2-3 minutes before authentication is fully synced\nVerify the default persistent volumes in the cluster.\noc get pv Output:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-00dac374-a45e-43fa-a313-ae0491e8edf1 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-1 gp3-customer-kms 26m pvc-7d211496-4ddf-4200-921c-1404b754afa5 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-0 gp3-customer-kms 26m pvc-b5243cef-ec30-4e5c-a348-aeb8136a908c 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-0 gp3-customer-kms 26m pvc-ec60c1cf-72cf-4ac6-ab12-8e9e5afdc15f 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-1 gp3-customer-kms 26m You should see the StroageClass set to gp3-customer-kms. This is the default StorageClass which is encrypted using the customer-provided key.\nCleanup Delete the ROSA cluster\nrosa delete cluster -c $ROSA_CLUSTER_NAME Once the cluster is deleted, delete the cluster’s STS roles.\nrosa delete operator-roles -c $ROSA_CLUSTER_NAME --yes --mode auto rosa delete oidc-provider -c $ROSA_CLUSTER_NAME --yes --mode auto ","description":"","tags":["AWS","ROSA"],"title":"Creating a ROSA cluster in STS mode with custom KMS key","uri":"/docs/rosa/kms/"},{"content":"Prerequisites an Azure Red Hat OpenShift cluster Get Started Run this oc command to enable the Managed Upgrade Operator (MUO)\noc patch cluster.aro.openshift.io cluster --patch \\ '{\"spec\":{\"operatorflags\":{\"rh.srep.muo.enabled\": \"true\",\"rh.srep.muo.managed\": \"true\",\"rh.srep.muo.deploy.pullspec\":\"arosvc.azurecr.io/managed-upgrade-operator@sha256:f57615aa690580a12c1e5031ad7ea674ce249c3d0f54e6dc4d070e42a9c9a274\"}}}' \\ --type=merge Wait a few moments to ensure the Management Upgrade Operator is ready\noc -n openshift-managed-upgrade-operator \\ get deployment managed-upgrade-operator NAME READY UP-TO-DATE AVAILABLE AGE managed-upgrade-operator 1/1 1 1 2m2s Configure the Managed Upgrade Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: managed-upgrade-operator-config namespace: openshift-managed-upgrade-operator data: config.yaml: | configManager: source: LOCAL localConfigName: managed-upgrade-config watchInterval: 1 maintenance: controlPlaneTime: 90 ignoredAlerts: controlPlaneCriticals: - ClusterOperatorDown - ClusterOperatorDegraded upgradeWindow: delayTrigger: 30 timeOut: 120 nodeDrain: timeOut: 45 expectedNodeDrainTime: 8 scale: timeOut: 30 healthCheck: ignoredCriticals: - PrometheusRuleFailures - CannotRetrieveUpdates - FluentdNodeDown ignoredNamespaces: - openshift-logging - openshift-redhat-marketplace - openshift-operators - openshift-user-workload-monitoring - openshift-pipelines EOF Restart the Managed Upgrade Operator\noc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=0 oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=1 Look for available Upgrades\nIf there output is nil there are no available upgrades and you cannot continue.\noc get clusterversion version -o jsonpath='{.status.availableUpdates}' Schedule an Upgrade\nSet the Channel and Version to the desired values from the above list of available upgrades.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: upgrade.managed.openshift.io/v1alpha1 kind: UpgradeConfig metadata: name: managed-upgrade-config namespace: openshift-managed-upgrade-operator spec: type: \"ARO\" upgradeAt: $(date -u --iso-8601=seconds --date \"+5 minutes\") PDBForceDrainTimeout: 60 capacityReservation: false desired: channel: \"stable-4.9\" version: \"4.9.27\" EOF Check the status of the scheduled upgrade\noc -n openshift-managed-upgrade-operator get \\ upgradeconfigs.upgrade.managed.openshift.io \\ managed-upgrade-config -o jsonpath='{.status}' | jq The output of this command should show upgrades in progress\n{ \"history\": [ { \"conditions\": [ { \"lastProbeTime\": \"2022-04-12T14:42:02Z\", \"lastTransitionTime\": \"2022-04-12T14:16:44Z\", \"message\": \"ControlPlaneUpgraded still in progress\", \"reason\": \"ControlPlaneUpgraded not done\", \"startTime\": \"2022-04-12T14:16:44Z\", \"status\": \"False\", \"type\": \"ControlPlaneUpgraded\" }, You can verify the upgrade has completed successfully via the following\noc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.9.27 True False 161m Cluster version is 4.9.27 ","description":"","tags":["ARO","Azure"],"title":"Enable the Managed Upgrade Operator in ARO and schedule Upgrades","uri":"/docs/aro/managed-upgrade-operator/"},{"content":"The following instructions will detail how to configure GitLab as the identity provider for Azure Red Hat OpenShift:\nRegister a new application in GitLab Create OAuth callback URL in ARO Log in and confirm Add administrative users or groups Register a new application in GitLab Log into GitLab and execute the following steps:\nGo to Preferences\nSelect Applications from the left navigation bar\nProvide a Name and enter an OAuth Callback URL as the Redirect URI in GitLab\nNote: the OAuth Callback has the following format: https://oauth-openshift.apps.\u003ccluster-id\u003e.\u003cregion\u003e.aroapp.io/oauth2callback/GitLab\nCheck the openid box and save the application\nAfter saving the GitLab application you will be provided with an Application ID and a Secret\nCopy both the Application ID and Secret for use in the ARO console\nCreate OAuth provider in ARO Log in to the ARO console as an administrator to add a GitLab identity provider\nSelect the ‘Administration’ drop down and click ‘Cluster Settings’\nOn the ‘Configuration’ scroll down and click on ‘OAuth’\nSelect ‘GitLab’ from the Identity Providers drop down\nEnter a Name, the base URL of your GitLab OAuth server, and the Client ID and CLient Secret from the previous step\nClick Add to confirm the configuration\nLog in and confirm Go to the ARO console in a new browser to bring up the OpenShift login page. An option for GitLab should now be available.\nNote: I can take 2-3 minutes for this update to occur\nAfter selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm.\nOnce you have successfully logged in using GitLab, your userid should display under Users in the User Management section of the ARO console\nNote: On initial login users do NOT have elevated access\nAdd administrative users or groups Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OpenShift roles. This can be accomplished at the user or group level.\nTo elevate a users permissions, select the user in the OpenShift console and click Create Binding from the RoleBindings tab\nChoose the scope (namespace/cluster), assign a name to the RoleBinding, and choose a role.\nAfter clicking Create the assigned user will have elevated access once they log in.\nTo elevate a groups permissions, create a group in the OpenShift console.\nEdit the group YAML to specify a custom name and initial user set\nCreate a RoleBinding for the group, similar to what was configured previously for an individual user\nAdd additional users to the YAML file as needed and they will assume the elevated access\n","description":"","tags":null,"title":"Configure GitLab as an identity provider for ARO","uri":"/docs/idp/gitlab-aro/"},{"content":"Prerequisites an Azure Red Hat OpenShift cluster a DNS zone that you can easily modify Get Started Create some environment variables\nDOMAIN=custom.azure.mobb.ninja EMAIL=example@email.com SCRATCH_DIR=/tmp/aro Create a certificate for the ingress controller\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" Create a secret for the certificate\noc create secret tls custom-tls \\ -n openshift-ingress \\ --cert=$SCRATCH_DIR/config/live/$DOMAIN/fullchain.pem \\ --key=$SCRATCH_DIR/config/live/$DOMAIN/privkey.pem Create an ingress controller\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: custom namespace: openshift-ingress-operator spec: domain: $DOMAIN nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" routeSelector: matchLabels: type: custom defaultCertificate: name: custom-tls httpEmptyRequestsPolicy: Respond httpErrorCodePages: name: \"\" replicas: 3 EOF NOTE: By default the ingress controller is created with external scope. This means that the corresponding Azure Load Balancer will have a public frontend IP. If you wish to deploy a privately visible ingress controller add the following lines to the spec:\nspec: ... endpointPublishingStrategy: loadBalancer: scope: Internal type: LoadBalancerService ... Wait a few moments then get the EXTERNAL-IP of the new ingress controller\noc get -n openshift-ingress svc router-custom In case of an Externally (publicly) scoped ingress controller the output should look like:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.90.84 20.120.48.78 80:32160/TCP,443:32511/TCP 49s In case of an Internal (private) one:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.55.36 10.0.2.4 80:30475/TCP,443:30249/TCP 10s Optionally verify in the Azure portal or using CLI that the Load Balancer Service has gotten the new Frontend IP and two Load Balancing Rules - one for port 80 and another one for port 443. In case of an Internally scoped Ingress Controller the changes are to be observed within the Load Balancer that has the -internal suffix.\nCreate a wildcard DNS record pointing at the EXTERNAL-IP\nTest that the Ingress is working\nNOTE: For the Internal ingress controller, make sure that the test host has the necessary reachability to the VPC/subnet as well as the DNS resolver.\ncurl -s https://test.$DOMAIN | head \u003chtml\u003e \u003chead\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e Create a new project to deploy an application to\noc new-project demo Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Expose\ncat \u003c\u003c EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift type: custom name: hello-openshift-tls spec: host: hello.$DOMAIN port: targetPort: 8080-tcp tls: termination: edge insecureEdgeTerminationPolicy: Redirect to: kind: Service name: hello-openshift EOF Verify it works\ncurl https://hello.custom.azure.mobb.ninja Hello OpenShift! ","description":"","tags":["ARO","Azure"],"title":"Adding an additional ingress controller to an ARO cluster","uri":"/docs/aro/additional-ingress-controller/"},{"content":"The following instructions will detail how to configure GitLab as the identity provider for Managed OpenShift through the OpenShift Cluster Manager (OCM):\nCreate OAuth callback URL in OCM Register a new application in GitLab Configure the identity provider credentials and URL Add cluster-admin or dedicated-admin users Log in and confirm Create OAuth callback URL in OCM Log in to the OpenShift Cluster Manager (OCM) to add a GitLab identity provider\nSelect your cluster in OCM and then go to the ‘Access control’ tab and select ‘Identity Providers’\nChoose GitLab as identity provider from the identity providers list\nProvide a name for the new identity provider\nCopy the OAuth callback URL. It will be needed later\nNote: the OAuth Callback has the following format:\nhttps://oauth-openshift.apps.\u003ccluster_name\u003e.\u003ccluster_domain\u003e/oauth2callback/\u003cidp_name\u003e At this point, leave the Client ID, Client secret, and URL blank while configuring GitLab\nRegister a new application in GitLab Log into GitLab and execute the following steps:\nGo to Preferences\nSelect Applications from the left navigation bar\nProvide a Name and enter the OAuth Callback URL copied from OCM above and enter it as the Redirect URL in GitLab\nCheck the openid box and save the application\nAfter saving the GitLab application you will be provided with an Application ID and a Secret\nCopy both the Application ID and Secret and return to the OCM console\nConfigure the identity provider credentials and URL Returning to the OCM console, enter the Application ID and Secret obtained from GitLab in the previous step and enter them as Client ID and Client Secret respectively in the OCM console. Additionally, provide the GitLab URL where credentials were obtained and click Add\nThe new GitLab identity provider should display in the IDP list\nAdd cluster-admin or dedicated-admin users Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OCM and OpenShift roles. Under Cluster Roles and Access select Add user and enter an existing GitLab user. Then choose to assign dedicated-admin or cluster-admin permissions to the user and click Add user\nThe new user should now display, with proper permissions, in the cluster-admin or dedicated-admin user lists\nLog in and confirm Select the Open console button in OCM to bring up the OpenShift login page. An option for GitLab should now be available.\nNote: I can take 1-2 minutes for this update to occur\nAfter selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm.\nCongratulations!\n","description":"","tags":null,"title":"Configure GitLab as an identity provider for ROSA/OSD","uri":"/docs/idp/gitlab/"},{"content":"This document will take you through deploying 3scale in any OSD or ROSA cluster. Review the official documentation here for more information or how to further customize or use 3scale.\nPrerequisites An existing ROSA or OSD cluster Access to an AWS account with permissions to create S3 buckets, IAM users, and IAM policies A subscription for 3scale API Management A wildcard domain configured with a CNAME to your cluster’s ingress controller Prepare AWS Account Set environment variables (ensuring you update the variables appropriately!)\nexport S3_BUCKET=mobb-3scale-bucket export REGION=us-east-1 export S3_IAM_USER_NAME=mobb-3scale-user export S3_IAM_POLICY_NAME=3scale-s3-access export AWS_PAGER=\"\" export PROJECT_NAME=3scale-example export WILDCARD_DOMAIN=3scale.example.com Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Apply the proper S3 bucket CORS configuration\naws s3api put-bucket-cors --bucket $S3_BUCKET --cors-configuration \\ '{ \"CORSRules\": [{ \"AllowedMethods\": [ \"GET\" ], \"AllowedOrigins\": [ \"https://*\" ] }] }' Create an IAM policy for access to the S3 bucket\nPOLICY_ARN=$(aws iam create-policy --policy-name \"$S3_IAM_POLICY_NAME\" \\ --output text --query \"Policy.Arn\" \\ --policy-document \\ '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::'$S3_BUCKET'\", \"arn:aws:s3:::'$S3_BUCKET'/*\" ] } ] }') Create an IAM user to access the S3 bucket\naws iam create-user --user-name $S3_IAM_USER_NAME Generate an access key for the newly created S3 user\nACCESS_CREDS=$(aws iam create-access-key --user-name $S3_IAM_USER_NAME \\ --output text --query \"AccessKey.[AccessKeyId, SecretAccessKey]\") Apply the IAM policy to the newly created S3 user\naws iam attach-user-policy --user-name $S3_IAM_USER_NAME \\ --policy-arn $POLICY_ARN Install the 3Scale API Management Operator Create a new project to install 3Scale API Management into.\noc new-project $PROJECT_NAME Inside of the OpenShift Web Console, navigate to Operators -\u003e OperatorHub.\nSearch for “3scale” and select the “Red Hat Integration - 3scale” Operator.\nClick “Install” and select the project you wish to install the operator into.\nFor this example, I’m deploying into the “3scale-example” project that I have just created.\nOnce the 3Scale operator successfully installs, return to your terminal.\nDeploy 3Scale API Management Create a secret that contains the Amazon S3 configuration.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: creationTimestamp: null name: aws-auth stringData: AWS_ACCESS_KEY_ID: \"$(echo $ACCESS_CREDS | cut -f 1)\" AWS_SECRET_ACCESS_KEY: \"$(echo $ACCESS_CREDS | cut -f 2)\" AWS_BUCKET: \"$S3_BUCKET\" AWS_REGION: \"$REGION\" type: Opaque EOF Create an APIManager custom resource\ncat \u003c\u003c EOF | oc apply -f - echo 'apiVersion: apps.3scale.net/v1alpha1 kind: APIManager metadata: name: example-apimanager spec: wildcardDomain: '$WILDCARD_DOMAIN' system: fileStorage: simpleStorageService: configurationSecretRef: name: aws-auth EOF Once the APIManager instance becomes available, you can login to the 3Scale Admin (located at https://3scale-admin.$WILDCARD_DOMAIN) using the credentials from the below commands:\noc get secret system-seed -o jsonpath={.data.ADMIN_USER} | base64 -d oc get secret system-seed -o jsonpath={.data.ADMIN_PASSWORD} | base64 -d Congratulations! You’ve successfully deployed 3Scale API Management to ROSA/OSD.\n","description":"","tags":["ROSA","OSD"],"title":"Deploying 3scale API Management to ROSA and OSD","uri":"/docs/redhat/3scale/"},{"content":" This document will take you through deploying ACM Observability on a ROSA cluster. see here for the original documentation.\nPrerequisites An existing ROSA cluster An Advanced Cluster Management (ACM) deployment Set up environment Set environment variables\nexport CLUSTER_NAME=my-cluster export S3_BUCKET=$CLUSTER_NAME-acm-observability export REGION=us-east-2 export NAMESPACE=open-cluster-management-observability export SA=tbd export SCRATCH_DIR=/tmp/scratch export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Prepare AWS Account Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Create a Policy for access to S3\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:CreateBucket\", \"s3:DeleteBucket\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy\nS3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create service account\naws iam create-user --user-name $CLUSTER_NAME-acm-obs \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name $CLUSTER_NAME-acm-obs \\ --policy-arn ${S3_POLICY} Create Access Keys\nread -r ACCESS_KEY_ID ACCESS_KEY \u003c \u003c(aws iam create-access-key \\ --user-name $CLUSTER_NAME-acm-obs \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) ACM Hub Log into the OpenShift cluster that is running your ACM Hub. We’ll set up Observability here\nCreate a namespace for the observability\noc new-project $NAMESPACE Generate a pull secret (this will check if the pull secret exists, if not, it will create it)\nDOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-` || \\ DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-` \u0026\u0026 \\ oc create secret generic multiclusterhub-operator-pull-secret \\ -n open-cluster-management-observability \\ --from-literal=.dockerconfigjson=\"$DOCKER_CONFIG_JSON\" \\ --type=kubernetes.io/dockerconfigjson Create a Secret containing your S3 details\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: thanos-object-storage namespace: open-cluster-management-observability type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: $S3_BUCKET endpoint: s3.$REGION.amazonaws.com signature_version2: false access_key: $ACCESS_KEY_ID secret_key: $ACCESS_KEY EOF Create a CR for MulticlusterHub\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: observability.open-cluster-management.io/v1beta2 kind: MultiClusterObservability metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: name: thanos-object-storage key: thanos.yaml EOF Access ACM Observability Log into Advanced Cluster management and access the new Grafana dashboard ","description":"","tags":["Observability","ROSA","ACM"],"title":"Advanced Cluster Management Observability on ROSA","uri":"/docs/redhat/acm/observability/rosa/"},{"content":"","description":"","tags":null,"title":"Observability","uri":"/tags/observability/"},{"content":"Federating Metrics from ROSA/OSD is a bit tricky as the cluster metrics require pulling from its /federated endpoint while the user workload metrics require using the prometheus remoteWrite configuration.\nThis guide will walk you through using the MOBB Helm Chart to deploy the necessary agents to federate the metrics into AWS Prometheus and then use Grafana to visualize those metrics.\nAs a bonus it will set up a CloudWatch datasource to view any metrics or logs you have in Cloud Watch.\nMake sure to use a region where Amazon Prometheus service is supported\nPrerequisites A ROSA cluster deployed with STS aws CLI jq Set up environment Create environment variables\nexport CLUSTER=my-cluster export REGION=us-east-2 export PROM_NAMESPACE=custom-metrics export PROM_SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create namespace\noc new-project $PROM_NAMESPACE Deploy Operators Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n $PROM_NAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-aws-prometheus/files/operatorhub.yaml Deploy and Configure the AWS Sigv4 Proxy and the Grafana Agent Create a Policy for access to AWS Prometheus\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/PermissionPolicyIngest.json { \"Version\": \"2012-10-17\", \"Statement\": [ {\"Effect\": \"Allow\", \"Action\": [ \"aps:RemoteWrite\", \"aps:GetSeries\", \"aps:GetLabels\", \"aps:GetMetricMetadata\" ], \"Resource\": \"*\" } ] } EOF Apply the Policy\nPROM_POLICY=$(aws iam create-policy --policy-name $PROM_SA-prom \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyIngest.json \\ --query 'Policy.Arn' --output text) echo $PROM_POLICY Create a Policy for access to AWS CloudWatch\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/PermissionPolicyCloudWatch.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadingMetricsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:DescribeAlarmsForMetric\", \"cloudwatch:DescribeAlarmHistory\", \"cloudwatch:DescribeAlarms\", \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingLogsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"logs:DescribeLogGroups\", \"logs:GetLogGroupFields\", \"logs:StartQuery\", \"logs:StopQuery\", \"logs:GetQueryResults\", \"logs:GetLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingTagsInstancesRegionsFromEC2\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingResourcesForTags\", \"Effect\": \"Allow\", \"Action\": \"tag:GetResources\", \"Resource\": \"*\" } ] } EOF Apply the Policy\nCW_POLICY=$(aws iam create-policy --policy-name $PROM_SA-cw \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyCloudWatch.json \\ --query 'Policy.Arn' --output text) echo $CW_POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${PROM_NAMESPACE}:${PROM_SA}\", \"system:serviceaccount:${PROM_NAMESPACE}:grafana-serviceaccount\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch\nPROM_ROLE=$(aws iam create-role \\ --role-name \"prometheus-$CLUSTER\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $PROM_ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY Create an AWS Prometheus Workspace\nPROM_WS=$(aws amp create-workspace --alias $CLUSTER \\ --query \"workspaceId\" --output text) echo $PROM_WS Deploy AWS Prometheus Proxy Helm Chart\nhelm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Configure remoteWrite for user workloads\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://aws-prometheus-proxy.$PROM_NAMESPACE.svc.cluster.local:8005/workspaces/$PROM_WS/api/v1/remote_write\" EOF Verify Metrics are being collected Access Grafana and check for metrics\noc get route -n custom-metrics grafana-route -o jsonpath='{.status.ingress[0].host}' Browse to the URL provided in the above command and log in with your OpenShift Credentials\nEnable Admin by hitting sign in and user admin and password\nBrowse to /datasources and verify that cloudwatch and prometheus are present\nIf not, you may have hit a race condition that can be fixed by running the following then trying again\nkubectl delete grafanadatasources.integreatly.org aws-prometheus-proxy-prometheus helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Browse to /dashboards and select the custom-metrics-\u003eNodeExporter / Use Method / Cluster dashboard\nCleanup Delete the aws-prometheus-proxy Helm Release\nhelm delete -n custom-metrics aws-prometheus-proxy Delete the custom-metrics-operators Helm Release\nhelm delete -n custom-metrics custom-metrics-operators Delete the custom-metrics namespace\nkubectl delete namespace custom-metrics Detach AWS Role Policies\naws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY Delete the custom Cloud Watch Policy\naws iam delete-policy --policy-arn $CW_POLICY Delete the AWS Prometheus Role\naws iam delete-role --role-name \"prometheus-$CLUSTER\" Delete AWS Prometheus Workspace\naws amp delete-workspace --workspace-id $PROM_WS ","description":"","tags":["AWS","ROSA"],"title":"ROSA - Federating Metrics to AWS Prometheus","uri":"/docs/rosa/cluster-metrics-to-aws-prometheus/"},{"content":"This guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Azure AD as an OIDC identity provider for ROSA/OSD guide.\nTo set up group synchronization from Azure Active Directory (AD) to ROSA/OSD you must:\nDefine groups and assign users in Azure AD Add the required API permissions to the app registration in Azure AD Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Azure AD To synchronize groups and users with ROSA/OSD they must exist in Azure AD\nCreate groups to syncronize with ROSA/OSD if they do not already exist\nCreate user IDs to synchronize with ROSA/OSD if they do not already exist\nAssign newly created users to the appropriate group\nAdd API Permissions to Azure AD App Registration The GroupSync job requires permissions on the Azure AD tenant beyond those of the OIDC IdP. For it to work, add the these entries:\nGroup.Read.All GroupMember.Read.All User.Read.All ..under the ‘API Permissions’ menu item. These three should all be ‘Application’ rather than ‘Delegated’ and this will require clicking on ‘Grant admin consent’ button above the permissions list. When done, the screen should look like this: Install the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator\nInstall the operator in the group-sync-operator namespace\nCreate and configure a new Group Sync instance Create a new secret named azure-group-sync in the group-sync-operator namespace. For this you will need the following values:\nAZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET Using the OpenShift CLI, create the secret using the following format:\noc create secret generic azure-group-sync \\ --from-literal=AZURE_TENANT_ID=\u003cinsert-id\u003e \\ --from-literal=AZURE_CLIENT_ID=\u003cinsert-id\u003e \\ --from-literal=AZURE_CLIENT_SECRET=\u003cinsert-secret\u003e Create a new Group Sync instance in the group-sync-operator namespace\nSelect all the default YAML and replace is with a modified version of the the example below, customizingthe YAML to match the group names and save the configuration.\nSample YAML:\napiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: azure-groupsync namespace: group-sync-operator spec: providers: - name: azure azure: credentialsSecret: name: azure-group-sync namespace: group-sync-operator groups: - rosa_admin - rosa_project_owner - rosa_viewer prune: false schedule: '* * * * *' Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after.\nThe schedule setting of schedule: * * * * * would result in synchronization occuring every minute. It also supports the cron “slash” notation (e.g., “*/5 * * * *”, which would synchronize every five minutes).\nTesting the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message\nCheck to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list\nValidate that all users specified in Azure AD also show up as members of the associated group in ROSA/OSD\nAdd a new user in Azure AD and assign it to the admin group\nVerify that the user now appears in ROSA/OSD (after the specified synchronization time)\nNow delete a user from the Azure AD admin group\nVerify the user has been deleted from the ROSA/OSD admin group\nBinding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.\nAdditional Notes The prune key in the YAML controls how the sync handles groups that are removed from Azure AD. If they key isn’t present, the default value is false, which means that if a group is removed from Azure AD, it will still persist in OpenShift. If it is set to true, removal of a group from Azure AD will also remove the corresponding OpenShift Group.\nIf there is a need to have multiple GroupSync configurations against multiple providers, note that there is no “merge” functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., azure-ocp-admins or something like contoso_ocp_admins in the case of multiple Azure AD providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.\n","description":"","tags":["Azure","ROSA"],"title":"Using Group Sync Operator with Azure Active Directory and ROSA","uri":"/docs/idp/az-ad-grp-sync/"},{"content":"Currently, the logging-addon is not working on ROSA STS clusters. This is due to permissions missing from the Operator itself. This is a work around to provide credentials to the addon.\nNote: Please see the official Red Hat KCS for more information.\nPrerequisites An STS based ROSA Cluster Workaround Uninstall the logging-addon from the cluster\nrosa uninstall addon -c \u003cmycluster\u003e cluster-logging-operator -y Create a IAM Trust Policy document\ncat \u003c\u003c EOF \u003e /tmp/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:GetLogEvents\", \"logs:PutRetentionPolicy\", \"logs:GetLogRecord\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF Create IAM Policy\nPOLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatchAddon\" --policy-document file:///tmp/trust-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\naws iam create-user --user-name RosaCloudWatchAddon \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name RosaCloudWatchAddon \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml)\naws iam create-access-key --user-name RosaCloudWatchAddon export AWS_ID=\u003cfrom above\u003e export AWS_KEY=\u003cfrom above\u003e Create a secret for the addon to use\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: instance namespace: openshift-logging stringData: aws_access_key_id: ${AWS_ID} aws_secret_access_key: ${AWS_KEY} EOF Install the logging-addon from the cluster\nrosa install addon -c \u003cmycluster\u003e cluster-logging-operator -y Accept the defaults (or change them as appropriate)\n? Use AWS CloudWatch: Yes ? Collect Applications logs: Yes ? Collect Infrastructure logs: Yes ? Collect Audit logs (optional): No ? CloudWatch region (optional): I: Add-on 'cluster-logging-operator' is now installing. To check the status run 'rosa list addons -c mycluster' ","description":"","tags":["AWS","ROSA","STS"],"title":"Work Around to fix the issue with the logging-addon on ROSA STS Clusters","uri":"/docs/rosa/sts-cluster-logging-addon/"},{"content":"Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) provide a simple way for the cluster administrator to configure one or more identity providers for their cluster[s] via the OpenShift Cluster Manager (OCM) , while Azure Red Hat OpenShift relies on the internal cluster authentication operator .\nThe identity providers available for use are:\nGitHub GitLab Google LDAP OpenID HTPasswd Configuring Specific Identity Providers ARO GitLab Azure AD Azure AD with Group Claims Azure AD via CLI Azure AD with Red Hat SSO ROSA/OSD GitLab Azure AD Azure AD with Group Claims (ROSA Only) Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD ","description":"","tags":["ROSA","OSD","ARO\"","Azure","AWS"],"title":"Configuring IDP for ROSA, OSD and ARO","uri":"/docs/idp/"},{"content":"This document has been removed as it was written for older ROSA clusters which did not allow for custom Alert Manager configs as a way to provide a second Prometheus with a configurable Alert Manager.\nIf you want to configure custom Alerts, you can upgrade your cluster and follow the steps found at Custom Alerts in ROSA 4.11.x .\nIf you want to federate your metrics to a central location we recommend using one of the following:\nFederating System and User metrics to S3 in Red Hat OpenShift for AWS Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA If you wish to view the old (likely no longer functional) document you can find it in the git history of the mobb.ninja site .\n","description":"","tags":["AWS","ROSA"],"title":"Federating Metrics to a centralized Prometheus Cluster","uri":"/docs/rosa/federated-metrics-prometheus/"},{"content":"Starting with OpenShift 4.11 it is possible to manage alerting rules for user-defined projects . Similarly, in ROSA clusters the OpenShift Administrator can enable a second AlertManager instance in the user workload monitoring namespace which can be used to create such alerts.\nNote: Currently this is not a managed feature of ROSA. Such an implementation may get overwritten if the User Workload Monitoring functionality is toggled off and on using the OpenShift Cluster Manager (OCM). We\nPrerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.11.0 or higher Create Environment Variables Configure User Workload Monitoring to include AlertManager Edit the user workload config to include AlertManager\nNote: If you have other modifications to this config, you will need to hand edit the resource rather than brute forcing it like below.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | alertmanager: enabled: true enableAlertmanagerConfig: true EOF Verify that a new Alert Manager instance is defined\noc -n openshift-user-workload-monitoring get alertmanager NAME VERSION REPLICAS AGE user-workload 0.24.0 2 2m If you want non-admin users to be able to define alerts in their own namespaces you can run the following.\noc -n \u003cnamespace\u003e adm policy add-role-to-user alert-routing-edit \u003cuser\u003e Create a Slack webhook integration in your Slack workspace. Create environment variables for it.\nSLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' Update the Alert Manager Configuration file\nThis will create a basic AlertManager configuration to send alerts to your slack channel. If you have an existing configuration you’ll need to edit it, not brute force paste like shown.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: alertmanager-user-workload namespace: openshift-user-workload-monitoring stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: Default group_by: [alertname] receivers: - name: Default slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true EOF Create an Example Alert Create a Namespace for your custom alert\noc create namespace custom-alert Verify it works by creating a Prometheus Rule that will fire off an alert\ncat \u003c\u003c EOF | oc apply -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules namespace: custom-alert spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Log into your OpenShift Console and switch to the Developer view. Select the Observe item from the menu on the left and then click Alerts. Make your your Project is set to custom-alert\nCheck the Alert was sent to Slack\nWhat about Cluster Alerts ? By default, cluster alerts are only sent to Red Hat SRE and you cannot modify the cluster alert receivers. However you can make a copy of any of the alerts that you wish to see in your own alerting namespace which will then get picked up by the user workload alert-manager.\nDuplicate a subset of the Cluster Monitoring Prometheus Rules\noc -n openshift-monitoring get prometheusrule \\ cluster-monitoring-operator-prometheus-rules -o json | \\ jq '.metadata.namespace = \"custom-alert\"' | \\ kubectl create -f - Check the Alerts in the custom-alert Project in the OpenShift Console, and you’ll now see the Watchdog and other cluster alerts.\nYou should also have received an alert for the Watchdog alert in your Slack channel.\n","description":"","tags":["AWS","ROSA"],"title":"Custom Alerts in ROSA 4.11.x","uri":"/docs/rosa/custom-alertmanager/"},{"content":"In this example we will deploy the Amazon Ingress Controller that uses ALBs, and configure it to use STS authentication.\nDeployment Configure STS Make sure your cluster has the pod identity webhook\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Download the IAM Policy for the AWS Load Balancer Hooks\nwget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json Create AWS Role with inline policy\naws iam create-role \\ --role-name AWSLoadBalancerController --query Policy.Arn --output text Create AWS Policy and Service Account\nPOLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam_policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\nNote I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out.\nSA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key\nACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user\nPaste the AccessKeyId and SecretAccessKey into values.yaml\ntag your public subnet with ``\nCreate a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml ","description":"","tags":["AWS","ROSA","STS"],"title":"Extending ROSA STS to include authentication with AWS Services","uri":"/docs/rosa/using-sts-with-aws-services/"},{"content":"Prerequisites ROSA CLI AWS CLI ROSA Cluster with STS ","description":"","tags":["AWS","ROSA","STS"],"title":"Integrating with AWS resources using Pod Identity","uri":"/docs/rosa/sts-and-pod-identity/"},{"content":"This document shows how you can use the AWS Cloud Watch agent to scrape Prometheus endpoints and publish metrics to CloudWatch in a Red Hat OpenShift Container Platform (ROSA) cluster.\nIt pulls from The AWS documentation for installing the CloudWatch agent to Kubernetes and collections and publishes metrics for the Kubernetes API Server and provides a simple Dashboard to view the results.\nCurrently the AWS Cloud Watch Agent does not support pulling all metrics from the Prometheus federated endpoint, but the hope is that when it does we can ship all Cluster and User Workload metrics to CloudWatch.\nPrerequisites AWS CLI jq A ROSA Cluster Prepare AWS Account Turn off AWS CLI Paging\nexport AWS_PAGER=\"\" Set some environment variables\nChange these to suit your environment.\nexport CLUSTER_NAME=metrics export CLUSTER_REGION=us-east-2 export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create an AWS IAM User for Cloud Watch\naws iam create-user \\ --user-name $CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH_DIR/aws-user.json Fetch Access and Secret Keys for IAM User\naws iam create-access-key \\ --user-name $CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH_DIR/aws-access-key.json Attach Policy to AWS IAM User\naws iam attach-user-policy \\ --user-name $CLUSTER_NAME-cloud-watch \\ --policy-arn \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\" Deploy Cloud Watch Prometheus Agent Create a namespace for Cloud Watch\noc create namespace amazon-cloudwatch Download the Cloud Watch Agent Kubernetes manifests\nwget -O $SCRATCH_DIR/cloud-watch.yaml https://mobb.ninja/docs/rosa/metrics-to-cloudwatch-agent/cloud-watch.yaml Update the Cloud Watch Agent Kubernetes manifests\nsed -i .bak \"s/__cluster_name__/$CLUSTER_NAME/g\" $SCRATCH_DIR/cloud-watch.yaml sed -i .bak \"s/__cluster_region__/$CLUSTER_REGION/g\" $SCRATCH_DIR/cloud-watch.yaml Provide AWS Creds to the Cloud Watch Agent\nAWS_ID=`cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.AccessKeyId'` AWS_KEY=`cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` echo \"[AmazonCloudWatchAgent]\\naws_access_key_id = $AWS_ID\\naws_secret_access_key = $AWS_KEY\" \\ \u003e $SCRATCH_DIR/credentials oc --namespace amazon-cloudwatch \\ create secret generic aws-credentials \\ --from-file=credentials=$SCRATCH_DIR/credentials Allow Cloud Watch Agent to run as Root user (inside the container)\noc -n amazon-cloudwatch adm policy \\ add-scc-to-user anyuid -z cwagent-prometheus Apply the Cloud Watch Agent Kubernetes manifests\noc apply -f $SCRATCH_DIR/cloud-watch.yaml Check the Pod is running\noc get pods -n amazon-cloudwatch You should see:\nNAME READY STATUS RESTARTS AGE cwagent-prometheus-54cd498c9c-btmjm 1/1 Running 0 60m Create Sample Dashboard Download the Sample Dashboard\nwget -O $SCRATCH_DIR/dashboard.json https://raw.githubusercontent.com/rh-mobb/documentation/main/content/docs/rosa/metrics-to-cloudwatch-agent/dashboard.json Update the Sample Dashboard\nsed -i .bak \"s/__CLUSTER_NAME__/$CLUSTER_NAME/g\" $SCRATCH_DIR/dashboard.json sed -i .bak \"s/__REGION_NAME__/$CLUSTER_REGION/g\" $SCRATCH_DIR/dashboard.json Browse to https://us-east-2.console.aws.amazon.com/cloudwatch Create a Dashboard, call it “Kubernetes API Server”\nClick Actions-\u003eView/edit source\nPaste the JSON contents from $SCRATCH_DIR/dashboard.json into the text area\nView the dashboard\n","description":"","tags":["AWS","ROSA"],"title":"Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA","uri":"/docs/rosa/metrics-to-cloudwatch-agent/"},{"content":"Registering an ARO cluster to OpenShift Cluster Manager ARO clusters do not come connected to OpenShift Cluster Manager by default, because Azure would like customers to specifically opt-in to connections / data sent outside of Azure. This is the case with registering to OpenShift cluster manager, which enables a telemetry service in ARO.\nPrerequisites An Red Hat account. If you have any subscriptions with Red Hat, you will have a Red Hat account. If not, then you can create an account easily at https://cloud.redhat.com . Steps Login to https://console.redhat.com with you Red Hat account.\nGo to https://console.redhat.com/openshift/downloads and download your pull-secret file. This is a file that includes an authentication for cloud.openshift.com which is used by OpenShift Cluster Manager.\nFollow the Update pull secret instructions to merge your pull-secret (in particular cloud.openshift.com) in your ARO pull secret. Be careful not to overwrite the ARO cluster pull secrets that come by default - it explains how in that article.\nAfter waiting a few minutes (but it could be up to an hour), your cluster should be automatically registered in this list in OpenShift Cluster Manager; https://console.redhat.com/openshift You can check the cluster ID within the Cluster Overview section of the admin console with the ID of the cluster in OCM to make sure the right cluster is registered.\nThe cluster will appear as a 60-day self-supported evaluation cluster. However, again, wait about an hour (but in this case, it can take up to 24 hours), and the cluster will be automatically updated to an ARO type cluster, with full support. You don’t need to change the support level yourself.\nThis makes the cluster a fully supported cluster within the Red Hat cloud console, with access to raise support tickets, also.\n","description":"","tags":["ARO","Azure"],"title":"Registering an ARO cluster to OpenShift Cluster Manager","uri":"/docs/aro/ocm/"},{"content":"The HashiCorp Vault Secret CSI Driver allows you to access secrets stored in HashiCorp Vault as Kubernetes Volumes.\nPrerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.3.2 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\noc -n k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Install HashiCorp Vault with CSI driver enabled Add the HashiCorp Helm Repository\nhelm repo add hashicorp https://helm.releases.hashicorp.com Update your Helm Repositories\nhelm repo update Create a namespace for Vault\noc new-project hashicorp-vault Create a SCC for the CSI driver\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Create a values file for Helm to use\ncat \u003c\u003c EOF \u003e values.yaml global: openshift: true csi: enabled: true daemonSet: providersDir: /var/run/secrets-store-csi-providers injector: enabled: false server: image: repository: \"registry.connect.redhat.com/hashicorp/vault\" tag: \"1.8.0-ubi\" dev: enabled: true EOF Install Hashicorp Vault with CSI enabled\nhelm install -n hashicorp-vault vault \\ hashicorp/vault --values values.yaml Patch the CSI daemonset\nCurrently the CSI has a bug in its manifest which we need to patch\nkubectl patch daemonset vault-csi-provider --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/securityContext\", \"value\": {\"privileged\": true} }]' Configure Hashicorp Vault Get a bash prompt inside the Vault pod\noc exec -it vault-0 -- bash Create a Secret in Vault\nvault kv put secret/db-pass password=\"hunter2\" Configure Vault to use Kubernetes Auth\nvault auth enable kubernetes Check your Cluster’s token issuer\noc get authentication.config cluster \\ -o json | jq -r .spec.serviceAccountIssuer Configure Kubernetes auth method\nIf the issuer here does not match the above, update it.\nvault write auth/kubernetes/config \\ issuer=\"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy for our app\nvault policy write internal-app - \u003c\u003cEOF path \"secret/data/db-pass\" { capabilities = [\"read\"] } EOF Create an auth role to access it\nvault write auth/kubernetes/role/database \\ bound_service_account_names=webapp-sa \\ bound_service_account_namespaces=default \\ policies=internal-app \\ ttl=20m exit from the vault-0 pod\nexit Deploy a sample application Create a SecretProviderClass in the default namespace\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: vault-database namespace: default spec: provider: vault parameters: vaultAddress: \"http://vault.hashicorp-vault:8200\" roleName: \"database\" objects: | - objectName: \"db-password\" secretPath: \"secret/data/db-pass\" secretKey: \"password\" EOF Create a service account webapp-sa\nkubectl create serviceaccount -n default webapp-sa Create a Pod to use the secret\ncat \u003c\u003c EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: webapp namespace: default spec: serviceAccountName: webapp-sa containers: - image: jweissig/app:0.0.1 name: webapp volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"vault-database\" EOF Check the Pod has the secret\nkubectl -n default exec webapp \\ -- cat /mnt/secrets-store/db-password The output should match\nhunter2 Uninstall HashiCorp Vault with CSI driver enabled Delete the pod and\nkubectl delete -n default pod webapp kubectl delete -n default secretproviderclass vault-database kubectl delete -n default serviceaccount webapp-sa Delete the Hashicorp Vault Helm\nhelm delete -n hashicorp-vault vault Delete the SCC for Hashicorp Vault\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Delete the Hashicorp vault project\noc delete project hashicorp-vault Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver ","description":"","tags":["ROSA","ARO","OSD","OCP"],"title":"Installing the HashiCorp Vault Secret CSI Driver","uri":"/docs/misc/secrets-store-csi/hashicorp-vault/"},{"content":"The Kubernetes Secret Store CSI is a storage driver that allows you to mount secrets from external secret management systems like HashiCorp Vault and AWS Secrets.\nIt comes in two parts, the Secret Store CSI, and a Secret provider driver. This document covers just the CSI itself.\nPrerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.3.2 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\noc -n k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Provider Specifics HashiCorp Vault ","description":"","tags":["ARO","ROSA","OSD","OCP"],"title":"Installing the Kubernetes Secret Store CSI on OpenShift","uri":"/docs/misc/secrets-store-csi/"},{"content":"This document is adapted from the Azure Key Vault CSI Walkthrough specifically to run with Azure Red Hat OpenShift (ARO).\nPrerequisites An ARO cluster The AZ CLI (logged in) The OC CLI (logged in) Helm 3.x CLI Environment Variables Run this command to set some environment variables to use throughout\nNote if you created the cluster from the instructions linked above these will re-use the same environment variables, or default them to openshift and eastus.\nexport KEYVAULT_RESOURCE_GROUP=${AZR_RESOURCE_GROUP:-\"openshift\"} export KEYVAULT_LOCATION=${AZR_RESOURCE_LOCATION:-\"eastus\"} export KEYVAULT_NAME=secret-store-$(cat /dev/urandom | LC_ALL=C tr -dc 'a-zA-Z0-9' | fold -w 10 | head -n 1) export AZ_TENANT_ID=$(az account show -o tsv --query tenantId) Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.3.2 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\noc -n k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Deploy Azure Key Store CSI Add the Azure Helm Repository\nhelm repo add csi-secrets-store-provider-azure \\ https://azure.github.io/secrets-store-csi-driver-provider-azure/charts Update your local Helm Repositories\nhelm repo update Install the Azure Key Vault CSI provider\nhelm install -n k8s-secrets-store-csi azure-csi-provider \\ csi-secrets-store-provider-azure/csi-secrets-store-provider-azure \\ --set linux.privileged=true --set secrets-store-csi-driver.install=false \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" \\ --version=v1.4.1 Set SecurityContextConstraints to allow the CSI driver to run\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:csi-secrets-store-provider-azure Create Keyvault and a Secret Create a namespace for your application\noc new-project my-application Create an Azure Keyvault in your Resource Group that contains ARO\naz keyvault create -n ${KEYVAULT_NAME} \\ -g ${KEYVAULT_RESOURCE_GROUP} \\ --location ${KEYVAULT_LOCATION} Create a secret in the Keyvault\naz keyvault secret set \\ --vault-name ${KEYVAULT_NAME} \\ --name secret1 --value \"Hello\" Create a Service Principal for the keyvault\nNote: If this gives you an error, you may need upgrade your Azure CLI to the latest version.\nexport SERVICE_PRINCIPAL_CLIENT_SECRET=\"$(az ad sp create-for-rbac \\ --name http://$KEYVAULT_NAME --query 'password' -otsv)\" export SERVICE_PRINCIPAL_CLIENT_ID=\"$(az ad sp list \\ --display-name http://$KEYVAULT_NAME --query '[0].appId' -otsv)\" Set an Access Policy for the Service Principal\naz keyvault set-policy -n ${KEYVAULT_NAME} \\ --secret-permissions get \\ --spn ${SERVICE_PRINCIPAL_CLIENT_ID} Create and label a secret for Kubernetes to use to access the Key Vault\noc create secret generic secrets-store-creds \\ -n my-application \\ --from-literal clientid=${SERVICE_PRINCIPAL_CLIENT_ID} \\ --from-literal clientsecret=${SERVICE_PRINCIPAL_CLIENT_SECRET} oc -n my-application label secret \\ secrets-store-creds secrets-store.csi.k8s.io/used=true Deploy an Application that uses the CSI Create a Secret Provider Class to give access to this secret\ncat \u003c\u003cEOF | oc apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: azure-kvname namespace: my-application spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"false\" userAssignedIdentityID: \"\" keyvaultName: \"${KEYVAULT_NAME}\" objects: | array: - | objectName: secret1 objectType: secret objectVersion: \"\" tenantId: \"${AZ_TENANT_ID}\" EOF Create a Pod that uses the above Secret Provider Class\ncat \u003c\u003cEOF | oc apply -f - kind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline namespace: my-application spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"azure-kvname\" nodePublishSecretRef: name: secrets-store-creds EOF Check the Secret is mounted\noc exec busybox-secrets-store-inline -- ls /mnt/secrets-store/ Output should match:\nsecret1 Print the Secret\noc exec busybox-secrets-store-inline \\ -- cat /mnt/secrets-store/secret1 Output should match:\nHello Cleanup Uninstall Helm\nhelm uninstall -n k8s-secrets-store-csi azure-csi-provider Delete the app\noc delete project my-application Delete the Azure Key Vault\naz keyvault delete -n ${KEYVAULT_NAME} Delete the Service Principal\naz ad sp delete --id ${SERVICE_PRINCIPAL_CLIENT_ID} Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver ","description":"","tags":["Azure","ARO"],"title":"Azure Key Vault CSI on Azure Red Hat OpenShift","uri":"/docs/misc/secrets-store-csi/azure-key-vault/"},{"content":" This is a combination of the private-link and sts setup documents to show the full picture\nPrerequisites AWS CLI Rosa CLI v1.1.7 jq AWS Preparation If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Create the AWS Virtual Private Cloud (VPC) and Subnets For this scenario, we will be using a newly created VPC with both public and private subnets. All of the cluster resources will reside in the private subnet. The public subnet will be used for traffic to the Internet (egress)\nNote: If you already have a Transit Gateway (TGW) or similar, you can skip the public subnet configuration\nNote: When creating subnets, make sure that subnet(s) are created in availability zones that have ROSA instances types available. If AZ is not “forced”, the subnet is created in a random AZ in the region. Force AZ using the --availability-zone argument in the create-subnet command.\nUse rosa list instance-types to list the ROSA instance types\nUse aws ec2 describe-instance-type-offerings to check that your desired AZ supports your desired instance type\nExample using us-east-1, us-east-1b, and m5.xlarge: aws ec2 describe-instance-type-offerings --location-type availability-zone \\ --filters Name=location,Values=us-east-1b --region us-east-1 \\ --output text | egrep m5.xlarge Result should display INSTANCETYPEOFFERINGS [instance-type] [az] availability-zone if your selected region supports your desired instance type\nConfigure the following environment variables, adjusting for ROSA_CLUSTER_NAME, VERSION and REGION as necessary\nexport VERSION=4.9.15 \\ ROSA_CLUSTER_NAME=pl-sts-cluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-1 \\ AWS_PAGER=\"\" Create a VPC for use by ROSA\nCreate the VPC and return the ID as VPC_ID\nVPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` echo $VPC_ID Tag the newly created VPC with the cluster name\naws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Configure the VPC to allow DNS hostnames for their public IP addresses\naws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames The new VPC should be visible in the AWS console\nCreate a Public Subnet to allow egress traffic to the Internet\nCreate the public subnet in the VPC CIDR block range and return the ID as PUBLIC_SUBNET\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` echo $PUBLIC_SUBNET Tag the public subnet with the cluster name\naws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public Create a Private Subnet for the cluster\nCreate the private subnet in the VPC CIDR block range and return the ID as PRIVATE_SUBNET\nPRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID \\ --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` echo $PRIVATE_SUBNET Tag the private subnet with the cluster name\naws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Both subnets should now be visible in the AWS console\nCreate an Internet Gateway for NAT egress traffic\nCreate the Internet Gateway and return the ID as I_GW\nI_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` echo $I_GW Attach the new Internet Gateway to the VPC\naws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW Tag the Internet Gateway with the cluster name\naws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new Internet Gateway should be created and attached to your VPC\nCreate a Route Table for NAT egress traffic\nCreate the Route Table and return the ID as R_TABLE\nR_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE Create a route with no IP limitations (0.0.0.0/0) to the Internet Gateway\naws ec2 create-route --route-table-id $R_TABLE \\ --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW Verify the route table settings\naws ec2 describe-route-tables --route-table-id $R_TABLE Example output\nAssociate the Route Table with the Public subnet\naws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET \\ --route-table-id $R_TABLE Example output\nTag the Route Table with the cluster name\naws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Create a NAT Gateway for the Private network\nAllocate and elastic IP address and return the ID as EIP\nEIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` echo $EIP Create a new NAT Gateway in the Public subnet with the new Elastic IP address and return the ID as NAT_GW\nNAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` echo $NAT_GW Tag the Elastic IP with the cluster name\naws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new NAT Gateway should be created and associated with your VPC\nCreate a Route Table for the Private subnet to the NAT Gateway\nCreate a Route Table in the VPC and return the ID as R_TABLE_NAT\nR_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE_NAT Loop through a Route Table check until it is created\nwhile ! aws ec2 describe-route-tables \\ --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done Example output! Create a route in the new Route Table for all addresses to the NAT Gateway\naws ec2 create-route --route-table-id $R_TABLE_NAT \\ --destination-cidr-block 0.0.0.0/0 \\ --gateway-id $NAT_GW Associate the Route Table with the Private subnet\naws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET \\ --route-table-id $R_TABLE_NAT Tag the Route Table with the cluster name\naws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Configure the AWS Security Token Service (STS) for use with ROSA The AWS Security Token Service (STS) allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies to gain access to the AWS resources needed to install and operate the cluster.\nThis is a summary of the official OpenShift docs that can be used as a line by line install guide.\nNote that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $region instead or you will fail installation.\nMake you your ROSA CLI version is correct (v1.1.0 or higher)\nrosa version Create the IAM Account Roles\nrosa create account-roles --mode auto --yes Deploy ROSA cluster Run the rosa cli to create your cluster\nrosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} \\ --subnet-ids=$PRIVATE_SUBNET \\ --private-link --machine-cidr=10.0.0.0/16 \\ --sts Confirm the Private Link set up Create the Operator Roles\nrosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider.\nrosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate The cluster is now installing\nThe State should have moved beyond pending and show installing or ready.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing it is time to validate. Validation when using Private Link requires the use of a jump host.\nYou can create them using the AWS Console or the AWS CLI as depicted below:\nOption 1: Create a jump host instance through the AWS Console\nNavigate to the EC2 console and launch a new instance\nSelect the AMI for your instance, if you don’t have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details\nChange the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are fine. Make the following changes in the 6. Configure Security Group tab (either by clicking through the screens or selecting from the top bar)\nIf you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list, otherwise, select Create a new security group and continue.\nTo allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch, verify all settings are correct, and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys.\nOnce launched, open the instance summary for the jump host instance and note the public IP address.\nOption 2: Create a jumphost instance using the AWS CLI\nCreate an additional Security Group for the jumphost\nTAG_SG=\"$ROSA_CLUSTER_NAME-jumphost-sg\" aws ec2 create-security-group --group-name ${ROSA_CLUSTER_NAME}-jumphost-sg --description ${ROSA_CLUSTER_NAME}-jumphost-sg --vpc-id ${VPC_ID} --tag-specifications \"ResourceType=security-group,Tags=[{Key=Name,Value=$TAG_SG}]\" Grab the Security Group Id generated in the previous step\nPublicSecurityGroupId=$(aws ec2 describe-security-groups --filters \"Name=tag:Name,Values=${ROSA_CLUSTER_NAME}-jumphost-sg\" | jq -r '.SecurityGroups[0].GroupId') echo $PublicSecurityGroupId Add a rule to Allow the ssh into the Public Security Group\naws ec2 authorize-security-group-ingress --group-id $PublicSecurityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0 (Optional) Create a Key Pair for your jumphost if your have not a previous one\naws ec2 create-key-pair --key-name $ROSA_CLUSTER_NAME-key --query 'KeyMaterial' --output text \u003e PATH/TO/YOUR_KEY.pem chmod 400 PATH/TO/YOUR_KEY.pem Define an AMI_ID to be used for your jump host\nAMI_ID=\"ami-0022f774911c1d690\" This AMI_ID corresponds an Amazon Linux within the us-east-1 region and could be not available in your region. Find your AMI ID and use the proper ID.\nLaunch an ec2 instance for your jumphost using the parameters defined in early steps:\nTAG_VM=\"$ROSA_CLUSTER_NAME-jumphost-vm\" aws ec2 run-instances --image-id $AMI_ID --count 1 --instance-type t2.micro --key-name $ROSA_CLUSTER_NAME-key --security-group-ids $PublicSecurityGroupId --subnet-id $PUBLIC_SUBNET --associate-public-ip-address --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=$TAG_VM}]\" This instance will be associated with a Public IP directly.\nWait until the ec2 instance is in Running state, grab the Public IP associated to the instance and check the if the ssh port and:\nIpPublicBastion=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=$TAG_VM\" | jq -r '.Reservations[0].Instances[0].PublicIpAddress') echo $IpPublicBastion nc -vz $IpPublicBastion 22 Create a ROSA admin user and save the login command for use later\nrosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed\nrosa describe cluster -c $ROSA_CLUSTER_NAME update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below\n127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP\nsudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP From your EC2 jump instances, download the OC CLI and install it locally\nDownload the OC CLI for Linux wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz Unzip and untar the binary gunzip openshift-client-linux.tar.gz tar -xvf openshift-client-linux.tar log into the cluster using oc login command from the create admin command above. ex.\n./oc login https://api.$YOUR_OPENSHIFT_DNS.p1.openshiftapps.com:6443 --username cluster-admin --password $YOUR_OPENSHIFT_PWD Check that you can access the Console by opening the console url in your browser. Cleanup Delete ROSA\nrosa delete cluster -c $ROSA_CLUSTER_NAME -y Watch the logs and wait until the cluster is deleted\nrosa logs uninstall -c $ROSA_CLUSTER_NAME --watch Clean up the STS roles\nNote you can get the correct commands with the ID filled in from the output of the previous step.\nrosa delete operator-roles -c \u003cid\u003e --mode auto --yes rosa delete oidc-provider -c \u003cid\u003e --mode auto --yes Delete AWS resources\naws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-internet-gateway --internet-gateway-id $I_GW | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq . ","description":"","tags":["AWS","ROSA","STS","Private Link"],"title":"Creating a ROSA cluster with Private Link enabled (custom VPC) and STS","uri":"/docs/rosa/sts-with-private-link/"},{"content":"A guide to shipping logs and metrics on OpenShift\nPrerequisites OpenShift CLI (oc) Rights to install operators on the cluster Setup OpenShift Logging This is for setup of centralized logging on OpenShift making use of Elasticsearch OSS edition. This largely follows the processes outlined in the OpenShift documentation here . Retention and storage considerations are reviewed in Red Hat’s primary source documentation.\nThis setup is primarily concerned with simplicity and basic log searching. Consequently it is insufficient for long-lived retention or for advanced visualization of logs. For more advanced observability setups, you’ll want to look at Forwarding Logs to Third Party Systems Create a namespace for the OpenShift Elasticsearch Operator.\nThis is necessary to avoid potential conflicts with community operators that could send similarly named metrics/logs into the stack.\noc create -f - \u003c\u003cEOF apiVersion: v1 kind: Namespace metadata: name: openshift-operators-redhat annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Create a namespace for the OpenShift Logging Operator\noc create -f - \u003c\u003cEOF apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Install the OpenShift Elasticsearch Operator by creating the following objects:\nOperator Group for OpenShift Elasticsearch Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-operators-redhat namespace: openshift-operators-redhat spec: {} EOF Subscription object to subscribe a Namespace to the OpenShift Elasticsearch Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: \"elasticsearch-operator\" namespace: \"openshift-operators-redhat\" spec: channel: \"stable\" installPlanApproval: \"Automatic\" source: \"redhat-operators\" sourceNamespace: \"openshift-marketplace\" name: \"elasticsearch-operator\" EOF Verify Operator Installation\noc get csv --all-namespaces Example Output\nNAMESPACE NAME DISPLAY VERSION REPLACES PHASE default elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-node-lease elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-public elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-system elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication elasticsearch-operator.5.0. 0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded ... Install the Red Hat OpenShift Logging Operator by creating the following objects:\nThe Cluster Logging OperatorGroup\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging EOF Subscription Object to subscribe a Namespace to the Red Hat OpenShift Logging Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"stable\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Verify the Operator installation, the PHASE should be Succeeded\noc get csv -n openshift-logging Example Output\nNAME DISPLAY VERSION REPLACES PHASE cluster-logging.5.0.5-11 Red Hat OpenShift Logging 5.0.5-11 Succeeded elasticsearch-operator.5.0.5-11 OpenShift Elasticsearch Operator 5.0.5-11 Succeeded Create an OpenShift Logging instance:\nNOTE: For the storageClassName below, you will need to adjust for the platform on which you’re running OpenShift. managed-premium as listed below is for Azure Red Hat OpenShift (ARO). You can verify your available storage classes with oc get storageClasses\noc create -f - \u003c\u003cEOF apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: \"managed-premium\" size: 200G resources: requests: memory: \"8Gi\" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: \"SingleRedundancy\" visualization: type: \"kibana\" kibana: replicas: 1 curation: type: \"curator\" curator: schedule: \"30 3 * * *\" collection: logs: type: \"fluentd\" fluentd: {} EOF It will take a few minutes for everything to start up. You can monitor this progress by watching the pods.\nwatch oc get pods -n openshift-logging Your logging instances are now configured and recieving logs. To view them, you will need to log into your Kibana instance and create the appropriate index patterns. For more information on index patterns, see the Kibana documentation. NOTE: The following restrictions and notes apply to index patterns:\nAll users can view the app- logs for namespaces they have access to Only cluster-admins can view the infra- and audit- logs For best accuracy, use the @timestamp field for determining chronology ","description":"","tags":["Observability","OCP"],"title":"OpenShift Logging","uri":"/docs/o11y/openshift-logging/"},{"content":"This document follows the steps outlined by Microsoft in their documentation Follow docs.\nStep 4, needs additional command of:\naz resource list --resource-type Microsoft.RedHatOpenShift/OpenShiftClusters -o json to capture resource ID of ARO cluster as well, needed for export in step 6\nbash enable-monitoring.sh --resource-id $azureAroV4ClusterResourceId --workspace-id $logAnalyticsWorkspaceResourceId works successfully\ncan verify pods starting\nVerify logs flowing with container solutions showing in log analytics workbook?\nConfigure Prometheus metric scraping following steps outlined here: https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration It looks like config maps are not set in the previous step despite what the article says. This may actually be an OpenShift v3 thing and not a v4 thing. I had to do the apply process after downloading the config.\nAfterward pods did not restart on their own and had to be manually deleted. Automatic recreation pulls in new config and should begins shipping metrics\nVerify metrics with a query: (from https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-log-search#query-prometheus-metrics-data )\nInsightsMetrics | where TimeGenerated \u003e ago(1h) | where Name == 'reads' | extend Tags = todynamic(Tags) | extend HostName = tostring(Tags.hostName), Device = Tags.name | extend NodeDisk = strcat(Device, \"/\", HostName) | order by NodeDisk asc, TimeGenerated asc | serialize | extend PrevVal = iif(prev(NodeDisk) != NodeDisk, 0.0, prev(Val)), PrevTimeGenerated = iif(prev(NodeDisk) != NodeDisk, datetime(null), prev(TimeGenerated)) | where isnotnull(PrevTimeGenerated) and PrevTimeGenerated != TimeGenerated | extend Rate = iif(PrevVal \u003e Val, Val / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1), iif(PrevVal == Val, 0.0, (Val - PrevVal) / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1))) | where isnotnull(Rate) | project TimeGenerated, NodeDisk, Rate | render timechart ","description":"","tags":["Observability","Azure"],"title":"Shipping logs to Azure Log Analytics","uri":"/docs/o11y/az-log-analytics/"},{"content":"This is a high level overview of disaster recovery options for Azure Red Hat OpenShift. It is not a detailed design, but rather a starting point for a more detailed design.\nWhat is Disaster Recovery (DR) Disaster Recovery is an umbrella term that includes the following:\nBackup (and restore!) Failover (and failback!) High Availability Disaster Avoidence The most important part of Disaster Recovery is the “Recovery”. Whatever your DR plan it must be tested and ideally performed on a semi-regular basis.\nYou can use RTO (Recovery Time Objective) and RPO (Recovery Point Objective) to help determine what level of DR is right for your company. These Objectives are often application dependent and may mean choosing full HA for one application, and Backup/Restore for another even if they’re both on the same OpenShift cluster.\nRecovery Time Objective (RTO) How long can your application be down without causing significant issues for your business. This can differ from application to application. Some applications may only affect internal staff while others may affect customers and revenue.\nIn general you will categorize your applications by priority and potential damage to the business and match your DR plans accordingly.\nRecovery Point Objective (RPO) How much data can you lose before significant damage is done to your business. The traditional backup strategy is Daily. If you can survive a loss of 24 hours of data, or you have an alternative way to restore that data then this is often good enough.\nCombined RTO / RPO When combined you will account for “how long can the application be offline” and “how much data can I lose”. If the answer zero or approaching zero for both then your DR strategy must be focussed around High Availabily and real time data replication.\nBackup In OpenShift it is not necessary to back up the cluster itself, but instead you back up the “active state” of your resources, any Persistent Volumes, and any backing services.\nAzure provides documentation on the basic Backup and Restore of the applications running on your ARO cluster.\nAzure also provides documentation on Backing up the various PaaS backing services that you may have connected to your applications such as Azure PostgreSQL .\nFailover An ARO cluster can be deployed into Multiple Availability Zones (AZs) in a single region. To protect your applications from region failure you must deploy your application into multiple ARO clusters across different regions. Here are some considerations:\nStart with a solid and tested Backup/Restore manual cutover DR solution Decide on RPO/RTO (Recovery Point Objective / Recovery Time Objective) for DR Decide whether your regions should be hot/hot, hot/warm, or hot/cold. Choose regions close to your consumers. choose two \u003ccode\u003e\u0026quot;paired\u0026quot;\u003c/code\u003e regions . Use global virtual network peering to connect your networks together. Use Front Door or Traffic Manager to route public traffic to the correct region. Enable geo-replication for container images (If using ACR). Remove service state from inside containers. Create a storage migration plan. Backup and Restore - Manual Cutover ( Hot / Cold ) Do you currently have the ability to do a point in time restore of Backups of your applications?\nCreate a backup of your Kubernetes cluster\nIf you restore these backups to a new cluster and manually cutover the DNS, will your applications be full functional?\nCreate backups of any regionally co-located resources (like Redis, Postgres, etc.).\nSome Azure PaaS services such as Azure Container Registry can replicate to another region which may assist in performing backups or restore. This replication is often one way, therefore a new replication relationship must be created from the new region to another for the next DR event.\nIf using DNS based failover, make sure TTLs are set to a suitable value.\nDetermine if Non-regionally co-located resources (such as SaaS products) have appropriate failover plans and ensure that any special networking arrangements are available at the DR region.\nFailover to an existing cluster in the DR region (Hot / Warm) In a Hot / Warm situation the destination cluster should be similar to the the source cluster, but for financial reasons may be smaller, or be single AZ. If this is the case you may either run the DR cluster with lower expectations on performance and resiliance with the idea of failing back to the original cluster ASAP, or you will expand the DR cluster to match the original cluster and turn the original cluster into the next DR site.\nIdeally Your applications and data should be replicated to the DR site and should be ready to switch over within a very short window.\nHigh Availability ( Hot / Hot ) In a Hot / Hot scenario you have two-way synchronous replication of your data. The end user can access the application in either site and have the exact same experience.\nARO Specific Example This following example will create two ARO clusters, each in its own Region. Virtual Network Peering is used to make it easier for resources to communicate for replication.\nCreate a Primary Cluster Set the following environment variables:\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=ARO-DR-1 AZR_CLUSTER=ARODR1 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster using the above environment variables.\nCreate a Secondary Cluster Set the following environment variables:\nAZR_RESOURCE_LOCATION=centralus AZR_RESOURCE_GROUP=ARO-DR-2 AZR_CLUSTER=ARODR2 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.1.0.0/20 CONTROL_SUBNET=10.1.0.0/24 MACHINE_SUBNET=10.1.1.0/24 FIREWALL_SUBNET=10.1.2.0/24 JUMPHOST_SUBNET=10.1.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster using the above environment variables.\nConnect the clusters via Virtual Network Peering Virtual network peering allows two Azure regions to connect to each other via a virtual network. Ideally you will use a Hub-Spoke topology and create appropriate firewalling in the Hub network but that is an excercise left for the reader and here we’re creating a simple open peering between the two networks.\nGet the ID of the two networks you created in the previous step.\nDR1_VNET=$(az network vnet show \\ --resource-group ARO-DR-1 \\ --name ARODR1-aro-vnet-eastus \\ --query id --out tsv) echo $DR1_VNET DR2_VNET=$(az network vnet show \\ --resource-group ARO-DR-2 \\ --name ARODR2-aro-vnet-centralus \\ --query id --out tsv) echo $DR2_VNET Create peering from the Primary network to the Secondary network.\naz network vnet peering create \\ --name primary-to-secondary \\ --resource-group ARO-DR-1 \\ --vnet-name ARODR1-aro-vnet-eastus \\ --remote-vnet $DR2_VNET \\ --allow-vnet-access Create peering from the Secondary network to the Primary network.\naz network vnet peering create \\ --name secondary-to-primary \\ --resource-group ARO-DR-2 \\ --vnet-name ARODR2-aro-vnet-centralus \\ --remote-vnet $DR1_VNET \\ --allow-vnet-access Verify that the Jump Host in the Primary region is able to reach the Jump Host in the Secondary region.\nssh -i $HOME/.ssh/id_rsa aro@$JUMP_IP ping 10.1.3.4 PING 10.1.3.4 (10.1.3.4) 56(84) bytes of data. 64 bytes from 10.1.3.4: icmp_seq=1 ttl=64 time=23.8 ms 64 bytes from 10.1.3.4: icmp_seq=2 ttl=64 time=23.10 ms Use sshuttle to create a ssh vpn via the jump host (use a separate terminal session)\nreplace the IP with the IP of the jump box from the previous step.\nsshuttle --dns -NHr \"aro@${JUMP_IP}\" 10.0.0.0/8 Login to your cluster console on your browser using the console URL on both clusters to see if they are accessible from one to another.\nFrom here the two clusters are visible to each other via their frontends. This means they can access eachother’s ingress endpoints, routes and Load Balancers, but not pod-to-pod. A PostgreSQL pod in the primary cluster could replicate to a PostgreSQL pod in the secondary cluster via a service of type LoadBalancer.\nCross Region Registry Replication Openshift comes with a local registry that is used for local builds etc, but it is likely that you use a centralized registry for your own applications and images. Ensure that your registry supports replication to the DR region. Ensure that you understand if it supports active/active replication or if its a one way replication.\nIn a Hot/Warm scenario where you’ll only ever use the DR region as a backup to the primary region its likely okay for one-way replication to be used.\nRedhat Quay Azure Container Registry (must use Premium SKU for geo-replication) Example - Create a ACR in the Primary Region Create a new ACR in the primary region.\naz acr create --resource-group ARO-DR-1 \\ --name acrdr1 --sku Premium Log into and push an Image to the ACR.\naz acr login --name acrdr1 podman pull mcr.microsoft.com/hello-world podman tag mcr.microsoft.com/hello-world acrdr1.azurecr.io/hello-world:v1 podman push acrdr1.azurecr.io/hello-world:v1 If you are seeing podman authentication error, you may want to login using following command:\nACR_NAME=\"acrdr1\" podman login \\ --username \"00000000-0000-0000-0000-000000000000\" \\ --password $(az acr login -n \"${ACR_NAME}\" --expose-token --query \"accessToken\" -o tsv) \\ \"${ACR_NAME}.azurecr.io\" Replicate the registry to the DR2 region.\naz acr replication create --location centralus --registry acrdr1 Wait a few moments and then check the replication status.\naz acr replication show --name centralus --registry acrdr1 --query status Red Hat Advanced Cluster Management Advanced Cluster Management (ACM) is a set of tools that can be used to manage the lifecycle of multiple OpenShift clusters. ACM gives you a single view into your clusters and provides gitops style management of you workloads and also has compliance features.\nYou can run ACM from a central infrastructure (or your Primary DR) cluster and connect your ARO clusters to it.\nFailover for Application Ingress If you want to expose your Applications to the internet you can use Azure’s Front Door or Traffic Manager resources which you can use to fail the routing over to the DR site.\nHowever if you are running private clusters your choices are a bit more limited.\nYou could provide people with the application ingress URL for each cluster and expect them to know to use the DR one if the Primary is down You can add a custom domain (and TLS certificate) and use your internal DNS to switch from the Primary to the DR site. You can provision a Load Balancer in your network that you can point the custom domain at and use that to switch from the Primary to the DR site as needed. Example using simple DNS:\nCreate a new wildcard DNS record with a low TTL pointing to the Primary Cluster’s Ingress/Route ExternalIP in your private DNS zone. (in our case it was *.aro-dr.mobb.ninja)\nModify the route for both apache examples to use the new wildcard DNS record.\nTest access\nUpdate the DNS record to point to the DR site’s Ingress/Route ExternalIP.\nTest access\n","description":"","tags":["ARO","Azure"],"title":"ARO - Considerations for Disaster Recovery","uri":"/docs/aro/disaster-recovery/"},{"content":"A Quickstart guide to deploying a Private Azure Red Hat OpenShift cluster.\nOnce the cluster is running you will need a way to access the private network that ARO is deployed into.\nAuthors: Paul Czarkowski , Ricardo Macedo Martins Prerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Install sshuttle using homebrew\nbrew install sshuttle Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli sshuttle Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned click the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nexport AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift-private export AZR_CLUSTER=private-cluster export AZR_PULL_SECRET=~/Downloads/pull-secret.txt export NETWORK_SUBNET=10.0.0.0/20 export CONTROL_SUBNET=10.0.0.0/24 export MACHINE_SUBNET=10.0.1.0/24 export FIREWALL_SUBNET=10.0.2.0/24 export JUMPHOST_SUBNET=10.0.3.0/24 Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Create an Azure Service Principal\nAZ_SUB_ID=$(az account show --query id -o tsv) AZ_SP_PASS=$(az ad sp create-for-rbac -n \"${AZR_CLUSTER}-SP\" --role contributor \\ --scopes \"/subscriptions/${AZ_SUB_ID}/resourceGroups/${AZR_RESOURCE_GROUP}\" \\ --query \"password\" -o tsv) AZ_SP_ID=$(az ad sp list --display-name \"${AZR_CLUSTER}-SP\" --query \"[0].appId\" -o tsv) Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes $NETWORK_SUBNET \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $CONTROL_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $MACHINE_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies for Private Link Service on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Egress You have the choice of running a NAT GW or Firewall service for your Internet Egress.\nRun through the step of one of the two following options\nNat GW This replaces the routes for the cluster to go through the Azure NAT GW service for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course.\nCreate a Public IP\naz network public-ip create -g $AZR_RESOURCE_GROUP \\ -n $AZR_CLUSTER-natgw-ip \\ --sku \"Standard\" --location $AZR_RESOURCE_LOCATION Create the NAT Gateway\naz network nat gateway create \\ --resource-group ${AZR_RESOURCE_GROUP} \\ --name \"${AZR_CLUSTER}-natgw\" \\ --location ${AZR_RESOURCE_LOCATION} \\ --public-ip-addresses \"${AZR_CLUSTER}-natgw-ip\" Get the Public IP of the NAT Gateway\nGW_PUBLIC_IP=$(az network public-ip show -g ${AZR_RESOURCE_GROUP} \\ -n \"${AZR_CLUSTER}-natgw-ip\" --query \"ipAddress\" -o tsv) echo $GW_PUBLIC_IP Reconfigure Subnets to use Nat GW\naz network vnet subnet update \\ --name \"${AZR_CLUSTER}-aro-control-subnet-${AZR_RESOURCE_LOCATION}\" \\ --resource-group ${AZR_RESOURCE_GROUP} \\ --vnet-name \"${AZR_CLUSTER}-aro-vnet-${AZR_RESOURCE_LOCATION}\" \\ --nat-gateway \"${AZR_CLUSTER}-natgw\" az network vnet subnet update \\ --name \"${AZR_CLUSTER}-aro-machine-subnet-${AZR_RESOURCE_LOCATION}\" \\ --resource-group ${AZR_RESOURCE_GROUP} \\ --vnet-name \"${AZR_CLUSTER}-aro-vnet-${AZR_RESOURCE_LOCATION}\" \\ --nat-gateway \"${AZR_CLUSTER}-natgw\" Firewall + Internet Egress This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course.\nYou can skip this step if you don’t need to restrict egress.\nMake sure you have the AZ CLI firewall extensions\naz extension add -n azure-firewall az extension update -n azure-firewall Create a firewall network, IP, and firewall\naz network vnet subnet create \\ -g $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ -n \"AzureFirewallSubnet\" \\ --address-prefixes $FIREWALL_SUBNET az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip \\ --sku \"Standard\" --location $AZR_RESOURCE_LOCATION az network firewall create -g $AZR_RESOURCE_GROUP \\ -n aro-private -l $AZR_RESOURCE_LOCATION Configure the firewall and configure IP Config (this may take 15 minutes)\naz network firewall ip-config create -g $AZR_RESOURCE_GROUP \\ -f aro-private -n fw-config --public-ip-address fw-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query \"ipAddress\" -o tsv) FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query \"ipConfigurations[0].privateIPAddress\" -o tsv) echo $FWPUBLIC_IP echo $FWPRIVATE_IP Create and configure a route table\naz network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr sleep 10 az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \\ --route-table-name aro-udr --address-prefix 0.0.0.0/0 \\ --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet \\ --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \\ --next-hop-type VirtualNetworkGateway Create firewall rules for ARO resources\nNote: ARO clusters do not need access to the internet, however your own workloads running on them may. You can skip this step if you don’t need any egress at all.\nCreate a Network Rule to allow all http/https egress traffic (not recommended)\naz network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'allow-https' --name allow-all \\ --action allow --priority 100 \\ --source-addresses '*' --dest-addr '*' \\ --protocols 'Any' --destination-ports 1-65535 Create Application Rules to allow to a restricted set of destinations\nreplace the target-fqdns with your desired destinations\naz network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Egress' \\ --action allow \\ --priority 100 \\ -n 'required' \\ --source-addresses '*' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns '*.google.com' '*.bing.com' az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Docker' \\ --action allow \\ --priority 200 \\ -n 'docker' \\ --source-addresses '*' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns '*cloudflare.docker.com' '*registry-1.docker.io' 'apt.dockerproject.org' 'auth.docker.io' Update the subnets to use the Firewall\nOnce the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule.\naz network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr Create the cluster This will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --pull-secret @$AZR_PULL_SECRET \\ --client-id \"${AZ_SP_ID}\" \\ --client-secret \"${AZ_SP_PASS}\" Jump Host With the cluster in a private network, we can create a Jump host in order to connect to it. You can do this while the cluster is being created.\nCreate jump subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name JumpSubnet \\ --address-prefixes $JUMPHOST_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a jump host\naz vm create --name jumphost \\ --resource-group $AZR_RESOURCE_GROUP \\ --ssh-key-values $HOME/.ssh/id_rsa.pub \\ --admin-username aro \\ --image \"RedHat:RHEL:9_1:9.1.2022112113\" \\ --subnet JumpSubnet \\ --public-ip-address jumphost-ip \\ --public-ip-sku Standard \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" Save the jump host public IP address\nJUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \\ --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress') echo $JUMP_IP Use sshuttle to create a ssh vpn via the jump host (use a separate terminal session)\nreplace the IP with the IP of the jump box from the previous step.\nsshuttle --dns -NHr \"aro@${JUMP_IP}\" 10.0.0.0/8 Get OpenShift console URL\nset these variables to match the ones you set at the start.\nAPISERVER=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query apiserverProfile.url) echo $APISERVER Get OpenShift credentials\nADMINPW=$(az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ --query kubeadminPassword \\ -o tsv) log into OpenShift\noc login $APISERVER --username kubeadmin --password ${ADMINPW} Delete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP Addendum Adding Quota to ARO account Create an Azure Support Request Set Issue Type to “Service and subscription limits (quotas)”\nSet Quota Type to “Compute-VM (cores-vCPUs) subscription limit increases”\nClick Next Solutions »\nClick Enter details\nSet Deployment Model to “Resource Manager\nSet Locations to “(US) East US”\nSet Types to “Standard”\nUnder Standard check “DSv3” and “DSv4”\nSet New vCPU Limit for each (example “60”)\nClick Save and continue\nClick Review + create »\nWait until quota is increased.\n","description":"","tags":["ARO","Azure"],"title":"Private ARO Cluster with access via JumpHost","uri":"/docs/aro/private-cluster/"},{"content":"Prerequisites A private ARO cluster Deploy the Egressip Ipam Operator Via GUI Log into the ARO cluster’s Console\nSwitch to the Administrator view\nClick on Operators -\u003e Operator Hub\nSearch for “Egressip Ipam Operator”\nInstall it with the default settings\nor\nVia CLI Deploy the egress-ipam-operator\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: egressip-ipam-operator --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: egressip-ipam-operator namespace: openshift-operators labels: operators.coreos.com/egressip-ipam-operator.egressip-ipam-operator: '' spec: channel: alpha installPlanApproval: Automatic name: egressip-ipam-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: egressip-ipam-operator.v1.2.2 EOF Configure EgressIP Create an EgressIPAM resource for your cluster. Update the CIDR to reflect the worker node subnet.\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: redhatcop.redhat.io/v1alpha1 kind: EgressIPAM metadata: name: egressipam-azure annotations: egressip-ipam-operator.redhat-cop.io/azure-egress-load-balancer: none spec: cidrAssignments: - labelValue: \"\" CIDR: 10.0.1.0/24 reservedIPs: [] topologyLabel: \"node-role.kubernetes.io/worker\" nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF Create test namespaces\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure --- apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test-1 annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure EOF Check the namespaces have IPs assigned\nkubectl get namespace egressipam-azure-test \\ egressipam-azure-test-1 -o yaml | grep egressips The output should look like:\negressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.8 egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.7 Check they’re actually set as Egress IPs\noc get netnamespaces | egrep 'NAME|egress' The output should look like:\nNAME NETID EGRESS IPS egressip-ipam-operator 6374875 egressipam-azure-test 6917470 [\"10.0.1.8\"] egressipam-azure-test-1 16320378 [\"10.0.1.7\"] Finally check the Host Subnets for Egress IPS\noc get hostsubnets The output should look like:\nNAME HOST HOST IP SUBNET EGRESS CIDRS EGRESS IPS private-cluster-bj275-master-0 private-cluster-bj275-master-0 10.0.0.8 10.129.0.0/23 private-cluster-bj275-master-1 private-cluster-bj275-master-1 10.0.0.7 10.128.0.0/23 private-cluster-bj275-master-2 private-cluster-bj275-master-2 10.0.0.9 10.130.0.0/23 private-cluster-bj275-worker-eastus1-zt59t private-cluster-bj275-worker-eastus1-zt59t 10.0.1.4 10.128.2.0/23 [\"10.0.1.8\"] private-cluster-bj275-worker-eastus2-bfrwt private-cluster-bj275-worker-eastus2-bfrwt 10.0.1.5 10.129.2.0/23 [\"10.0.1.7\"] private-cluster-bj275-worker-eastus3-fgjzk private-cluster-bj275-worker-eastus3-fgjzk 10.0.1.6 10.131.0.0/23 Test Egress Log into your jumpbox and allow http into firewall\nsudo firewall-cmd --zone=public --add-service=http Install and start apache httpd\nsudo yum -y install httpd sudo systemctl start httpd Create a index.html\necho HELLO | sudo tee /var/www/html/index.html tail apache logs\nsudo tail -f /var/log/httpd/access_log Start an interactive pod in one of your new namespaces\nkubectl run -n egressipam-azure-test -i \\ --tty --rm debug --image=alpine \\ --restart=Never -- wget -O - 10.0.3.4 The output should look the following (the IP should match the egress IP of your namespace):\n10.0.1.7 - - [03/Feb/2022:19:33:54 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"Wget\" ","description":"","tags":["ARO","Azure"],"title":"Using the Egressip Ipam Operator with a Private ARO Cluster","uri":"/docs/aro/egress-ipam-operator/"},{"content":"In OpenShift images (stored in the in-cluster registry) are protected by Kubernetes RBAC and by default only the namespace in which the image was built can access it.\nFor example if you build an image in project-a only project-a can use that image, or build from it. If you wanted the default service account in project-b to have access to the images in project-a you would run the following.\noc policy add-role-to-user \\ system:image-puller system:serviceaccount:project-b:default \\ --namespace=project-a However if you had to do this for every namespace it could become quite combersome. Instead if you choose to have a set of common images in a common-images namespace you could make them available to all authenticated users like so.\noc adm policy add-cluster-role-to-group system:image-puller \\ system:authenticated --namespace=common-images oc adm policy add-role-to-group view system:authenticated \\ -n common-images Note: It’s important to understand and accept the security implications that come with this. If any Pod in the cluster is compromised it will have access to pull any images in this namespace.\nSee Global Image Puller for an example Kubernetes Controller that may allow for a more surgical (but still automated) way to grant access to images.\n","description":"","tags":["OCP"],"title":"OpenShift - Sharing Common images","uri":"/docs/misc/common-images-namespace/"},{"content":"","description":"","tags":null,"title":"Quickstarts","uri":"/tags/quickstarts/"},{"content":"Note: It is recommended that you use the Cloud Front based guide unless you absolutely must use an ALB based solution.\nHere ’s a good overview of AWS LB types and what they support\nProblem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB\nTodo Configure TLS + DNS for that Ingress (Lets Encrypt + WildCard DNS) Pre Requisites A ROSA / OSD on AWS cluster Helm 3 cli oc / kubectl AWS cli Disable AWS cli output paging\nexport AWS_PAGER=\"\" Set the ALB Controller version\nexport ALB_VERSION=\"v2.2.0\" Set the name of your cluster for lookup\nexport CLUSTER_NAME=\"waf-demo\" Deployment Create a new public ROSA cluster called waf-demo and make sure to set it to be multi-AZ enabled, or replace the cluster name variable with your own cluster name.\nAWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources\nApplication Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation\nCreate AWS Policy and Service Account\ncurl -so iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/${ALB_VERSION}/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\naws iam create-user --user-name aws-lb-controller \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name aws-lb-controller \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml)\naws iam create-access-key --user-name aws-lb-controller export AWS_ID=\u003cfrom above\u003e export AWS_KEY=\u003cfrom above\u003e Modify the VPC ID and cluster name in the values.yaml with the output from (replace poc-waf with your cluster name):\nVPC_ID=$(aws ec2 describe-vpcs --output json --filters \\ Name=tag-value,Values=\"${CLUSTER_NAME}*\" \\ --query \"Vpcs[].VpcId\" --output text) echo ${VPC_ID} Modify the subnet list in ingress.yaml with the output from: (replace poc-waf with your cluster name)\nSUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line)\naws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value= Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared Create a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --set \"env.AWS_ACCESS_KEY_ID=${AWS_ID}\" \\ --set \"env.AWS_SECRET_ACCESS_KEY=${AWS_KEY}\" \\ --set \"vpcID=${VPC_ID}\" \\ --set \"clusterName=${CLUSTER_NAME}\" \\ --set \"image.tag=${ALB_VERSION}\" \\ --create-namespace Deploy Sample Application Create a new application in OpenShift\noc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # alb.ingress.kubernetes.io/subnets: subnet-0982bb73ca67d61de,subnet-0aa9967e8767d792f,subnet-0fd57669a80eb7596 alb.ingress.kubernetes.io/shield-advanced-protection: \"true\" # wafv2 arn to use # alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-2:660250927410:regional/webacl/waf-demo/6565d2a1-6d26-4b6b-b56f-1e996c7e9e8f labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: /* backend: service: name: django-ex port: number: 8080 Check the logs of the ALB controller\nkubectl logs -f deployment/aws-load-balancer-controller use the second address from the ingress to browse to the app\nkubectl -n demo get ingress curl -s --header \"Host: foo.bar\" k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com | head WAF time Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new and use the Core and SQL Injection rules. (make sure region matches us-east-2)\nView your WAF\naws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . set the waf annotation to match the ARN provided above (and uncomment it) then re-apply the ingress\nkubectl apply -f ingress.yaml test the app still works\ncurl -s --header \"Host: foo.bar\" --location \"k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com\" test the WAF denies a bad request\nYou should get a 403 Forbidden error\ncurl -X POST http://k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com -F \"user='\u003cscript\u003e\u003calert\u003eHello\u003e\u003c/alert\u003e\u003c/script\u003e'\" ","description":"","tags":["AWS","ROSA","OSD"],"title":"AWS ALB","uri":"/docs/rosa/waf/alb/"},{"content":"Author: Steve Mirman Video Walkthrough If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube .\nThe purpose of this document is to help you get OpenShift GitOps running in your cluster, including deploying a sample application and demonstrating how ArgoCD ensures environment consistency.\nThis demo assumes you have a Managed OpenShift Cluster available and cluster-admin rights.\nGitHub resources referenced in the demo: BGD Application: gitops-bgd-app OpenShift / ArgoCD configuration: gitops-demo Required command line (CLI) tools GitHub: git OpenShift: oc ArgoCD: argocd Kustomize: kam Environment Set Up Install the OpenShift GitOps operator Install the OpenShift GitOps operator from the Operator Hub\nPull files from GitHub Clone the gitops-demo GitHub repository to your local machine\ngit clone https://github.com/rh-mobb/gitops-demo gitops Export your local path to the GitHub files\nexport GITOPS_HOME=\"$(pwd)/gitops\" cd $GITOPS_HOME Log in to OpenShift via the CLI Retrieve the login command from the OpenShift console Enter the command in your terminal to authenticate with the OpenShift CLI (oc)\nOutput should appear similar to:\nLogged into \"https://\u003cYOUR-INSTANCE\u003e.openshiftapps.com:6443\" as \"\u003cYOUR-ID\u003e\" using the token provided. Deploy the ArgoCD Project Create a new OpenShift project Create a new OpenShift project called gitops oc new-project gitops Edit service account permissions Add cluster-admin rights to the openshift-gitops-argocd-application-controller service account in the openshift-gitops namespace oc adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops Log in to ArgoCD Retrieve ArgoCD URL:\nargoURL=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}{\"\\n\"}') echo $argoURL Retrieve ArgoCD Password:\nargoPass=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d) echo $argoPass In a browser, navigate to the ArgoCD console using the $argoURL value returned above Log in with the user name admin and the password returned as $argoPass above Optional step if you prefer CLI access Login to the CLI:\nargocd login --insecure --grpc-web $argoURL --username admin --password $argoPass Deploy the ArgoCD project Use kubectl to apply the bgd-app.yaml file\nkubectl apply -f documentation/modules/ROOT/examples/bgd-app/bgd-app.yaml The bgd-app.yaml file defines several things, including the repo location for the gitops-bgd-app application\nCheck the rollout running the following command:\nkubectl rollout status deploy/bgd -n bgd Once the rollout is complete get the route to the application\noc get route bgd -n bgd -o jsonpath='{.spec.host}{\"\\n\"}' In your browser, paste the route to open the application Go back to your ArgoCD window and verify the configuration shows there as well Exploring the application in ArgoCD, you can see all the components are green (synchronized) Deploy a change to the application In the terminal, enter the following command which will introduce a chance into the bgd application\nkubectl -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' Go back to your ArgoCD window. The application should no longer be synchronized Refresh the bgd application window and notice the change in box color\nThe new deployment changed the box from blue to green, but only within OpenShift, not in the source code repository\nSynchronize the application In the ArgoCD console, click the SYNC button to re-synchronize the bgd application with the approved configuration in the source code repository Refresh the bgd application window and notice the change in box color\nDetails from GitHub perspective TBD\n","description":"","tags":["GitOps","ROSA","ARO"],"title":"Demonstrate GitOps on Managed OpenShift with ArgoCD","uri":"/docs/redhat/gitops/"},{"content":"Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nQuick Introduction by Paul Czarkowski \u0026 Ryan Niksch on YouTube Solutions Cloud Front -\u003e WAF -\u003e CustomDomain -\u003e $APP This is the preferred method and can also work with most third party WAF systems that act as a reverse proxy\nUses a custom domain, custom route, LE cert. CloudFront and WAF\nUsing Cloud Front Application Load Balancer -\u003e ALB Operator -\u003e $APP Installs the ALB Operator, and uses the ALB to route via WAF, one ALB per app though!\nApplication Load Balancer ","description":"","tags":["AWS","ROSA","OSD","OCP"],"title":"Examples of using a WAF in front of ROSA / OSD on AWS / OCP on AWS","uri":"/docs/rosa/waf/"},{"content":"This is a POC of ROSA with a AWS WAF service Non working (yet) instructions for using STS to manage the creds for ALB\nSee https://issues.redhat.com/browse/CTONET-858 for a similar request\nHere ’s a good overview of AWS LB types and what they support\nProblem Statement Customer requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nCustomer does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/ Deploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB\nConfigure TLS + DNS for that Ingress\nLets Encrypt + WildCard DNS Deployment AWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources\nApplication Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation\nConfigure STS Make sure your cluster has the pod identity webhook\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Create AWS Policy and Service Account\nPOLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\nNote I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out.\nSA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key\nACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user\nPaste the AccessKeyId and SecretAccessKey into values.yaml\ntag your public subnet with ``\nCreate a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml ","description":"","tags":["AWS","ROSA"],"title":"This is a POC of ROSA with a AWS WAF service","uri":"/docs/rosa/waf/readme-complex/"},{"content":"Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Add a CustomDomain resource to the cluster using a wildcard DNS and TLS certificate.\nSet the Wildcard DNS CNAME’s to CloudFront and enable the CloudFront + WAF services to reverse proxy and inspect the traffic before sending it to the cluster.\nPreparation Create a cluster\nrosa create cluster --cluster-name poc-waf --multi-az \\ --region us-east-2 --version 4.7.9 --compute-nodes 3 \\ --machine-cidr 10.0.0.0/16 --service-cidr 172.30.0.0/16 \\ --pod-cidr 10.128.0.0/14 --host-prefix 23 When its ready create a admin user and follow the instructions to log in\nrosa create admin -c poc-waf Set some environment variables\nEMAIL=username.taken@gmail.com DOMAIN=waf.mobb.ninja Certificate and DNS Use certbot to create a wildcard cert\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" Follow Certbot’s instructions to create a DNS TXT record. certificate records will be saved on your system, in my case in /etc/letsencrypt/live/waf.mobb.ninja/. set that as an enviroment variable.\nCERTS=/etc/letsencrypt/live/waf.mobb.ninja Custom OpenShift Domain Create a project and add the certs as a secret\noc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain1.pem --key=$CERTS/privkey1.pem Create a Custom Domain resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait until your Custom Domain has an Endpoint\nwatch oc get customdomains AWS WAF + CloudFront Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2 and use the Core and SQL Injection rules and set it as a CloudFront distribution resource type.\nView your WAF\naws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . Add a certificate to ACM - https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/ , Paste in the cert, key, certchain from the files certbot game you.\nMake sure you create it in the US-EAST-1 region (otherwise cloud front can’t use it)\nLog into the AWS console and Create a Cloud Front distribution (make sure its the same region as your cluster).\nOrigin Domain Name: Origin Protocol Policy: HTTPS only Viewer Protocol Policy: Redirect HTTP to HTTPS Allowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE AWS WAF Web ACL: demo-waf-acl Alternate Domain Names: *. Custom SSL Certificate: Origin Request Policy: create a new policy whitelist: Origin, user-agent, referer, host (IMPORTANT) Hit Create then wait until the Status is Ready.\nDNS CNAME Create a CNAME in your DNS provider for *.\u003c$DOMAIN\u003e that points at the endpoint from the above status page. It should look something like d1vm7mfs9sc24l.cloudfront.net. Deploy an Application Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application\noc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.waf.mobb.ninja Test the WAF Make sure you can access your application with curl\ncurl https://hello.waf.mobb.ninja You should get a simple hello response\nHello OpenShift! Try do a XSS injection\ncurl -X POST https://hello.waf.mobb.ninja \\ -F \"user='\u003cscript\u003e\u003calert\u003eHello\u003e\u003c/alert\u003e\u003c/script\u003e'\" you should see this\n\u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"\u003e \u003cHTML\u003e\u003cHEAD\u003e\u003cMETA HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\"\u003e \u003cTITLE\u003eERROR: The request could not be satisfied\u003c/TITLE\u003e \u003c/HEAD\u003e\u003cBODY\u003e \u003cH1\u003e403 ERROR\u003c/H1\u003e ","description":"","tags":["AWS","ROSA"],"title":"Using CloudFront + WAF","uri":"/docs/rosa/waf/cloud-front/"},{"content":"Common Managed OpenShift References / Tasks Managed OpenShift Overviews Red Hat OpenShift Managed services Microsoft Azure Red Hat OpenShift - ARO Red Hat OpenShift on AWS - ROSA Red Hat OpenShift on IBM Cloud Red Hat OpenShift Dedicated - OSD Managed OpenShift Documentation OpenShift Container Platform v4.7 Azure Red Hat OpenShift v4.x - ARO Red Hat OpenShift on AWS v4.x - ROSA Red Hat OpenShift on IBM Cloud v4.x OpenShift Dedicated v4.x - OSD Common Customer Topics Red Hat OpenShift on AWS - ROSA Creating a ROSA cluster with Private Link enabled ROSA Installation Prerequisites ROSA STS Workflow Shared Responsiblity Matrix (who does what) Red Hat Process and Security for ROSA ROSA Support Azure on Red Hat OpenShift ARO ARO Installation Process ARO Support Azure Compliance Monitoring Authentication Education ARO Getting Started ROSA Getting Started ","description":"","tags":null,"title":"Common Managed OpenShift References / Tasks","uri":"/docs/misc/references/"},{"content":"This is example Terraform to create a single AZ VPC in which to deploy a single AZ ROSA cluster. This is intended to be used as a guide to get started quickly, not to be used in production.\nPre-Requisites Terraform Deploy Download this repo\ngit clone https://github.com/rh-mobb/documentation.git cd documentation/docs/rosa/byo-vpc Modify main.tf as needed, then run\nterraform init terraform plan terraform apply Cleanup To destroy resources terraform destroy ","description":"","tags":["AWS","ROSA"],"title":"Creating a Public/Private BYO VPC for ROSA","uri":"/docs/rosa/byo-vpc/"},{"content":"Prerequisites AWS CLI Rosa CLI v1.0.8 jq Create VPC and Subnets The following instructions use the AWS CLI to create the necessary networking to deploy a Private Link ROSA cluster into a Single AZ and are intended to be a guide. Ideally you would use an Automation tool like Ansible or Terraform to manage your VPCs.\nWhen creating subnets, make sure that subnet(s) are created to an availability zone that has ROSA instances types available. If AZ is not “forced”, subnet is created to random AZ in the region. Force the AZ using --availability-zone argument in create-subnet command.\nUse rosa list instance-types to list ROSA instance types and check available types availability in AZ with the following\naws ec2 describe-instance-type-offerings \\ --location-type availability-zone \\ --filters Name=location,Values=AZ_NAME_HERE \\ --region REGION_HERE --output text | egrep \"YOU_PREFERRED_INSTANCE_TYPE\" As an example, you cannot install ROSA to us-east-1e AZ, but us-east-1b works fine.\nOption 1 - VPC with a private subnet and AWS Site-to-Site VPN access. Todo\nOption 2 - VPC with public and private subnets and AWS Site-to-Site VPN access Todo\nOption 3 - VPC with public and private subnets (NAT) This will create both a Private and Public subnet. All cluster resources will live in the private subnet, the public subnet only exists to NAT the egress traffic to the Internet.\nAs an alternative use the Terraform instructions provided here then skip down to the rosa create command.\nSet a Cluster name\nROSA_CLUSTER_NAME=private-link Create a VPC to install a ROSA cluster into\nVPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames | jq . Create a Public Subnet for the cluster to NAT egress traffic out of\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public | jq . Create a Private Subnet for the cluster machines to live in\nPRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . Create an Internet Gateway for NAT egress traffic\nI_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a Route Table for NAT egress traffic\nR_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` aws ec2 create-route --route-table-id $R_TABLE --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW | jq . aws ec2 describe-route-tables --route-table-id $R_TABLE | jq . aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET --route-table-id $R_TABLE | jq . aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a NAT Gateway for the Private network\nEIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` NAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a Route Table for the Private subnet to the NAT\nR_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` while ! aws ec2 describe-route-tables --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done aws ec2 create-route --route-table-id $R_TABLE_NAT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GW | jq . aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET --route-table-id $R_TABLE_NAT | jq . aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . Deploy ROSA Create ROSA cluster in the private subnet\nrosa create cluster --private-link \\ --cluster-name=$ROSA_CLUSTER_NAME \\ --machine-cidr=10.0.0.0/16 \\ --subnet-ids=$PRIVATE_SUBNET Test Connectivity Create an Instance to use as a jump host\nTODO: CLI instructions\nThrough the GUI:\nNavigate to the EC2 console and launch a new instance\nSelect the AMI for your instance, if you don’t have a standard, the Amazon Linux 2 AMI works just fine\nChoose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details\nChange the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are OK, if you do not need to change them for your own reasons, select 6. Configure Security Group from the top navigation or click through using the Next buttons\nIf you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list and skip to Review and Launch. Otherwise, select Create a new security group and continue.\nTo allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch, verify all settings are correct and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys.\nOnce launched, open the instance summary for the jump host instance and note the public IP address.\nCreate a ROSA admin user and save the login command for use later\nrosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed\nrosa describe cluster -c private-link update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below\n127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP\nsudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP Log into the cluster using oc login command from the create admin command above. ex.\noc login https://api.private-test.3d1n.p1.openshiftapps.com:6443 --username cluster-admin --password GQSGJ-daqfN-8QNY3-tS9gU Check that you can access the Console by opening the console url in your browser.\nCleanup Delete ROSA\nrosa delete cluster -c $ROSA_CLUSTER_NAME -y Delete AWS resources\naws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID \\ --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq . ","description":"","tags":["AWS","ROSA","Private Link"],"title":"Creating a ROSA cluster with Private Link enabled","uri":"/docs/rosa/private-link/"},{"content":"This guide walks through setting up federating Prometheus metrics to S3 storage.\nToDo - Add Authorization in front of Thanos APIs\nPrerequisites A ROSA cluster deployed with STS aws CLI Set up environment Create environment variables\nexport CLUSTER_NAME=my-cluster export S3_BUCKET=my-thanos-bucket export REGION=us-east-2 export NAMESPACE=federated-metrics export SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Create namespace\noc new-project $NAMESPACE AWS Preperation Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Create a Policy for access to S3\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy\nS3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-thanos \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create a Trust Policy\ncat «EOF \u003e $SCRATCH_DIR/TrustPolicy.json { “Version”: “2012-10-17”, “Statement”: [ { “Effect”: “Allow”, “Principal”: { “Federated”: “arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}” }, “Action”: “sts:AssumeRoleWithWebIdentity”, “Condition”: { “StringEquals”: { “${OIDC_PROVIDER}:sub”: [ “system:serviceaccount:${NAMESPACE}:${SA}” ] } } } ] } EOF ```\nCreate Role for AWS Prometheus and CloudWatch\nS3_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER-thanos-s3\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $S3_ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"$CLUSTER-thanos-s3\" \\ --policy-arn $S3_POLICY Deploy Operators Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n $echNAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-thanos-s3/files/operatorhub.yaml Deploy Thanos Store Gateway Deploy ROSA Thanos S3 Helm Chart\nhelm upgrade -n $NAMESPACE rosa-thanos-s3 --install mobb/rosa-thanos-s3 \\ --set \"aws.roleArn=$ROLE_ARN\" \\ --set \"rosa.clusterName=$CLUSTER_NAME\" Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos.\nCheck if the User Workload Config Map exists:\noc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn’t exist run:\ncat « EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: “http://thanos-receive.${NAMESPACE}.svc.cluster.local:9091/api/v1/receive” EOF ```\n**Otherwise update it with the following:** ```bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config ``` ```yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\" ``` Check metrics are flowing by logging into Grafana get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret).\noc -n thanos-receiver get route grafana-route Once logged in go to Dashboards-\u003eManage and expand the federated-metrics group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.\n","description":"","tags":["AWS","ROSA"],"title":"Federating System and User metrics to S3 in Red Hat OpenShift for AWS","uri":"/docs/rosa/federated-metrics/"},{"content":"In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it.\nEnabling See docs for more indepth details.\nCheck the cluster-monitoring-config ConfigMap object\noc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following\nIf the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually.\noc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and\nThis will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you’re going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource\noc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics.\nSwitch to Developer mode Change the Project to ns1 Click the Monitoring button Grafana Create a Project for the Grafana Operator + Application\noc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI)\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance\ncat \u003c\u003c EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above.\noc -n custom-grafana get routes The output should look like\nNAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working.\nGrant the grafana instance access to cluster-metrics\noc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable\nBEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier\ncat \u003c\u003c EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana\nThe dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace.\noc apply -f dashboards.yaml ","description":"","tags":null,"title":"User Workload Monitoring on Azure Red Hat OpenShift","uri":"/docs/aro/user-workload-monitoring/"},{"content":"By default Azure Red Hat OpenShift (ARO) stores metrics in Ephemeral volumes, and its advised that users do not change this setting. However its not unreasonable to expect that metrics should be persisted for a set amount of time.\nThis guide shows how to set up Thanos to federate both System and User Workload Metrics to a Thanos gateway that stores the metrics in Azure Files and makes them available via a Grafana instance (managed by the Grafana Operator).\nToDo - Add Authorization in front of Thanos APIs\nPre-Prequsites An ARO cluster\nSet some environment variables to use throughout to suit your environment\nNote: AZR_STORAGE_ACCOUNT_NAME must be unique\nexport AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arometrics$(cat /dev/urandom | LC_ALL=C tr -dc 'a-z0-9' | fold -w 5 | head -n 1) export CLUSTER_NAME=openshift export NAMESPACE=aro-thanos-af Azure Preperation Create an Azure storage account\nmodify the arguments to suit your environment\naz storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 Get the account key and update the secret in thanos-store-credentials.yaml\nAZR_STORAGE_KEY=$(az storage account keys list -g $AZR_RESOURCE_GROUP \\ -n $AZR_STORAGE_ACCOUNT_NAME --query \"[0].value\" -o tsv) Create a namespace to use\noc new-project $NAMESPACE Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the grafana operator\nhelm upgrade -n $NAMESPACE $NAMESPACE-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/grafana-operator.yaml Use the mobb/operatorhub chart to deploy the resource-locker operator\n\u003e Note: Skip this if you already have the resource-locker operator installed, or if you do not plan to use User Workload Metrics\nhelm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --version 0.1.1 --create-namespace --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/resourcelocker-operator.yaml Deploy ARO Thanos Azure Files Helm Chart (mobb/aro-thanos-af)\n\u003e Note: enableUserWorkloadMetrics=true will overwrite configs for cluster and userworkload metrics, remove it from the helm command below if you already have custom settings. The Addendum at the end of this doc will explain the changes you’ll need to make instead.\nhelm upgrade -n $NAMESPACE aro-thanos-af \\ --install mobb/aro-thanos-af --version 0.4.1 \\ --set \"aro.storageAccount=$AZR_STORAGE_ACCOUNT_NAME\" \\ --set \"aro.storageAccountKey=$AZR_STORAGE_KEY\" \\ --set \"aro.storageContainer=$CLUSTER_NAME\" \\ --set \"enableUserWorkloadMetrics=true\" Validate Grafana is installed and seeing metrics from Azure Files get the Route URL for Grafana (remember its https) and login using username admin and the password password.\noc -n $NAMESPACE get route grafana-route Once logged in go to Dashboards-\u003eManage and expand the thanos-receiver group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.\nNote: If it complains about a missing datasource run the following: oc annotate -n $NAMESPACE grafanadatasource aro-thanos-af-prometheus \"retry=1\"\nCleanup Uninstall the aro-thanos-af chart\nhelm delete -n $NAMESPACE aro-thanos-af Uninstall the federated-metrics-operators chart\nhelm delete -n $NAMESPACE federated-metrics-operators Delete the aro-thanos-af namespace\noc delete namespace $NAMESPACE Delete the storage account\naz storage account delete \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP Addendum Enabling User Workload Monitoring See docs for more indepth details.\nCheck the cluster-monitoring-config ConfigMap object\noc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following\nIf the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually.\noc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely.\noc patch configmap cluster-monitoring-config -n openshift-monitoring \\ -p='{\"data\":{\"config.yaml\": \"enableUserWorkload: true\\n\"}}' Check that the User workload monitoring is starting up\noc -n openshift-user-workload-monitoring get pods Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos.\nCheck if the User Workload Config Map exists:\noc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config -o yaml If the config doesn’t exist (or is empty) run:\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://thanos-receive.$NAMESPACE.svc.cluster.local:9091/api/v1/receive\" EOF Otherwise update it with the following:\noc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\" ","description":"","tags":["ARO","Azure"],"title":"Federating System and User metrics to Azure Files in Azure Red Hat OpenShift","uri":"/docs/aro/federated-metrics/"},{"content":" see here for public clusters.\nThis assumes you’ve already got a private ARO cluster installed. You could also follow the same instructions to create a public Astronomer, just use a regular DNS zone and skip the private parts.\nA default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace.\nCreate a private DNS Log into Azure and click to private dns Click + Add\nSet the Resource Group to match your ARO Resource Group\nSet Name to your TLD (astro.mobb.ninja in the example)\nClick Review and Create and create the Zone\nInside the Domain settings click Virtual network links -\u003e + Add\nLink Name: astro-aro\nSelect the correct Subscription and Network from the dropdown boxes\nClick OK\nCreate TLS Secret Next we need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don’t need to be public)\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.astro.mobb.ninja\" Follow certbot’s instructions (something like ):\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a Secret from the Cert (use the paths provided from the above command):\noc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem Deploy Astronomer update the values.yaml and set baseDomain: astro.mobb.ninja\nInstall\nhelm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer While that’s running add our DNS In another shell run\nkubectl get svc -n astronomer astronomer-nginx Go back to your private DNS zone in Azure and create a record set * and copy the contents of EXTERNAL-IP from the above command.\nFix SCCs for elasticsearch oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}' Validate the Install Check the Helm install has finished\nNAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: - Astronomer dashboard: https://app.astro.mobb.ninja - Grafana dashboard: https://grafana.astro.mobb.ninja - Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. Since this is a private LB you’ll need to access it from inside the network. The quick hacky way to do this is\nkubectl exec -ti astronomer-cli-install-6f899c87d5-2c84f -- wget -O - https://install.astro.mobb.ninja and you should see\n#! /usr/bin/env bash TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ","description":"","tags":["ARO","Azure"],"title":"Installing Astronomer on a private ARO cluster","uri":"/docs/aro/astronomer/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"}]